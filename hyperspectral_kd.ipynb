{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import shutil, json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "V8MVvYkwPUsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "metadata": {
        "id": "9UWpj8baGdEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "\n",
        "    return patchesData, patchesLabels.astype(\"int\")"
      ],
      "metadata": {
        "id": "eyK_NRHiGdHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir teacher\n",
        "!mkdir student"
      ],
      "metadata": {
        "id": "JBOK5Q_hcsZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = envi.open('./drive/MyDrive/HSI-datasets/IP_DataSet/indianpines_ds_raw.hdr', \\\n",
        "                './drive/MyDrive/HSI-datasets/IP_DataSet/indianpines_ds_raw.raw')"
      ],
      "metadata": {
        "id": "On0mC8PYPd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "b715a887-f7cb-4450-f234-4c70dc3ebff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1babd5efc357>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data = envi.open('./drive/MyDrive/HSI-datasets/IP_DataSet/indianpines_ds_raw.hdr', \\\n\u001b[0m\u001b[1;32m      2\u001b[0m                 './drive/MyDrive/HSI-datasets/IP_DataSet/indianpines_ds_raw.raw')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'envi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disjoint_pixels = envi.open('./drive/MyDrive/HSI-datasets/IP_TrainSet/indianpines_ts_raw_classes.hdr', \\\n",
        "                './drive/MyDrive/HSI-datasets/IP_TrainSet/indianpines_ts_raw_classes.raw')"
      ],
      "metadata": {
        "id": "Cp5B8r7wPsJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disjoint_pixels = disjoint_pixels.read_bands([0])\n",
        "disjoint_pixels = disjoint_pixels.reshape(disjoint_pixels.shape[0], disjoint_pixels.shape[1])"
      ],
      "metadata": {
        "id": "lQ7MKLXSPsLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = copy.deepcopy(disjoint_pixels)\n",
        "for i, val in enumerate([0,2,3,5,6,8,10,11,12,14,1,4,7,9,13,15,16]): y_train[disjoint_pixels==i] = val"
      ],
      "metadata": {
        "id": "5FWA7sOfVLfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the grid\n",
        "plt.imshow(y_train, cmap='bwr')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "aixUM7b0QBak",
        "outputId": "931d1c62-3a74-4c06-ece6-12aff5063e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/indian_pines_gt.mat')\n",
        "labels = labels['indian_pines_gt']"
      ],
      "metadata": {
        "id": "L6yYoYl5RIUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the grid\n",
        "plt.imshow(y_test, cmap='bwr')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "hUds9I73RT4M",
        "outputId": "f5e3f6b7-026b-4225-a45b-830e6767ead7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ['India', 'Pavia'][0]\n",
        "random_split = False"
      ],
      "metadata": {
        "id": "TaNSMu8Zdd7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == 'India':\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/indian_pines_corrected.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indianpines_disjoint_dset.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indian_pines_gt.mat')\n",
        "\n",
        "    X_all = raw_data['indian_pines_corrected']\n",
        "    y_disjoint = disjoint_data['indianpines_disjoint_dset']\n",
        "    y_all = all_labels['indian_pines_gt']\n",
        "\n",
        "    test_ratio = 0.45\n",
        "else:\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/paviaU.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/TRpavia_fixed.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/paviaU_gt.mat')\n",
        "\n",
        "    X_all = raw_data['paviaU']\n",
        "    y_disjoint = disjoint_data['TRpavia_fixed']\n",
        "    y_all = all_labels['paviaU_gt']\n",
        "\n",
        "    test_ratio = 0.93"
      ],
      "metadata": {
        "id": "_RL2WcWcdhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_disjoint, return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ZGnu1tWyjH",
        "outputId": "69759c5b-60b5-4da0-fde7-f32a44e169e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
              "       dtype=uint8),\n",
              " array([15336,   762,   435,   232,   394,   235,   470,  1424,   328,\n",
              "          728,    29,   146,    16,    10,   132,   291,    57]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_all, return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ioIHe0VW8eJ",
        "outputId": "b95cdf5a-32a5-4b28-bf06-8882b96c9efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
              "       dtype=uint8),\n",
              " array([10776,    46,  1428,   830,   237,   483,   730,    28,   478,\n",
              "           20,   972,  2455,   593,   205,  1265,   386,    93]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Uci233aici8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "_nYjGrFZci-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")"
      ],
      "metadata": {
        "id": "quGUfFsw5HoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = X_all.shape[-1]\n",
        "W = X_all.shape[0]\n",
        "H = X_all.shape[1]\n",
        "num_components = 40\n",
        "window = 19\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "Iq60eCjE3uN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = X_all.reshape(W*H, num_channels)\n",
        "y_disjoint = y_disjoint.reshape(-1, 1).flatten()\n",
        "\n",
        "X_all = X_all.astype(np.float32)"
      ],
      "metadata": {
        "id": "pfy8EN34aYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=num_components)\n",
        "X_all = pca.fit_transform(X_all)\n",
        "\n",
        "X_all = X_all.reshape(W, H, num_components)"
      ],
      "metadata": {
        "id": "CC7d6YQN7aI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = createImageCubes(X_all, y_all, windowSize=window, removeZeroLabels = False)"
      ],
      "metadata": {
        "id": "9oB6lWLu8HO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('data.npy', X)\n",
        "np.save('labels.npy', y)"
      ],
      "metadata": {
        "id": "Yxzb3j0_oBWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_all, y_all, X, y, raw_data"
      ],
      "metadata": {
        "id": "MF7Ll8wSagy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load('data.npy', mmap_mode='c')\n",
        "y = np.load('labels.npy', mmap_mode='c')"
      ],
      "metadata": {
        "id": "7SC0qR7soTS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.transpose(X, (0, 3, 1, 2))"
      ],
      "metadata": {
        "id": "UQbljf-2-DAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSszW6uQFKcW",
        "outputId": "5446cf23-ea00-4a3e-da3e-5fd6119d3ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21025, 40, 19, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if random_split:\n",
        "    nonzero_idx = np.where(y != 0)\n",
        "    X = X[nonzero_idx]\n",
        "    y = y[nonzero_idx]\n",
        "    y = y - 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
        "else:\n",
        "    train_idx = np.where((y_disjoint != 0) & (y != 0))\n",
        "    X_train = X[train_idx]\n",
        "    y_train = y[train_idx]\n",
        "\n",
        "    test_idx = np.where((y_disjoint == 0) & (y != 0))\n",
        "    X_test = X[test_idx]\n",
        "    y_test = y[test_idx]\n",
        "\n",
        "    y_train -= 1\n",
        "    y_test -= 1\n",
        "\n",
        "    print('Disjoint')"
      ],
      "metadata": {
        "id": "g7UKdrCxd55M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2fbc43-60e9-44d2-9377-07fa8583bdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disjoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], num_components*window*window)\n",
        "X_test = X_test.reshape(X_test.shape[0], num_components*window*window)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], num_components, window, window)\n",
        "X_test = X_test.reshape(X_test.shape[0], num_components, window, window)"
      ],
      "metadata": {
        "id": "VEgdEKhfY6t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train, return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVi8LuhzXqk8",
        "outputId": "92765ce1-ebbd-419b-d714-db8b55b7b69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]),\n",
              " array([  21,  753,  426,  138,  209,  376,   26,  228,   10,  469, 1390,\n",
              "         311,  125,  720,  287,   49]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "HXY841qwCwEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_train) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_train)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size) # create your dataloader"
      ],
      "metadata": {
        "id": "JY6DCjbUcWIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_val) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_val)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "val_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size) # create your dataloader"
      ],
      "metadata": {
        "id": "NvNX80DayaKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_test)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size) # create your dataloader"
      ],
      "metadata": {
        "id": "hm46r6mzcWKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn2d(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN2d network\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(0.5)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "    def __init__(self, input_channels, n_classes, cfg=(50, 100), fc=100):\n",
        "        super(cnn2d, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=cfg[0], kernel_size=(5, 5))\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=cfg[0], out_channels=cfg[1], kernel_size=(5, 5))\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.fc1 = nn.Linear(int(25*cfg[1]), fc)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(fc, n_classes)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def get_feat_modules(self):\n",
        "        feat_m = nn.ModuleList([])\n",
        "        feat_m.append(self.conv1)\n",
        "        feat_m.append(self.relu1)\n",
        "        feat_m.append(self.conv2)\n",
        "        feat_m.append(self.relu2)\n",
        "        feat_m.append(self.pool)\n",
        "        feat_m.append(self.fc1)\n",
        "        feat_m.append(self.relu3)\n",
        "        feat_m.append(self.fc2)\n",
        "        return feat_m\n",
        "    \n",
        "    def distill_seq(self):\n",
        "        feat_m = nn.ModuleList([])  \n",
        "        #feat_m.append(self.conv1)       \n",
        "        feat_m.append(nn.Sequential(\n",
        "            self.conv1, self.relu1))\n",
        "        feat_m.append(nn.Sequential(\n",
        "            self.conv2, self.relu2))\n",
        "        feat_m.append(nn.Sequential(\n",
        "            self.pool,\n",
        "            nn.Flatten()))\n",
        "        feat_m.append(nn.Sequential(\n",
        "            self.pool, nn.Flatten(), self.fc1, self.relu3, self.fc2))\n",
        "        return feat_m\n",
        "\n",
        "    def forward(self, x, is_feat=False, preact=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        f0 = x\n",
        "        x = self.conv2(x)\n",
        "        f1_pre = x\n",
        "        x = self.relu2(x)\n",
        "        f1 = x\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
        "        f2 = x\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        if is_feat:\n",
        "            if preact:\n",
        "                return [f0, f1_pre, f2], x\n",
        "            else:\n",
        "                return [f0, f1, f2], x\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "gOO1L2whltkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "9i4TWcqGFsRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"validation\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input = input.float()\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "25F14fQb9f7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "kI42GAp9Ft9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    \n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "    \n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def value(self):\n",
        "        return self.total/float(self.steps)"
      ],
      "metadata": {
        "id": "bXWRpOUxSM23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Method: Offline Distillation**"
      ],
      "metadata": {
        "id": "nBnlO6BHEAII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "kd_method = ['kd', 'hint', 'crd', 'attention', 'fsp', 'correlation', 'simkd'][-1]\n",
        "\n",
        "kd_T = 10 # temperature of soft target method\n",
        "alpha = 1\n",
        "beta = 1\n",
        "gamma = 1\n",
        "learning_rate = 8e-4\n",
        "feat_dim = 32 # feature dimention of intermediate layers\n",
        "factor = 2\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.89  # prune ratio of conv layers\n",
        "linear_r = 0.89 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "KuUEJenbjy3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillKL(nn.Module):\n",
        "    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n",
        "    def __init__(self, T):\n",
        "        super(DistillKL, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "    def forward(self, y_s, y_t):\n",
        "        p_s = F.log_softmax(y_s/self.T, dim=1)\n",
        "        p_t = F.softmax(y_t/self.T, dim=1)\n",
        "        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]\n",
        "        return loss"
      ],
      "metadata": {
        "id": "hHvzP09UR1HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HintLoss(nn.Module):\n",
        "    \"\"\"Fitnets: hints for thin deep nets, ICLR 2015\"\"\"\n",
        "    def __init__(self):\n",
        "        super(HintLoss, self).__init__()\n",
        "        self.crit = nn.MSELoss()\n",
        "\n",
        "    def forward(self, f_s, f_t):\n",
        "        loss = self.crit(f_s, f_t)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "_sYh0a_KEDaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FSP(nn.Module):\n",
        "    \"\"\"A Gift from Knowledge Distillation:\n",
        "    Fast Optimization, Network Minimization and Transfer Learning\"\"\"\n",
        "    def __init__(self, s_shapes, t_shapes):\n",
        "        super(FSP, self).__init__()\n",
        "        assert len(s_shapes) == len(t_shapes), 'unequal length of feat list'\n",
        "        s_c = [s[1] for s in s_shapes]\n",
        "        t_c = [t[1] for t in t_shapes]\n",
        "        print(s_shapes)\n",
        "        print(t_shapes)\n",
        "        # if np.any(np.asarray(s_c) != np.asarray(t_c)):\n",
        "        #     raise ValueError('num of channels not equal (error in FSP)')\n",
        "\n",
        "    def forward(self, g_s, g_t):\n",
        "        s_fsp = self.compute_fsp(g_s)\n",
        "        t_fsp = self.compute_fsp(g_t)\n",
        "        loss_group = [self.compute_loss(s, t) for s, t in zip(s_fsp, t_fsp)]\n",
        "        return loss_group\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_loss(s, t):\n",
        "        return (s - t).pow(2).mean()\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_fsp(g):\n",
        "        fsp_list = []\n",
        "        for i in range(len(g) - 1):\n",
        "            bot, top = g[i], g[i + 1]\n",
        "            b_H, t_H = bot.shape[2], top.shape[2]\n",
        "            if b_H > t_H:\n",
        "                bot = F.adaptive_avg_pool2d(bot, (t_H, t_H))\n",
        "            elif b_H < t_H:\n",
        "                top = F.adaptive_avg_pool2d(top, (b_H, b_H))\n",
        "            else:\n",
        "                pass\n",
        "            bot = bot.unsqueeze(1)\n",
        "            top = top.unsqueeze(2)\n",
        "            bot = bot.view(bot.shape[0], bot.shape[1], bot.shape[2], -1)\n",
        "            top = top.view(top.shape[0], top.shape[1], top.shape[2], -1)\n",
        "\n",
        "            fsp = (bot * top).mean(-1)\n",
        "            fsp_list.append(fsp)\n",
        "        return fsp_list"
      ],
      "metadata": {
        "id": "3Jck-giRH9Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Correlation(nn.Module):\n",
        "    \"\"\"Correlation Congruence for Knowledge Distillation, ICCV 2019.\n",
        "    The authors nicely shared the code with me. I restructured their code to be \n",
        "    compatible with my running framework. Credits go to the original author\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Correlation, self).__init__()\n",
        "\n",
        "    def forward(self, f_s, f_t):\n",
        "        delta = torch.abs(f_s - f_t)\n",
        "        loss = torch.mean((delta[:-1] * delta[1:]).sum(1))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "QqPAxBFyQd_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks\n",
        "    via Attention Transfer\n",
        "    code: https://github.com/szagoruyko/attention-transfer\"\"\"\n",
        "    def __init__(self, p=2):\n",
        "        super(Attention, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, g_s, g_t):\n",
        "        return [self.at_loss(f_s, f_t) for f_s, f_t in zip(g_s, g_t)]\n",
        "\n",
        "    def at_loss(self, f_s, f_t):\n",
        "        s_H, t_H = f_s.shape[2], f_t.shape[2]\n",
        "        if s_H > t_H:\n",
        "            f_s = F.adaptive_avg_pool2d(f_s, (t_H, t_H))\n",
        "        elif s_H < t_H:\n",
        "            f_t = F.adaptive_avg_pool2d(f_t, (s_H, s_H))\n",
        "        else:\n",
        "            pass\n",
        "        return (self.at(f_s) - self.at(f_t)).pow(2).mean()\n",
        "\n",
        "    def at(self, f):\n",
        "        return F.normalize(f.pow(self.p).mean(1).view(f.size(0), -1))"
      ],
      "metadata": {
        "id": "VMCGyS6buODk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearEmbed(nn.Module):\n",
        "    \"\"\"Linear Embedding\"\"\"\n",
        "    def __init__(self, dim_in=1024, dim_out=128):\n",
        "        super(LinearEmbed, self).__init__()\n",
        "        self.linear = nn.Linear(dim_in, dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t6lew3WFQubA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvReg(nn.Module):\n",
        "    \"\"\"Convolutional regression for FitNet\"\"\"\n",
        "    def __init__(self, s_shape, t_shape, use_relu=True):\n",
        "        super(ConvReg, self).__init__()\n",
        "        self.use_relu = use_relu\n",
        "        s_N, s_C, s_H, s_W = s_shape\n",
        "        t_N, t_C, t_H, t_W = t_shape\n",
        "        if s_H == 2 * t_H:\n",
        "            self.conv = nn.Conv2d(s_C, t_C, kernel_size=3, stride=2, padding=1)\n",
        "        elif s_H * 2 == t_H:\n",
        "            self.conv = nn.ConvTranspose2d(s_C, t_C, kernel_size=4, stride=2, padding=1)\n",
        "        elif s_H >= t_H:\n",
        "            self.conv = nn.Conv2d(s_C, t_C, kernel_size=(1+s_H-t_H, 1+s_W-t_W))\n",
        "        else:\n",
        "            raise NotImplemented('student size {}, teacher size {}'.format(s_H, t_H))\n",
        "        self.bn = nn.BatchNorm2d(t_C)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.use_relu:\n",
        "            return self.relu(self.bn(x))\n",
        "        else:\n",
        "            return self.bn(x)"
      ],
      "metadata": {
        "id": "hX8dMGurEDch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Regress(nn.Module):\n",
        "    \"\"\"Simple Linear Regression for hints\"\"\"\n",
        "    def __init__(self, dim_in=1024, dim_out=1024):\n",
        "        super(Regress, self).__init__()\n",
        "        self.linear = nn.Linear(dim_in, dim_out)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vEGz6LXGEDfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CAMKD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CAMKD, self).__init__()\n",
        "        # self.crit_ce = nn.CrossEntropyLoss()\n",
        "        self.crit_ce = nn.CrossEntropyLoss(reduction='none')\n",
        "        self.crit_mse = nn.MSELoss(reduction='none')\n",
        "        # self.crit_mse = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, trans_feat_s_list, mid_feat_t_list, output_feat_t_list, target):\n",
        "    \n",
        "        bsz = target.shape[0]\n",
        "        loss_t = [self.crit_ce(logit_t, target) for logit_t in output_feat_t_list]\n",
        "        num_teacher = len(trans_feat_s_list)\n",
        "        loss_t = torch.stack(loss_t, dim=0)\n",
        "        weight = (1.0 - F.softmax(loss_t, dim=0)) / (num_teacher - 1)\n",
        "        loss_st = []\n",
        "        for mid_feat_s, mid_feat_t in zip(trans_feat_s_list, mid_feat_t_list):\n",
        "            tmp_loss_st = self.crit_mse(mid_feat_s, mid_feat_t).reshape(bsz, -1).mean(-1)\n",
        "            loss_st.append(tmp_loss_st)\n",
        "        loss_st = torch.stack(loss_st, dim=0)\n",
        "        loss = torch.mul(weight, loss_st).sum()\n",
        "        # loss = torch.mul(attention, loss_st).sum()\n",
        "        loss /= (1.0*bsz*num_teacher)\n",
        "\n",
        "        # avg weight\n",
        "        # loss_st = []\n",
        "        # for mid_feat_s, mid_feat_t in zip(trans_feat_s_list, mid_feat_t_list):\n",
        "        #     tmp_loss_st = self.crit_mse(mid_feat_s, mid_feat_t)\n",
        "        #     loss_st.append(tmp_loss_st)\n",
        "        # loss_st = torch.stack(loss_st, dim=0)\n",
        "        # loss = loss_st.mean(0)\n",
        "        return loss, weight"
      ],
      "metadata": {
        "id": "ViRVAhpOcTpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimKD(nn.Module):\n",
        "    \"\"\"CVPR-2022: Knowledge Distillation with the Reused Teacher Classifier\"\"\"\n",
        "    def __init__(self, *, s_n, t_n, factor=2): \n",
        "        super(SimKD, self).__init__()\n",
        "       \n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))       \n",
        "\n",
        "        def conv1x1(in_channels, out_channels, stride=1):\n",
        "            return nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride, bias=False)\n",
        "        def conv3x3(in_channels, out_channels, stride=1, groups=1):\n",
        "            return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False, groups=groups)\n",
        "        \n",
        "        # A bottleneck design to reduce extra parameters\n",
        "        setattr(self, 'transfer', nn.Sequential(\n",
        "            conv1x1(s_n, t_n//factor),\n",
        "            nn.BatchNorm2d(t_n//factor),\n",
        "            nn.ReLU(inplace=True),\n",
        "            conv3x3(t_n//factor, t_n//factor),\n",
        "            # depthwise convolution\n",
        "            #conv3x3(t_n//factor, t_n//factor, groups=t_n//factor),\n",
        "            nn.BatchNorm2d(t_n//factor),\n",
        "            nn.ReLU(inplace=True),\n",
        "            conv1x1(t_n//factor, t_n),\n",
        "            nn.BatchNorm2d(t_n),\n",
        "            nn.ReLU(inplace=True),\n",
        "            ))\n",
        "        \n",
        "    def forward(self, feat_s, feat_t, cls_t):\n",
        "        \n",
        "        # Spatial Dimension Alignment\n",
        "        s_H, t_H = feat_s.shape[2], feat_t.shape[2]\n",
        "        if s_H > t_H:\n",
        "            source = F.adaptive_avg_pool2d(feat_s, (t_H, t_H))\n",
        "            target = feat_t\n",
        "        else:\n",
        "            source = feat_s\n",
        "            target = F.adaptive_avg_pool2d(feat_t, (s_H, s_H))\n",
        "        \n",
        "        trans_feat_t=target\n",
        "        \n",
        "        # Channel Alignment\n",
        "        trans_feat_s = getattr(self, 'transfer')(source)\n",
        "\n",
        "        # Prediction via Teacher Classifier\n",
        "        temp_feat = self.avg_pool(trans_feat_s)\n",
        "        temp_feat = temp_feat.view(temp_feat.size(0), -1)\n",
        "        pred_feat_s = cls_t(temp_feat)\n",
        "        \n",
        "        return trans_feat_s, trans_feat_t, pred_feat_s"
      ],
      "metadata": {
        "id": "Vj2JPfDMcTsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"validation\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input = input.float()\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # if idx % print_freq == 0:\n",
        "            #     print('Test: [{0}/{1}]\\t'\n",
        "            #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "            #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "            #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "            #            idx, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "            #            top1=top1, top5=top5))\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "Gf0_NwfLeDqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vanilla(epoch, train_loader, model, criterion, optimizer):\n",
        "    \"\"\"vanilla training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.float()\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # ===================forward=====================\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "\n",
        "        # ===================backward=====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ===================meters=====================\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # tensorboard logger\n",
        "        pass\n",
        "\n",
        "        # print info\n",
        "        # if idx % print_freq == 0:\n",
        "        #     print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "        #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "        #            epoch, idx, len(train_loader), batch_time=batch_time,\n",
        "        #            data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "        #     sys.stdout.flush()\n",
        "\n",
        "    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, losses.avg"
      ],
      "metadata": {
        "id": "9XxUlf35b8Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_distill(epoch, train_loader, test_loader, module_list, criterion_list, optimizer, distill):\n",
        "    \"\"\"One epoch distillation\"\"\"\n",
        "    # set modules as train()\n",
        "    for module in module_list:\n",
        "        module.train()\n",
        "    # set teacher as eval()\n",
        "    module_list[-1].eval()\n",
        "\n",
        "    criterion_cls = criterion_list[0]\n",
        "    criterion_div = criterion_list[1]\n",
        "    criterion_kd = criterion_list[2]\n",
        "\n",
        "    model_s = module_list[0]\n",
        "    model_t = module_list[-1]\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "\n",
        "    for idx, data in enumerate(train_loader):\n",
        "        input, target = data\n",
        "        input = input.float()\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # ===================forward=====================\n",
        "        preact = False\n",
        "\n",
        "        feat_s, logit_s = model_s(input, is_feat=True, preact=preact)\n",
        "        with torch.no_grad():\n",
        "            feat_t, logit_t = model_t(input, is_feat=True, preact=preact)\n",
        "            feat_t = [f.detach() for f in feat_t]\n",
        "\n",
        "        cls_t = model_t.get_feat_modules()[-1]\n",
        "        # cls + kl div\n",
        "        loss_cls = criterion_cls(logit_s, target)\n",
        "        loss_div = criterion_div(logit_s, logit_t)\n",
        "\n",
        "        # other kd beyond KL divergence\n",
        "        if distill == 'kd':\n",
        "            loss_kd = 0\n",
        "        elif distill == 'hint':\n",
        "            f_s = module_list[1](feat_s[1])\n",
        "            f_t = feat_t[1]\n",
        "            loss_kd = criterion_kd(f_s, f_t)\n",
        "        elif distill == 'crd':\n",
        "            f_s = feat_s[-1]\n",
        "            f_t = feat_t[-1]\n",
        "            loss_kd = criterion_kd(f_s, f_t, index, contrast_idx)\n",
        "        elif distill == 'attention':\n",
        "            g_s = feat_s[1:-1]\n",
        "            g_t = feat_t[1:-1]\n",
        "            loss_group = criterion_kd(g_s, g_t)\n",
        "            loss_kd = sum(loss_group)\n",
        "        elif distill == 'nst':\n",
        "            g_s = feat_s[1:-1]\n",
        "            g_t = feat_t[1:-1]\n",
        "            loss_group = criterion_kd(g_s, g_t)\n",
        "            loss_kd = sum(loss_group)\n",
        "        elif distill == 'similarity':\n",
        "            g_s = [feat_s[-2]]\n",
        "            g_t = [feat_t[-2]]\n",
        "            loss_group = criterion_kd(g_s, g_t)\n",
        "            loss_kd = sum(loss_group)\n",
        "        elif distill == 'rkd':\n",
        "            f_s = feat_s[-1]\n",
        "            f_t = feat_t[-1]\n",
        "            loss_kd = criterion_kd(f_s, f_t)\n",
        "        elif distill == 'pkt':\n",
        "            f_s = feat_s[-1]\n",
        "            f_t = feat_t[-1]\n",
        "            loss_kd = criterion_kd(f_s, f_t)\n",
        "        elif distill == 'kdsvd':\n",
        "            g_s = feat_s[1:-1]\n",
        "            g_t = feat_t[1:-1]\n",
        "            loss_group = criterion_kd(g_s, g_t)\n",
        "            loss_kd = sum(loss_group)\n",
        "        elif distill == 'correlation':\n",
        "            f_s = module_list[1](feat_s[-1])\n",
        "            f_t = module_list[2](feat_t[-1])\n",
        "            loss_kd = criterion_kd(f_s, f_t)\n",
        "        elif distill == 'vid':\n",
        "            g_s = feat_s[1:-1]\n",
        "            g_t = feat_t[1:-1]\n",
        "            loss_group = [c(f_s, f_t) for f_s, f_t, c in zip(g_s, g_t, criterion_kd)]\n",
        "            loss_kd = sum(loss_group)\n",
        "        elif distill == 'abound':\n",
        "            # can also add loss to this stage\n",
        "            loss_kd = 0\n",
        "        elif distill == 'fsp':\n",
        "            # can also add loss to this stage\n",
        "            loss_kd = 0\n",
        "        elif distill == 'factor':\n",
        "            factor_s = module_list[1](feat_s[-2])\n",
        "            factor_t = module_list[2](feat_t[-2], is_factor=True)\n",
        "            loss_kd = criterion_kd(factor_s, factor_t)\n",
        "        elif distill == 'simkd':\n",
        "            trans_feat_s, trans_feat_t, pred_feat_s = module_list[1](feat_s[-2], feat_t[-2], cls_t)\n",
        "            logit_s = pred_feat_s\n",
        "            loss_kd = criterion_kd(trans_feat_s, trans_feat_t)\n",
        "        else:\n",
        "            raise NotImplementedError(distill)\n",
        "\n",
        "        loss = alpha * loss_cls + beta * loss_div + gamma * loss_kd\n",
        "\n",
        "        acc1, acc5 = accuracy(logit_s, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "        # ===================backward=====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print info\n",
        "        # if idx % print_freq == 0:\n",
        "        #     print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "        #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "        #         epoch, idx, len(train_loader), batch_time=batch_time,\n",
        "        #         data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "        #     sys.stdout.flush()\n",
        "\n",
        "        # print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "        #       .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, losses.avg"
      ],
      "metadata": {
        "id": "a18201HQEDwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_teacher(train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, cfg=t_cfg, fc=t_fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        # logger.log_value('train_acc', train_acc, epoch)\n",
        "        # logger.log_value('train_loss', train_loss, epoch)\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        # logger.log_value('test_acc', test_acc, epoch)\n",
        "        # logger.log_value('test_acc_top5', test_acc_top5, epoch)\n",
        "        # logger.log_value('test_loss', test_loss, epoch)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './teacher/t_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./teacher/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './teacher/t_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "6fCvtL5xctma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_student(train_loader, val_loader, distill, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # # tensorboard logger\n",
        "    # logger = tb_logger.Logger(logdir='./student/student_tensorboard', flush_secs=2)\n",
        "    # model\n",
        "    model_t = load_teacher()\n",
        "    model_s = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "\n",
        "    data = torch.randn(2, num_components, window, window)\n",
        "    model_t.eval()\n",
        "    model_s.eval()\n",
        "    feat_t, _ = model_t(data, is_feat=True)\n",
        "    feat_s, _ = model_s(data, is_feat=True)\n",
        "\n",
        "    module_list = nn.ModuleList([])\n",
        "    module_list.append(model_s)\n",
        "    trainable_list = nn.ModuleList([])\n",
        "    trainable_list.append(model_s)\n",
        "\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_div = DistillKL(kd_T)\n",
        "    if distill == 'kd':\n",
        "        criterion_kd = DistillKL(kd_T)\n",
        "    elif distill == 'hint':\n",
        "        criterion_kd = HintLoss()\n",
        "        regress_s = ConvReg(feat_s[1].shape, feat_t[1].shape)\n",
        "        module_list.append(regress_s)\n",
        "        trainable_list.append(regress_s)\n",
        "    elif distill == 'crd':\n",
        "        s_dim = feat_s[-1].shape[1]\n",
        "        t_dim = feat_t[-1].shape[1]\n",
        "        n_data = n_data\n",
        "        criterion_kd = CRDLoss(opt)\n",
        "        module_list.append(criterion_kd.embed_s)\n",
        "        module_list.append(criterion_kd.embed_t)\n",
        "        trainable_list.append(criterion_kd.embed_s)\n",
        "        trainable_list.append(criterion_kd.embed_t)\n",
        "    elif distill == 'attention':\n",
        "        criterion_kd = Attention()\n",
        "    elif distill == 'nst':\n",
        "        criterion_kd = NSTLoss()\n",
        "    elif distill == 'similarity':\n",
        "        criterion_kd = Similarity()\n",
        "    elif distill == 'rkd':\n",
        "        criterion_kd = RKDLoss()\n",
        "    elif distill == 'pkt':\n",
        "        criterion_kd = PKT()\n",
        "    elif distill == 'kdsvd':\n",
        "        criterion_kd = KDSVD()\n",
        "    elif distill == 'correlation':\n",
        "        criterion_kd = Correlation()\n",
        "        embed_s = LinearEmbed(feat_s[-1].shape[1], feat_dim)\n",
        "        embed_t = LinearEmbed(feat_t[-1].shape[1], feat_dim)\n",
        "        module_list.append(embed_s)\n",
        "        module_list.append(embed_t)\n",
        "        trainable_list.append(embed_s)\n",
        "        trainable_list.append(embed_t)\n",
        "    elif distill == 'vid':\n",
        "        s_n = [f.shape[1] for f in feat_s[1:-1]]\n",
        "        t_n = [f.shape[1] for f in feat_t[1:-1]]\n",
        "        criterion_kd = nn.ModuleList(\n",
        "            [VIDLoss(s, t, t) for s, t in zip(s_n, t_n)]\n",
        "        )\n",
        "        # add this as some parameters in VIDLoss need to be updated\n",
        "        trainable_list.append(criterion_kd)\n",
        "    elif distill == 'abound':\n",
        "        s_shapes = [f.shape for f in feat_s[1:-1]]\n",
        "        t_shapes = [f.shape for f in feat_t[1:-1]]\n",
        "        connector = Connector(s_shapes, t_shapes)\n",
        "        # init stage training\n",
        "        init_trainable_list = nn.ModuleList([])\n",
        "        init_trainable_list.append(connector)\n",
        "        init_trainable_list.append(model_s.get_feat_modules())\n",
        "        criterion_kd = ABLoss(len(feat_s[1:-1]))\n",
        "        init(model_s, model_t, init_trainable_list, criterion_kd, train_loader, logger, distill)\n",
        "        # classification\n",
        "        module_list.append(connector)\n",
        "    elif distill == 'factor':\n",
        "        s_shape = feat_s[-2].shape\n",
        "        t_shape = feat_t[-2].shape\n",
        "        paraphraser = Paraphraser(t_shape)\n",
        "        translator = Translator(s_shape, t_shape)\n",
        "        # init stage training\n",
        "        init_trainable_list = nn.ModuleList([])\n",
        "        init_trainable_list.append(paraphraser)\n",
        "        criterion_init = nn.MSELoss()\n",
        "        init(model_s, model_t, init_trainable_list, criterion_init, train_loader, logger, distill)\n",
        "        # classification\n",
        "        criterion_kd = FactorTransfer()\n",
        "        module_list.append(translator)\n",
        "        module_list.append(paraphraser)\n",
        "        trainable_list.append(translator)\n",
        "    elif distill == 'fsp':\n",
        "        s_shapes = [s.shape for s in feat_s[:-1]]\n",
        "        t_shapes = [t.shape for t in feat_t[:-1]]\n",
        "        criterion_kd = FSP(s_shapes, t_shapes)\n",
        "        # init stage training\n",
        "        init_trainable_list = nn.ModuleList([])\n",
        "        init_trainable_list.append(model_s.get_feat_modules())\n",
        "        init(model_s, model_t, init_trainable_list, criterion_kd, train_loader, logger, distill)\n",
        "        # classification training\n",
        "        pass\n",
        "    elif distill == 'simkd':\n",
        "        s_n = feat_s[-2].shape[1]\n",
        "        t_n = feat_t[-2].shape[1]\n",
        "        model_simkd = SimKD(s_n= s_n, t_n=t_n, factor=factor)\n",
        "        criterion_kd = nn.MSELoss()\n",
        "        module_list.append(model_simkd)\n",
        "        trainable_list.append(model_simkd)\n",
        "    else:\n",
        "        raise NotImplementedError(distill)\n",
        "\n",
        "    criterion_list = nn.ModuleList([])\n",
        "    criterion_list.append(criterion_cls)    # classification loss\n",
        "    criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
        "    criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(trainable_list.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    # append teacher after optimizer to avoid weight_decay\n",
        "    module_list.append(model_t)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        module_list.cuda()\n",
        "        criterion_list.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # validate teacher accuracy\n",
        "    teacher_acc, _, _ = validate(val_loader, model_t, criterion_cls)\n",
        "    print('teacher accuracy: ', teacher_acc)\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_distill(epoch, train_loader, val_loader, module_list, criterion_list, optimizer, distill)\n",
        "\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        # logger.log_value('train_acc', train_acc, epoch)\n",
        "        # logger.log_value('train_loss', train_loss, epoch)\n",
        "\n",
        "        test_acc, tect_acc_top5, test_loss = validate(val_loader, model_s, criterion_cls)\n",
        "\n",
        "        # logger.log_value('test_acc', test_acc, epoch)\n",
        "        # logger.log_value('test_loss', test_loss, epoch)\n",
        "        # logger.log_value('test_acc_top5', tect_acc_top5, epoch)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model_s.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "            }\n",
        "            save_file = './student/s_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model_s.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "            }\n",
        "            save_file = os.path.join('./student/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch. \n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model_s.state_dict(),\n",
        "    }\n",
        "    save_file = './student/s_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "ZSAP7OlJYT1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_teacher(model_path='./teacher/t_best.pth'):\n",
        "    print('==> loading teacher model')\n",
        "    model = cnn2d(num_components, num_classes, cfg=t_cfg, fc=t_fc)\n",
        "    model.load_state_dict(torch.load(model_path)['model'])\n",
        "    print('==> done')\n",
        "    return model"
      ],
      "metadata": {
        "id": "t0jLEVNJ4jQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_teacher(train_dataloader, val_dataloader, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82v7LjVBYT3d",
        "outputId": "1e3cad81-03a0-45ec-d92a-34bcb48f4465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * Acc@1 47.030 Acc@5 83.964\n",
            "epoch 1, total time 8.82\n",
            " * Acc@1 63.667 Acc@5 91.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 78.434 Acc@5 96.919\n",
            "epoch 2, total time 0.16\n",
            " * Acc@1 89.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 90.757 Acc@5 99.926\n",
            "epoch 3, total time 0.16\n",
            " * Acc@1 93.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 95.063 Acc@5 100.000\n",
            "epoch 4, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 97.587 Acc@5 100.000\n",
            "epoch 5, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 6, total time 0.16\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 7, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 9, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 10, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 11, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 12, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 13, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.369 Acc@5 100.000\n",
            "epoch 14, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.295 Acc@5 100.000\n",
            "epoch 15, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 16, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 18, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 19, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 20, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 21, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 22, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 23, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 24, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 25, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 28, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 29, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 30, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 31, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 32, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 33, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 34, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 35, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 36, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 98.924 Acc@5 100.000\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 38, total time 0.17\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 39, total time 0.18\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 40, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 41, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 42, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 43, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 44, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 45, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 46, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 47, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 48, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 49, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 50, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 51, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.220 Acc@5 100.000\n",
            "epoch 52, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 53, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 54, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 55, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 56, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 57, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 58, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 59, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 60, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 61, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 62, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 63, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 64, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 65, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 66, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 67, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 68, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 69, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 70, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 71, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 73, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 74, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 76, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 78, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 79, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 80, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 81, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 82, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 87, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 88, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 89, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 91, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 92, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 93, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 94, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 95, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 96, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 97, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 98, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 99, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 100, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_student(train_dataloader, val_dataloader, distill=kd_method, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38VVXk5qYT56",
        "outputId": "ef5c08ab-1596-46ff-82a1-4116bcd54720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> loading teacher model\n",
            "==> done\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "teacher accuracy:  tensor(99.6667, device='cuda:0')\n",
            "==> training...\n",
            "epoch 1, total time 0.43\n",
            " * Acc@1 40.667 Acc@5 69.333\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 2, total time 0.23\n",
            " * Acc@1 42.000 Acc@5 79.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 3, total time 0.22\n",
            " * Acc@1 55.333 Acc@5 88.333\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 4, total time 0.22\n",
            " * Acc@1 62.000 Acc@5 92.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 5, total time 0.22\n",
            " * Acc@1 71.333 Acc@5 96.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 6, total time 0.22\n",
            " * Acc@1 72.667 Acc@5 96.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 7, total time 0.22\n",
            " * Acc@1 73.000 Acc@5 96.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 8, total time 0.22\n",
            " * Acc@1 76.333 Acc@5 97.333\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 9, total time 0.22\n",
            " * Acc@1 79.333 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 10, total time 0.22\n",
            " * Acc@1 81.000 Acc@5 98.333\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 11, total time 0.21\n",
            " * Acc@1 82.000 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 12, total time 0.23\n",
            " * Acc@1 82.333 Acc@5 99.333\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 13, total time 0.22\n",
            " * Acc@1 87.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 14, total time 0.21\n",
            " * Acc@1 92.000 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 15, total time 0.21\n",
            " * Acc@1 94.333 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 16, total time 0.23\n",
            " * Acc@1 94.333 Acc@5 99.667\n",
            "==> training...\n",
            "epoch 17, total time 0.22\n",
            " * Acc@1 94.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 18, total time 0.21\n",
            " * Acc@1 95.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 19, total time 0.22\n",
            " * Acc@1 94.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 20, total time 0.22\n",
            " * Acc@1 95.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 21, total time 0.22\n",
            " * Acc@1 96.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 22, total time 0.21\n",
            " * Acc@1 96.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 23, total time 0.21\n",
            " * Acc@1 96.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 24, total time 0.21\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 25, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 26, total time 0.22\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 27, total time 0.24\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 28, total time 0.23\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 29, total time 0.25\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 30, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 31, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 32, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 33, total time 0.24\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 34, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 35, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 36, total time 0.24\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 37, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 38, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 39, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 40, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "epoch 41, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 42, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 43, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 44, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 45, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 46, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 47, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 48, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 49, total time 0.23\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 50, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 51, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 52, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 53, total time 0.21\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 54, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 55, total time 0.21\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 56, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 57, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 58, total time 0.21\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 59, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 60, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 61, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 62, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 63, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 64, total time 0.21\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 65, total time 0.21\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 66, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 67, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 68, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 69, total time 0.21\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 70, total time 0.21\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 71, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 72, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 73, total time 0.21\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 74, total time 0.21\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 75, total time 0.22\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 76, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 77, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 78, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 79, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 80, total time 0.24\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "epoch 81, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 82, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 83, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 84, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 85, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 86, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 87, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 88, total time 0.24\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 89, total time 0.24\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 90, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 91, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 92, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 93, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 94, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 95, total time 0.21\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 96, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 97, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 98, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 99, total time 0.23\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 100, total time 0.22\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "best accuracy: tensor(98.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_model = cnn2d(num_components, num_classes, cfg=t_cfg, fc=t_fc)\n",
        "t_model.load_state_dict(torch.load('./teacher/t_best.pth')['model'])\n",
        "\n",
        "s_model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "s_model.load_state_dict(torch.load('./student/s_best.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvsgIamvYT8W",
        "outputId": "9b5a502d-4f98-48ca-ef5c-96b8b2d135f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "t_model = t_model.to(cuda_device)\n",
        "s_model = s_model.to(cuda_device)\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, t_model, criterion)\n",
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, s_model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf_ZurazYT_H",
        "outputId": "4b5f131d-57af-4eed-f375-8f0eb430e7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 99.314 Acc@5 99.997\n",
            " * Acc@1 96.853 Acc@5 99.915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Student top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(t_acc_5))\n",
        "print(\"Student top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAkh-bgWYUBb",
        "outputId": "0694b96e-383b-4a18-ef1e-5315d642f068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 99.314\n",
            "Student top-1 test accuracy: 96.853\n",
            "Teacher top-5 test accuracy: 99.997\n",
            "Student top-5 test accuracy: 99.915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of teacher parameters:', sum(p.numel() for p in t_model.parameters() if p.requires_grad))\n",
        "print('Num of student parameters:', sum(p.numel() for p in s_model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyXyGa8fYUEE",
        "outputId": "dbb9d7c6-a4b8-4ea5-a984-68da7c46ed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of teacher parameters: 426159\n",
            "Num of student parameters: 8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size teacher(MB):', os.path.getsize(\"./teacher/t_best.pth\")/1e6)\n",
        "print('Size student(MB):', os.path.getsize(\"./student/s_best.pth\")/1e6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhfaeKEDYUGg",
        "outputId": "c593592e-e6da-44b5-f285-543e772f388e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size teacher(MB): 5.122847\n",
            "Size student(MB): 0.0384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in s_model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EUsPVs1sO7y",
        "outputId": "4fd339ad-eb65-4459-99ec-f3a975a571d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 5000\n",
            "conv1.bias 5\n",
            "conv2.weight 1250\n",
            "conv2.bias 10\n",
            "fc1.weight 2500\n",
            "fc1.bias 10\n",
            "fc2.weight 90\n",
            "fc2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Offline Distillation: multi-teacher (CA-MKD)**\n",
        "\n"
      ],
      "metadata": {
        "id": "-HNxjGjN2sUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Official GitHub: https://github.com/Rorozhl/CA-MKD\n",
        "\n",
        "num_teachers = 5\n",
        "factor = 2\n",
        "kd_T = 10 # temperature of soft target method\n",
        "alpha = 1\n",
        "beta = 1\n",
        "gamma = 50\n",
        "learning_rate = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.89  # prune ratio of conv layers\n",
        "linear_r = 0.89 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "uhVHgHsr-QsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillKL(nn.Module):\n",
        "    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n",
        "    def __init__(self, T):\n",
        "        super(DistillKL, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "    def forward(self, y_s, y_t, is_ca=False):\n",
        "        p_s = F.log_softmax(y_s/self.T, dim=1)\n",
        "        p_t = F.softmax(y_t/self.T, dim=1)\n",
        "        if is_ca: \n",
        "            loss = (nn.KLDivLoss(reduction='none')(p_s, p_t) * (self.T**2)).sum(-1)\n",
        "        else:\n",
        "            loss = nn.KLDivLoss(reduction='batchmean')(p_s, p_t) * (self.T**2)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "CbjJDnKqn72T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "    \"\"\"Embedding module\"\"\"\n",
        "    def __init__(self, dim_in=1024, dim_out=128, factor=2, convs=False):\n",
        "        super(Embed, self).__init__()\n",
        "        self.convs = convs\n",
        "        if self.convs:\n",
        "            self.transfer = nn.Sequential(\n",
        "                nn.Conv2d(dim_in, dim_in//factor, kernel_size=1),\n",
        "                nn.BatchNorm2d(dim_in//factor),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(dim_in//factor, dim_in//factor, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(dim_in//factor),\n",
        "                nn.ReLU(inplace=True), \n",
        "                nn.Conv2d(dim_in//factor, dim_out, kernel_size=1),\n",
        "                nn.BatchNorm2d(dim_out),\n",
        "                nn.ReLU(inplace=True)              \n",
        "            )\n",
        "        else:\n",
        "            self.transfer = nn.Sequential(\n",
        "                nn.Conv2d(dim_in, dim_out, kernel_size=1),\n",
        "                nn.BatchNorm2d(dim_out),\n",
        "                nn.ReLU(inplace=True) \n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transfer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dHvy9Pt4BohT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CalWeight(nn.Module):\n",
        "    def __init__(self, feat_s, feat_t_list):\n",
        "        super(CalWeight, self).__init__()\n",
        "\n",
        "        # student和teacher都用最后一层\n",
        "        s_channel = feat_s.shape[1]\n",
        "        for i in range(len(feat_t_list)):\n",
        "            t_channel = feat_t_list[i].shape[1]\n",
        "            setattr(self, 'embed'+str(i), Embed(s_channel, t_channel, factor, False))\n",
        "\n",
        "\n",
        "    def forward(self, feat_s, feat_t_list, model_t_list=None):\n",
        "        tmp_model = [model_t.distill_seq() for model_t in model_t_list]\n",
        "        trans_feat_s_list = []\n",
        "        output_feat_t_list = []\n",
        "        s_H = feat_s.shape[2]\n",
        "        for i, mid_feat_t in enumerate(feat_t_list):\n",
        "            t_H = mid_feat_t.shape[2]\n",
        "            if s_H >= t_H:\n",
        "                feat_s = F.adaptive_avg_pool2d(feat_s, (t_H, t_H))\n",
        "            else:\n",
        "                feat_s = F.interpolate(feat_s, size=(t_H, t_H), mode='bilinear')\n",
        "            trans_feat_s = getattr(self, 'embed'+str(i))(feat_s)\n",
        "            trans_feat_s_list.append(trans_feat_s)\n",
        "\n",
        "            output_feat_t = tmp_model[i][-1](trans_feat_s)\n",
        "            output_feat_t_list.append(output_feat_t)\n",
        "        return trans_feat_s_list, output_feat_t_list"
      ],
      "metadata": {
        "id": "7GQMAEzz_57-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_teacher(train_loader, val_loader, epochs, fname):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, cfg=t_cfg, fc=t_fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './teacher/' + fname\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./teacher/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './teacher/t_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "p4OdTiIM2w7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_distill_multi_teacher(epoch, train_loader, test_loader, module_list, criterion_list, optimizer, distill):\n",
        "    \"\"\"One epoch distillation\"\"\"\n",
        "    # set modules as train()\n",
        "    for module in module_list:\n",
        "        module.train()\n",
        "    # set teacher as eval()\n",
        "    [model_t.eval() for model_t in module_list[-num_teachers:]]\n",
        "\n",
        "    criterion_cls = criterion_list[0]\n",
        "    criterion_div = criterion_list[1]\n",
        "    criterion_kd = criterion_list[2]\n",
        "\n",
        "    model_s = module_list[0]\n",
        "    model_t_list = module_list[-num_teachers:]\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "\n",
        "    for idx, data in enumerate(train_loader):\n",
        "        input, target = data\n",
        "        input = input.float()\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # ===================forward=====================\n",
        "        preact = False\n",
        "\n",
        "        feat_s, logit_s = model_s(input, is_feat=True, preact=preact)\n",
        "\n",
        "        feat_t_list = []\n",
        "        logit_t_list = []\n",
        "        with torch.no_grad():\n",
        "            for model_t in model_t_list:\n",
        "                feat_t, logit_t = model_t(input, is_feat=True, preact=preact)\n",
        "                feat_t = [f.detach() for f in feat_t]\n",
        "                feat_t_list.append(feat_t)\n",
        "                logit_t_list.append(logit_t)\n",
        "\n",
        "\n",
        "        # cls + kl div\n",
        "        loss_cls = criterion_cls(logit_s, target)\n",
        "        # loss_div = criterion_div(logit_s, logit_t)\n",
        "\n",
        "        criterion_cls_lc = nn.CrossEntropyLoss(reduction='none')\n",
        "        loss_t_list = [criterion_cls_lc(logit_t, target) for logit_t in logit_t_list]\n",
        "        loss_t = torch.stack(loss_t_list, dim=0)\n",
        "        attention = (1.0 - F.softmax(loss_t, dim=0)) / (num_teachers - 1)\n",
        "        loss_div_list = [criterion_div(logit_s, logit_t, is_ca=True)\n",
        "                          for logit_t in logit_t_list]\n",
        "        loss_div = torch.stack(loss_div_list, dim=0)\n",
        "        bsz = loss_div.shape[1]\n",
        " \n",
        "        loss_div = (torch.mul(attention, loss_div).sum()) / (1.0*bsz*num_teachers) \n",
        "\n",
        "        mid_feat_t_list = [feat_t[-2] for feat_t in feat_t_list]\n",
        "\n",
        "        trans_feat_s_list, output_feat_t_list = module_list[1](feat_s[-2], mid_feat_t_list, model_t_list)\n",
        "\n",
        "        loss_kd, weight = criterion_kd(trans_feat_s_list, mid_feat_t_list, output_feat_t_list, target)   \n",
        "\n",
        "        loss = alpha * loss_cls + beta * loss_div + gamma * loss_kd\n",
        "\n",
        "        acc1, acc5 = accuracy(logit_s, target, topk=(1, 5))\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "        # ===================backward=====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print info\n",
        "        # if idx % print_freq == 0:\n",
        "        #     print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "        #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "        #         epoch, idx, len(train_loader), batch_time=batch_time,\n",
        "        #         data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "        #     sys.stdout.flush()\n",
        "\n",
        "        # print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "        #       .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, losses.avg"
      ],
      "metadata": {
        "id": "WIAojZV48Cye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_teachers):\n",
        "    file_name = 't_best_' + str(i) + '.pth'\n",
        "    train_teacher(train_dataloader, val_dataloader, epochs=200, fname=file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TscD8M2w9x",
        "outputId": "f461985d-850a-4ad8-d22d-ed4258a1ad02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * Acc@1 44.803 Acc@5 86.266\n",
            "epoch 1, total time 0.21\n",
            " * Acc@1 52.333 Acc@5 94.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 73.274 Acc@5 97.068\n",
            "epoch 2, total time 0.16\n",
            " * Acc@1 89.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 90.349 Acc@5 100.000\n",
            "epoch 3, total time 0.17\n",
            " * Acc@1 93.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 95.137 Acc@5 100.000\n",
            "epoch 4, total time 0.16\n",
            " * Acc@1 96.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 97.550 Acc@5 100.000\n",
            "epoch 5, total time 0.16\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.441 Acc@5 100.000\n",
            "epoch 6, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.220 Acc@5 100.000\n",
            "epoch 7, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 9, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 10, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 11, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 12, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 13, total time 0.15\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 14, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 15, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 16, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 18, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 19, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 20, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 21, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 22, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 23, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.406 Acc@5 100.000\n",
            "epoch 24, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 25, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 28, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 29, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 30, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 31, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 32, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 33, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 34, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 35, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 36, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 38, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 39, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 40, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 41, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 42, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 43, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 44, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 45, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 46, total time 0.15\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 47, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 48, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 49, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 50, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 51, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 52, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 53, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 54, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 55, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 56, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 57, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 58, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 59, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 60, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 61, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 62, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 63, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 64, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 65, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 66, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 67, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 68, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 69, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 70, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 71, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 73, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 74, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 76, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 78, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 79, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 80, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 81, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 82, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 87, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 88, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 89, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 91, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 92, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 93, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 94, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 95, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 96, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 97, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 98, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 99, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 100, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 101, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 102, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 103, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 104, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 105, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 106, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 107, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 108, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 109, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 110, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 111, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 112, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 113, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 114, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 115, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 116, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 117, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 118, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 119, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 120, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 121, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 122, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 123, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 124, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 125, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 126, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 127, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 128, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 129, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 130, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 131, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 132, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 133, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 134, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 135, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 136, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 137, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 138, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 139, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 140, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 141, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 142, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 143, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 144, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 145, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 146, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 147, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 148, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 149, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 150, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 151, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 152, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 153, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 154, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 155, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 156, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 157, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 158, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 159, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 160, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 161, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 162, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 163, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 164, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 165, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 166, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 167, total time 0.18\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 168, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 169, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 170, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 171, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 172, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 173, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 174, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 175, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 176, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 177, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 178, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 179, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 180, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 181, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 182, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 183, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 184, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 185, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 186, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 187, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 188, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 189, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 190, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 191, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 192, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 193, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 194, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 195, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 196, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 197, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 198, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 199, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 200, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n",
            "==> training...\n",
            " * Acc@1 47.587 Acc@5 85.189\n",
            "epoch 1, total time 0.17\n",
            " * Acc@1 62.000 Acc@5 96.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 81.774 Acc@5 98.886\n",
            "epoch 2, total time 0.16\n",
            " * Acc@1 91.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 91.759 Acc@5 100.000\n",
            "epoch 3, total time 0.16\n",
            " * Acc@1 94.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 95.843 Acc@5 100.000\n",
            "epoch 4, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.367 Acc@5 100.000\n",
            "epoch 5, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 6, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 7, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 9, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 10, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.332 Acc@5 100.000\n",
            "epoch 11, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 12, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 13, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 14, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 15, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 16, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 18, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 19, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 20, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 21, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 22, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 23, total time 0.15\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 24, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 25, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 28, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 29, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 30, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 31, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 32, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 33, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 34, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 35, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 36, total time 0.17\n",
            " * Acc@1 97.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 38, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 39, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 40, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 41, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 42, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 43, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 44, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 45, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 46, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 47, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 48, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.220 Acc@5 100.000\n",
            "epoch 49, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 50, total time 0.15\n",
            " * Acc@1 98.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 51, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 52, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 53, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 54, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 55, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 56, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 57, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 58, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 59, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 60, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 61, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 62, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 63, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 64, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 65, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 66, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 67, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 68, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 69, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 70, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 71, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 73, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 74, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 76, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 78, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 79, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 80, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 81, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 82, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 87, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 88, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 89, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 91, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 92, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 93, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 94, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 95, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 96, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 97, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 98, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 99, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 100, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 101, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 102, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 103, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 104, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 105, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 106, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 107, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 108, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 109, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 110, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 111, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 112, total time 0.15\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 113, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 114, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 115, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 116, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 117, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 118, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 119, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 120, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 121, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 122, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 123, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 124, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 125, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 126, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 127, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 128, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 129, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 130, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 131, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 132, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 133, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 134, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 135, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 136, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 137, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 138, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 139, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 140, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 141, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 142, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 143, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 144, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 145, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 146, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 147, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 148, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 149, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 150, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 151, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 152, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 153, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 154, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 155, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 156, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 157, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 158, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 159, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 160, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 161, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 162, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 163, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 164, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 165, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 166, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 167, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 168, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 169, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 170, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 171, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 172, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 173, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 174, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 175, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 176, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 177, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 178, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 179, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 180, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 181, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 182, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 183, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 184, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 185, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 186, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 187, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 188, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 189, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 190, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 191, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 192, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 193, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 194, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 195, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 196, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 197, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 198, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 199, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 200, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n",
            "==> training...\n",
            " * Acc@1 49.183 Acc@5 85.857\n",
            "epoch 1, total time 0.16\n",
            " * Acc@1 63.667 Acc@5 93.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 81.477 Acc@5 98.255\n",
            "epoch 2, total time 0.16\n",
            " * Acc@1 88.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 91.685 Acc@5 100.000\n",
            "epoch 3, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 96.362 Acc@5 100.000\n",
            "epoch 4, total time 0.15\n",
            " * Acc@1 97.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.181 Acc@5 100.000\n",
            "epoch 5, total time 0.16\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 6, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 7, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 9, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 10, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.369 Acc@5 100.000\n",
            "epoch 11, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 12, total time 0.15\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 13, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 14, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 15, total time 0.15\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 16, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 18, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 19, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 20, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 21, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 22, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 23, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 24, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.295 Acc@5 100.000\n",
            "epoch 25, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.480 Acc@5 100.000\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.406 Acc@5 100.000\n",
            "epoch 28, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.406 Acc@5 100.000\n",
            "epoch 29, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 30, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 31, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 32, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 33, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 34, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 35, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 36, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 38, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 39, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 40, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 41, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 42, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 43, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 44, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 45, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 46, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 47, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 48, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 49, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 50, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 51, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 52, total time 0.18\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 53, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 54, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 55, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 56, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 57, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 58, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.369 Acc@5 100.000\n",
            "epoch 59, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 60, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 61, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 62, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 63, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 64, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 65, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 66, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 67, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 68, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 69, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 70, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 71, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 73, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 74, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 76, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 98.924 Acc@5 100.000\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 78, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 79, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 80, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 81, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 82, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 87, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 88, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 89, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 91, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 92, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 93, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 94, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 95, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 96, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 97, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 98, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 99, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 100, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 101, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 102, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 103, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 104, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 105, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 106, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 107, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 108, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 109, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 110, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 111, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 112, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 113, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 114, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 115, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 116, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 117, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 118, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 119, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 120, total time 0.18\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 121, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 122, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 123, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 124, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 125, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 126, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 127, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 128, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 129, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 130, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 131, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 132, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 133, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 134, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 135, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 136, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 137, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 138, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.406 Acc@5 100.000\n",
            "epoch 139, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 140, total time 0.16\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 141, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 142, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 143, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 144, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 145, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 146, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 147, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 148, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 149, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 150, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 151, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 152, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 153, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 154, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 155, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 156, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 157, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 158, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 159, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 160, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 161, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 162, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 163, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 164, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 165, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 166, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 167, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 168, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 169, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 170, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 171, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 172, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 173, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 174, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 175, total time 0.18\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 176, total time 0.27\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 177, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 178, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 179, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 180, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 181, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 182, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 183, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 184, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 185, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 186, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 187, total time 0.18\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 188, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 189, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 190, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 191, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 192, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 193, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 194, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 195, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 196, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 197, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 198, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 199, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 200, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(100., device='cuda:0')\n",
            "==> training...\n",
            " * Acc@1 47.253 Acc@5 84.707\n",
            "epoch 1, total time 0.17\n",
            " * Acc@1 62.667 Acc@5 89.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 77.283 Acc@5 97.142\n",
            "epoch 2, total time 0.16\n",
            " * Acc@1 90.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 90.535 Acc@5 100.000\n",
            "epoch 3, total time 0.16\n",
            " * Acc@1 94.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 95.805 Acc@5 100.000\n",
            "epoch 4, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.107 Acc@5 100.000\n",
            "epoch 5, total time 0.17\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.998 Acc@5 100.000\n",
            "epoch 6, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.406 Acc@5 100.000\n",
            "epoch 7, total time 0.16\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 9, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 10, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 11, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 12, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 13, total time 0.17\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 14, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 15, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 16, total time 0.17\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 18, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 19, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 20, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 21, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 22, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 23, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 24, total time 0.18\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 25, total time 0.24\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 28, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 29, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 30, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 31, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 32, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 33, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 34, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 35, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 36, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 38, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 39, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 40, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 41, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 42, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 43, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 44, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 45, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 46, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 47, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 48, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 49, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 98.886 Acc@5 100.000\n",
            "epoch 50, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.332 Acc@5 100.000\n",
            "epoch 51, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 52, total time 0.15\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 53, total time 0.16\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 54, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 55, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 56, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 57, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 58, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 59, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 60, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 61, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 62, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 63, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 64, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 65, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 66, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 67, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 68, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 69, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 70, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 71, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 73, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 74, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 76, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 78, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 79, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 80, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 81, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 82, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 87, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 88, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 89, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 91, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 92, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 93, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 94, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 95, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 96, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 97, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 98, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 99, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 100, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 101, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 102, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 103, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 104, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 105, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 106, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 107, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 108, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 109, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 110, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 111, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 112, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 113, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 114, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 115, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 116, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 117, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 118, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 119, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 120, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 121, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 122, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 123, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 124, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 125, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 126, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 127, total time 0.18\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 128, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 129, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 130, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 131, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 132, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 133, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 134, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 135, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 136, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 137, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 138, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 139, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 140, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 141, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 142, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 143, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 144, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 145, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 146, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 147, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 148, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 149, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 150, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 151, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 152, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 153, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 154, total time 0.15\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 155, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.592 Acc@5 100.000\n",
            "epoch 156, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 157, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 158, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 159, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 160, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 161, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 162, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 163, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 164, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 165, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 166, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 167, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 168, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 169, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 170, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 171, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 172, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 173, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 174, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 175, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 176, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 177, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 178, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 179, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 180, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 181, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 182, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 183, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 184, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 185, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 186, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 187, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 188, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 189, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 190, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 191, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 192, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 193, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 194, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 195, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 196, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 197, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 198, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 199, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 200, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n",
            "==> training...\n",
            " * Acc@1 49.852 Acc@5 86.006\n",
            "epoch 1, total time 0.16\n",
            " * Acc@1 69.333 Acc@5 91.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 79.473 Acc@5 97.550\n",
            "epoch 2, total time 0.17\n",
            " * Acc@1 90.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 92.910 Acc@5 99.963\n",
            "epoch 3, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 96.251 Acc@5 100.000\n",
            "epoch 4, total time 0.16\n",
            " * Acc@1 96.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 5, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 6, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.146 Acc@5 100.000\n",
            "epoch 7, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.332 Acc@5 100.000\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 9, total time 0.17\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 10, total time 0.17\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 11, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 12, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 13, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 14, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.555 Acc@5 100.000\n",
            "epoch 15, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 16, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.406 Acc@5 100.000\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 18, total time 0.16\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 19, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 20, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 21, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 22, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 23, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 24, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 25, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 28, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 29, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 30, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 31, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 32, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 33, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.443 Acc@5 100.000\n",
            "epoch 34, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 98.627 Acc@5 100.000\n",
            "epoch 35, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.517 Acc@5 100.000\n",
            "epoch 36, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.629 Acc@5 100.000\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.740 Acc@5 100.000\n",
            "epoch 38, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 39, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 40, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 41, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 42, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 43, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 44, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 45, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 46, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 47, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 48, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 49, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 50, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 51, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 52, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 53, total time 0.18\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 54, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 55, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 56, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 57, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 58, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 59, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 60, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 61, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 62, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 63, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 64, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 65, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 66, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 67, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 68, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 69, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 70, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 71, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 73, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 74, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.703 Acc@5 100.000\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 76, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.777 Acc@5 100.000\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 78, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 79, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 80, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.814 Acc@5 100.000\n",
            "epoch 81, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 82, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 87, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 88, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 89, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 91, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 92, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 93, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 94, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 95, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 96, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 97, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 98, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 99, total time 0.17\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 100, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 101, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 102, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 103, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 104, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 105, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 106, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 107, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 108, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 109, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 110, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 111, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 112, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 113, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 114, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 115, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 116, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 117, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 118, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 119, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 120, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 121, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 122, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 123, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 124, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 125, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 126, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 127, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 128, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 129, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 130, total time 0.15\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 131, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 132, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 133, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 134, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 135, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 136, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 137, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 138, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 139, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 140, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 141, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 142, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 143, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 144, total time 0.17\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 145, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.666 Acc@5 100.000\n",
            "epoch 146, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 147, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 148, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 149, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 150, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 151, total time 0.17\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 152, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 153, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 154, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 155, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.852 Acc@5 100.000\n",
            "epoch 156, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 157, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.889 Acc@5 100.000\n",
            "epoch 158, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 159, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 160, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 161, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 162, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 163, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 164, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 165, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 166, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 167, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 168, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 169, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 170, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 171, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 172, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 173, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 174, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.926 Acc@5 100.000\n",
            "epoch 175, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 176, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 177, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 178, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 179, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 180, total time 0.15\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 181, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 182, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 183, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 184, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 185, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 186, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 187, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 188, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 189, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 190, total time 0.15\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 191, total time 0.16\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 192, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 193, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 194, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 195, total time 0.17\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 196, total time 0.16\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 197, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 198, total time 0.16\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 100.000 Acc@5 100.000\n",
            "epoch 199, total time 0.16\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n",
            " * Acc@1 99.963 Acc@5 100.000\n",
            "epoch 200, total time 0.15\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_multi(train_loader, val_loader, distill, epochs):\n",
        "    best_acc = 0\n",
        "    # model\n",
        "    model_t_list = [load_teacher(model_path='./teacher/t_best_'+str(i)+'.pth')\n",
        "                          for i in range(num_teachers)]\n",
        "    model_s = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "\n",
        "    data = torch.randn(2, num_components, window, window)\n",
        "\n",
        "    feat_t_list = []\n",
        "    model_s.eval()\n",
        "\n",
        "    for model_t in model_t_list:\n",
        "        model_t.eval()\n",
        "    for model_t in model_t_list:\n",
        "        feat_t, _ = model_t(data, is_feat=True)\n",
        "        feat_t_list.append(feat_t)\n",
        "    feat_s, _ = model_s(data, is_feat=True)\n",
        "\n",
        "    module_list = nn.ModuleList([])\n",
        "    module_list.append(model_s)\n",
        "    trainable_list = nn.ModuleList([])\n",
        "    trainable_list.append(model_s)\n",
        "\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_div = DistillKL(kd_T)\n",
        "    \n",
        "    criterion_kd = CAMKD()\n",
        "    feat_t_list = [feat_t[-2] for feat_t in feat_t_list]\n",
        "    cal_weight = CalWeight(feat_s[-2], feat_t_list)\n",
        "\n",
        "    module_list.append(cal_weight)\n",
        "    trainable_list.append(cal_weight)\n",
        "\n",
        "    criterion_list = nn.ModuleList([])\n",
        "    criterion_list.append(criterion_cls)    # classification loss\n",
        "    criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
        "    criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(trainable_list.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    # append teacher after optimizer to avoid weight_decay\n",
        "    module_list.extend(model_t_list)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        module_list.cuda()\n",
        "        criterion_list.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # validate teacher accuracy\n",
        "    teacher_acc, _, _ = validate(val_loader, model_t_list[0], criterion_cls)\n",
        "    print('teacher accuracy: ', teacher_acc)\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_distill_multi_teacher(epoch, train_loader, val_loader, module_list, criterion_list, optimizer, distill)\n",
        "\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, tect_acc_top5, test_loss = validate(val_loader, model_s, criterion_cls)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model_s.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "            }\n",
        "            save_file = './student/s_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model_s.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "            }\n",
        "            save_file = os.path.join('./student/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch. \n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model_s.state_dict(),\n",
        "    }\n",
        "    save_file = './student/s_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "K1u0ATn_2xA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_multi(train_dataloader, val_dataloader, distill=kd_method, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPZr7vn62xDu",
        "outputId": "7637421b-4db0-4c00-e0ef-3fdc2f875af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> loading teacher model\n",
            "==> done\n",
            "==> loading teacher model\n",
            "==> done\n",
            "==> loading teacher model\n",
            "==> done\n",
            "==> loading teacher model\n",
            "==> done\n",
            "==> loading teacher model\n",
            "==> done\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "teacher accuracy:  tensor(99.6667, device='cuda:0')\n",
            "==> training...\n",
            "epoch 1, total time 0.47\n",
            " * Acc@1 40.667 Acc@5 85.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 2, total time 0.46\n",
            " * Acc@1 45.000 Acc@5 82.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 3, total time 0.47\n",
            " * Acc@1 56.000 Acc@5 94.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 4, total time 0.49\n",
            " * Acc@1 67.000 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 5, total time 0.47\n",
            " * Acc@1 69.667 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 6, total time 0.50\n",
            " * Acc@1 81.333 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 7, total time 0.51\n",
            " * Acc@1 86.000 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 8, total time 0.50\n",
            " * Acc@1 91.000 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 9, total time 0.48\n",
            " * Acc@1 92.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 10, total time 0.48\n",
            " * Acc@1 92.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 11, total time 0.49\n",
            " * Acc@1 93.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 12, total time 0.48\n",
            " * Acc@1 94.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 13, total time 0.46\n",
            " * Acc@1 95.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 14, total time 0.45\n",
            " * Acc@1 95.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 15, total time 0.45\n",
            " * Acc@1 95.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 16, total time 0.45\n",
            " * Acc@1 95.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 17, total time 0.46\n",
            " * Acc@1 95.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 18, total time 0.46\n",
            " * Acc@1 96.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 19, total time 0.47\n",
            " * Acc@1 97.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 20, total time 0.48\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 21, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 22, total time 0.47\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 23, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 24, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "epoch 25, total time 0.47\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 26, total time 0.45\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 27, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 28, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 29, total time 0.50\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 30, total time 0.48\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 31, total time 0.53\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 32, total time 0.47\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 33, total time 0.49\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 34, total time 0.45\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 35, total time 0.45\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 36, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 37, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 38, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 39, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 40, total time 0.45\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "epoch 41, total time 0.45\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 42, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 43, total time 0.45\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 44, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 45, total time 0.46\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 46, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 47, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 48, total time 0.47\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 49, total time 0.45\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 50, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 51, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 52, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 53, total time 0.45\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 54, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 55, total time 0.50\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 56, total time 0.49\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 57, total time 0.48\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 58, total time 0.52\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 59, total time 0.50\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 60, total time 0.47\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 61, total time 0.46\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 62, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 63, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 64, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 65, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 66, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 67, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 68, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 69, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 70, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 71, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 72, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 73, total time 0.49\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 74, total time 0.48\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 75, total time 0.48\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 76, total time 0.49\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 77, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 78, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 79, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 80, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "epoch 81, total time 0.51\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 82, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 83, total time 0.52\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 84, total time 0.49\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 85, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 86, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 87, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 88, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 89, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 90, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 91, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 92, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 93, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 94, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 95, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 96, total time 0.45\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 97, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 98, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 99, total time 0.46\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n",
            "epoch 100, total time 0.47\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "best accuracy: tensor(98., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_model = cnn2d(num_components, num_classes, cfg=t_cfg, fc=t_fc)\n",
        "t_model.load_state_dict(torch.load('./teacher/t_best_0.pth')['model'])\n",
        "\n",
        "s_model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "s_model.load_state_dict(torch.load('./student/s_best.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrICQ_57GToM",
        "outputId": "1617dc13-9a48-4da7-88d2-a14fd7b42183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "t_model = t_model.to(cuda_device)\n",
        "s_model = s_model.to(cuda_device)\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, t_model, criterion)\n",
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, s_model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY6Oxuc8Gb8R",
        "outputId": "5192683e-6038-4f17-b8e4-154f528816ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 99.545 Acc@5 100.000\n",
            " * Acc@1 96.923 Acc@5 99.915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of teacher parameters:', sum(p.numel() for p in t_model.parameters() if p.requires_grad))\n",
        "print('Num of student parameters:', sum(p.numel() for p in s_model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK5UbeDIGb-7",
        "outputId": "a3cd6aec-0023-4b2e-fc47-ec1bf980368f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of teacher parameters: 426159\n",
            "Num of student parameters: 8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Online Distillaiton: Deep Mutual Learning**"
      ],
      "metadata": {
        "id": "68aBBwxg9GSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_num = 10\n",
        "lr = 1e-3\n",
        "num_train = len(train_dataloader)\n",
        "num_valid = len(val_dataloader)\n",
        "epochs = 100\n",
        "\n",
        "loss_ce = nn.CrossEntropyLoss()\n",
        "loss_kl = nn.KLDivLoss(reduction='batchmean')\n",
        "best_valid_accs = [0.] * model_num\n",
        "\n",
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "jyCPkwna9OVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "optimizers = []\n",
        "\n",
        "for i in range(model_num):\n",
        "    # build models\n",
        "    model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "    model.cuda()\n",
        "    \n",
        "    models.append(model)\n",
        "\n",
        "    # initialize optimizer and scheduler\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.02)\n",
        "   \n",
        "    optimizers.append(optimizer)"
      ],
      "metadata": {
        "id": "QnBem6iU88oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch, train_loader):\n",
        "    \"\"\"\n",
        "    Train the model for 1 epoch of the training set.\n",
        "    An epoch corresponds to one full pass through the entire\n",
        "    training set in successive mini-batches.\n",
        "    This is used by train() and should not be called manually.\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    for i in range(model_num):\n",
        "        models[i].train()\n",
        "        losses.append(AverageMeter())\n",
        "        accs.append(AverageMeter())\n",
        "\n",
        "    \n",
        "    tic = time.time()\n",
        "    with tqdm(total=num_train) as pbar:\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "            images, labels = Variable(images), Variable(labels)\n",
        "            \n",
        "            #forward pass\n",
        "            outputs=[]\n",
        "            for model in models:\n",
        "                outputs.append(model(images))\n",
        "            for i in range(model_num):\n",
        "                ce_loss = loss_ce(outputs[i], labels)\n",
        "                kl_loss = 0\n",
        "                for j in range(model_num):\n",
        "                    if i!=j:\n",
        "                        kl_loss += loss_kl(F.log_softmax(outputs[i], dim = 1), \n",
        "                                                F.softmax(Variable(outputs[j]), dim=1))\n",
        "                loss = ce_loss + kl_loss / (model_num - 1)\n",
        "                \n",
        "                # measure accuracy and record loss\n",
        "                prec = accuracy(outputs[i].data, labels.data, topk=(1,))[0]\n",
        "                losses[i].update(loss.item(), images.size()[0])\n",
        "                accs[i].update(prec.item(), images.size()[0])\n",
        "            \n",
        "\n",
        "                # compute gradients and update SGD\n",
        "                optimizers[i].zero_grad()\n",
        "                loss.backward()\n",
        "                optimizers[i].step()\n",
        "\n",
        "            # measure elapsed time\n",
        "            toc = time.time()\n",
        "            batch_time.update(toc-tic)\n",
        "\n",
        "            pbar.set_description(\n",
        "                (\n",
        "                    \"{:.1f}s - model1_loss: {:.3f} - model1_acc: {:.3f}\".format(\n",
        "                        (toc-tic), losses[0].avg, accs[0].avg\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            batch_size = images.shape[0]\n",
        "            pbar.update(batch_size)\n",
        "        \n",
        "        return losses, accs"
      ],
      "metadata": {
        "id": "PyZIUSeJ9Fhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dml(epoch, valid_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set.\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    accs = []\n",
        "    for i in range(model_num):\n",
        "        models[i].eval()\n",
        "        losses.append(AverageMeter())\n",
        "        accs.append(AverageMeter())\n",
        "\n",
        "    for i, (images, labels) in enumerate(valid_loader):\n",
        "\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        images, labels = Variable(images), Variable(labels)\n",
        "\n",
        "        #forward pass\n",
        "        outputs=[]\n",
        "        for model in models:\n",
        "            outputs.append(model(images))\n",
        "        for i in range(model_num):\n",
        "            ce_loss = loss_ce(outputs[i], labels)\n",
        "            kl_loss = 0\n",
        "            for j in range(model_num):\n",
        "                if i!=j:\n",
        "                    kl_loss += loss_kl(F.log_softmax(outputs[i], dim = 1),\n",
        "                                            F.softmax(Variable(outputs[j]), dim=1))\n",
        "            loss = ce_loss + kl_loss / (model_num - 1)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec = accuracy(outputs[i].data, labels.data, topk=(1,))[0]\n",
        "            losses[i].update(loss.item(), images.size()[0])\n",
        "            accs[i].update(prec.item(), images.size()[0])\n",
        "\n",
        "    return losses, accs"
      ],
      "metadata": {
        "id": "Nyav48gxDhNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dml(train_loader, val_loader):\n",
        "    \"\"\"\n",
        "    Train the model on the training set.\n",
        "    A checkpoint of the model is saved after each epoch\n",
        "    and if the validation accuracy is improved upon,\n",
        "    a separate ckpt is created for use on the test set.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n[*] Train on {} samples, validate on {} samples\".format(\n",
        "        num_train, num_valid)\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print(\n",
        "            '\\nEpoch: {}/{} - LR: {:.6f}'.format(\n",
        "                epoch+1, epochs, optimizers[0].param_groups[0]['lr'],)\n",
        "        )\n",
        "\n",
        "        # train for 1 epoch\n",
        "        train_losses, train_accs = train_one_epoch(epoch, train_loader)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        valid_losses, valid_accs = validate_dml(epoch, val_loader)\n",
        "\n",
        "        for i in range(model_num):\n",
        "            is_best = valid_accs[i].avg> best_valid_accs[i]\n",
        "            msg1 = \"model_{:d}: train loss: {:.3f} - train acc: {:.3f} \"\n",
        "            msg2 = \"- val loss: {:.3f} - val acc: {:.3f}\"\n",
        "            if is_best:\n",
        "                #self.counter = 0\n",
        "                msg2 += \" [*]\"\n",
        "            msg = msg1 + msg2\n",
        "            print(msg.format(i+1, train_losses[i].avg, train_accs[i].avg, valid_losses[i].avg, valid_accs[i].avg))\n",
        "\n",
        "        # check for improvement\n",
        "        #if not is_best:\n",
        "            #self.counter += 1\n",
        "        #if self.counter > self.train_patience:\n",
        "            #print(\"[!] No improvement in a while, stopping training.\")\n",
        "            #return\n",
        "            best_valid_accs[i] = max(valid_accs[i].avg, best_valid_accs[i])\n",
        "            save_checkpoint(i,\n",
        "                {'epoch': epoch + 1,\n",
        "                'model_state': models[i].state_dict(),\n",
        "                'optim_state': optimizers[i].state_dict(),\n",
        "                'best_valid_acc': best_valid_accs[i],\n",
        "                }, is_best\n",
        "            )"
      ],
      "metadata": {
        "id": "-Bp0gY-59FkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(i, state, is_best):\n",
        "    \"\"\"\n",
        "    Save a copy of the model so that it can be loaded at a future\n",
        "    date. This function is used when the model is being evaluated\n",
        "    on the test data.\n",
        "    If this model has reached the best validation accuracy thus\n",
        "    far, a seperate file with the suffix `best` is created.\n",
        "    \"\"\"\n",
        "    # print(\"[*] Saving model to {}\".format(self.ckpt_dir))\n",
        "\n",
        "    if is_best:\n",
        "        filename = 'dml_' + str(i+1) + '_ckpt.pth.tar'\n",
        "        ckpt_path = os.path.join('./', filename)\n",
        "        torch.save(state, ckpt_path)\n",
        "\n",
        "    # if is_best:\n",
        "    #     filename = 'dml_' + str(i+1) + '_model_best.pth.tar'\n",
        "    #     shutil.copyfile(\n",
        "    #         ckpt_path, os.path.join(self.ckpt_dir, filename)\n",
        "    #     )"
      ],
      "metadata": {
        "id": "jUtnR_i99Fm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dml(test_loader, model, criterion):\n",
        "    \"\"\"test\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (input, target) in enumerate(test_loader):\n",
        "\n",
        "            input = input.float()\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # if idx % print_freq == 0:\n",
        "            #     print('Test: [{0}/{1}]\\t'\n",
        "            #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "            #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "            #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "            #            idx, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "            #            top1=top1, top5=top5))\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "Lwf7jQ7eITnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dml(train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVlzlEgv9Fpo",
        "outputId": "386b39ba-286f-4631-8053-1271cde29b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[*] Train on 27 samples, validate on 3 samples\n",
            "\n",
            "Epoch: 1/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 1.650 - model1_acc: 42.279: : 2694it [00:01, 2074.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 1.650 - train acc: 42.279 - val loss: 1.369 - val acc: 40.667 [*]\n",
            "model_2: train loss: 1.654 - train acc: 42.242 - val loss: 1.354 - val acc: 41.333 [*]\n",
            "model_3: train loss: 1.638 - train acc: 42.539 - val loss: 1.359 - val acc: 40.667 [*]\n",
            "model_4: train loss: 1.763 - train acc: 39.421 - val loss: 1.446 - val acc: 40.667 [*]\n",
            "model_5: train loss: 1.612 - train acc: 43.430 - val loss: 1.335 - val acc: 41.000 [*]\n",
            "model_6: train loss: 1.664 - train acc: 42.316 - val loss: 1.367 - val acc: 40.667 [*]\n",
            "model_7: train loss: 1.676 - train acc: 42.242 - val loss: 1.366 - val acc: 40.667 [*]\n",
            "model_8: train loss: 1.759 - train acc: 42.094 - val loss: 1.439 - val acc: 40.667 [*]\n",
            "model_9: train loss: 1.631 - train acc: 42.687 - val loss: 1.322 - val acc: 41.667 [*]\n",
            "model_10: train loss: 1.622 - train acc: 43.690 - val loss: 1.344 - val acc: 40.667 [*]\n",
            "\n",
            "Epoch: 2/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 1.229 - model1_acc: 51.002: : 2694it [00:01, 2180.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 1.229 - train acc: 51.002 - val loss: 1.087 - val acc: 61.667 [*]\n",
            "model_2: train loss: 1.193 - train acc: 55.828 - val loss: 1.063 - val acc: 60.333 [*]\n",
            "model_3: train loss: 1.245 - train acc: 50.520 - val loss: 1.163 - val acc: 56.000 [*]\n",
            "model_4: train loss: 1.329 - train acc: 43.912 - val loss: 1.357 - val acc: 43.667 [*]\n",
            "model_5: train loss: 1.172 - train acc: 53.601 - val loss: 1.043 - val acc: 65.333 [*]\n",
            "model_6: train loss: 1.241 - train acc: 49.666 - val loss: 1.125 - val acc: 62.333 [*]\n",
            "model_7: train loss: 1.190 - train acc: 57.572 - val loss: 1.052 - val acc: 65.000 [*]\n",
            "model_8: train loss: 1.299 - train acc: 46.251 - val loss: 1.231 - val acc: 53.333 [*]\n",
            "model_9: train loss: 1.147 - train acc: 58.946 - val loss: 1.009 - val acc: 65.000 [*]\n",
            "model_10: train loss: 1.239 - train acc: 51.633 - val loss: 1.160 - val acc: 56.000 [*]\n",
            "\n",
            "Epoch: 3/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.850 - model1_acc: 71.344: : 2694it [00:01, 2217.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.850 - train acc: 71.344 - val loss: 0.700 - val acc: 73.000 [*]\n",
            "model_2: train loss: 0.838 - train acc: 71.455 - val loss: 0.680 - val acc: 79.333 [*]\n",
            "model_3: train loss: 0.881 - train acc: 70.082 - val loss: 0.726 - val acc: 79.000 [*]\n",
            "model_4: train loss: 1.346 - train acc: 55.642 - val loss: 1.095 - val acc: 67.667 [*]\n",
            "model_5: train loss: 0.817 - train acc: 73.274 - val loss: 0.665 - val acc: 77.667 [*]\n",
            "model_6: train loss: 0.823 - train acc: 74.091 - val loss: 0.624 - val acc: 81.333 [*]\n",
            "model_7: train loss: 0.829 - train acc: 72.940 - val loss: 0.714 - val acc: 74.333 [*]\n",
            "model_8: train loss: 0.957 - train acc: 70.007 - val loss: 0.741 - val acc: 73.000 [*]\n",
            "model_9: train loss: 0.751 - train acc: 76.578 - val loss: 0.595 - val acc: 85.000 [*]\n",
            "model_10: train loss: 0.842 - train acc: 71.826 - val loss: 0.656 - val acc: 80.333 [*]\n",
            "\n",
            "Epoch: 4/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.483 - model1_acc: 84.558: : 2694it [00:01, 2188.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.483 - train acc: 84.558 - val loss: 0.423 - val acc: 88.000 [*]\n",
            "model_2: train loss: 0.513 - train acc: 81.737 - val loss: 0.462 - val acc: 87.000 [*]\n",
            "model_3: train loss: 0.526 - train acc: 81.849 - val loss: 0.463 - val acc: 87.333 [*]\n",
            "model_4: train loss: 0.761 - train acc: 76.466 - val loss: 0.704 - val acc: 77.667 [*]\n",
            "model_5: train loss: 0.478 - train acc: 83.816 - val loss: 0.404 - val acc: 94.333 [*]\n",
            "model_6: train loss: 0.466 - train acc: 85.486 - val loss: 0.419 - val acc: 91.333 [*]\n",
            "model_7: train loss: 0.553 - train acc: 80.586 - val loss: 0.557 - val acc: 82.667 [*]\n",
            "model_8: train loss: 0.577 - train acc: 80.067 - val loss: 0.600 - val acc: 81.667 [*]\n",
            "model_9: train loss: 0.445 - train acc: 87.379 - val loss: 0.405 - val acc: 92.667 [*]\n",
            "model_10: train loss: 0.479 - train acc: 86.043 - val loss: 0.435 - val acc: 90.333 [*]\n",
            "\n",
            "Epoch: 5/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.323 - model1_acc: 90.460: : 2694it [00:01, 1978.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.323 - train acc: 90.460 - val loss: 0.303 - val acc: 93.333 [*]\n",
            "model_2: train loss: 0.346 - train acc: 89.235 - val loss: 0.306 - val acc: 93.000 [*]\n",
            "model_3: train loss: 0.347 - train acc: 88.419 - val loss: 0.314 - val acc: 94.333 [*]\n",
            "model_4: train loss: 0.560 - train acc: 81.552 - val loss: 0.612 - val acc: 80.000 [*]\n",
            "model_5: train loss: 0.316 - train acc: 91.908 - val loss: 0.275 - val acc: 96.667 [*]\n",
            "model_6: train loss: 0.335 - train acc: 90.238 - val loss: 0.308 - val acc: 94.000 [*]\n",
            "model_7: train loss: 0.423 - train acc: 85.709 - val loss: 0.373 - val acc: 93.667 [*]\n",
            "model_8: train loss: 0.480 - train acc: 83.705 - val loss: 0.507 - val acc: 86.000 [*]\n",
            "model_9: train loss: 0.311 - train acc: 93.430 - val loss: 0.282 - val acc: 96.333 [*]\n",
            "model_10: train loss: 0.344 - train acc: 90.015 - val loss: 0.325 - val acc: 94.333 [*]\n",
            "\n",
            "Epoch: 6/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.231 - model1_acc: 94.766: : 2694it [00:01, 1995.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.231 - train acc: 94.766 - val loss: 0.223 - val acc: 95.333 [*]\n",
            "model_2: train loss: 0.253 - train acc: 93.170 - val loss: 0.228 - val acc: 95.667 [*]\n",
            "model_3: train loss: 0.234 - train acc: 95.100 - val loss: 0.219 - val acc: 95.667 [*]\n",
            "model_4: train loss: 0.464 - train acc: 85.449 - val loss: 0.449 - val acc: 89.000 [*]\n",
            "model_5: train loss: 0.224 - train acc: 96.288 - val loss: 0.205 - val acc: 96.333\n",
            "model_6: train loss: 0.238 - train acc: 94.618 - val loss: 0.205 - val acc: 97.333 [*]\n",
            "model_7: train loss: 0.256 - train acc: 94.098 - val loss: 0.201 - val acc: 96.667 [*]\n",
            "model_8: train loss: 0.345 - train acc: 89.087 - val loss: 0.324 - val acc: 92.667 [*]\n",
            "model_9: train loss: 0.217 - train acc: 95.843 - val loss: 0.196 - val acc: 97.333 [*]\n",
            "model_10: train loss: 0.241 - train acc: 94.766 - val loss: 0.211 - val acc: 96.667 [*]\n",
            "\n",
            "Epoch: 7/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.158 - model1_acc: 96.882: : 2694it [00:01, 2212.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.158 - train acc: 96.882 - val loss: 0.162 - val acc: 96.667 [*]\n",
            "model_2: train loss: 0.177 - train acc: 96.214 - val loss: 0.187 - val acc: 96.333 [*]\n",
            "model_3: train loss: 0.154 - train acc: 97.661 - val loss: 0.161 - val acc: 96.333 [*]\n",
            "model_4: train loss: 0.322 - train acc: 91.388 - val loss: 0.303 - val acc: 91.667 [*]\n",
            "model_5: train loss: 0.153 - train acc: 97.587 - val loss: 0.151 - val acc: 97.000 [*]\n",
            "model_6: train loss: 0.152 - train acc: 97.587 - val loss: 0.150 - val acc: 98.667 [*]\n",
            "model_7: train loss: 0.153 - train acc: 97.365 - val loss: 0.143 - val acc: 97.333 [*]\n",
            "model_8: train loss: 0.223 - train acc: 94.618 - val loss: 0.209 - val acc: 96.000 [*]\n",
            "model_9: train loss: 0.149 - train acc: 97.736 - val loss: 0.155 - val acc: 97.667 [*]\n",
            "model_10: train loss: 0.159 - train acc: 97.439 - val loss: 0.159 - val acc: 98.000 [*]\n",
            "\n",
            "Epoch: 8/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.107 - model1_acc: 98.478: : 2694it [00:01, 2150.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.107 - train acc: 98.478 - val loss: 0.116 - val acc: 97.667 [*]\n",
            "model_2: train loss: 0.121 - train acc: 97.476 - val loss: 0.122 - val acc: 97.000 [*]\n",
            "model_3: train loss: 0.108 - train acc: 98.627 - val loss: 0.112 - val acc: 98.000 [*]\n",
            "model_4: train loss: 0.192 - train acc: 95.583 - val loss: 0.173 - val acc: 97.333 [*]\n",
            "model_5: train loss: 0.105 - train acc: 98.552 - val loss: 0.113 - val acc: 98.333 [*]\n",
            "model_6: train loss: 0.108 - train acc: 98.589 - val loss: 0.112 - val acc: 98.333\n",
            "model_7: train loss: 0.102 - train acc: 98.589 - val loss: 0.104 - val acc: 98.000 [*]\n",
            "model_8: train loss: 0.133 - train acc: 97.290 - val loss: 0.137 - val acc: 97.333 [*]\n",
            "model_9: train loss: 0.106 - train acc: 98.701 - val loss: 0.114 - val acc: 98.000 [*]\n",
            "model_10: train loss: 0.106 - train acc: 98.589 - val loss: 0.113 - val acc: 97.667\n",
            "\n",
            "Epoch: 9/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.072 - model1_acc: 99.332: : 2694it [00:01, 2143.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.072 - train acc: 99.332 - val loss: 0.096 - val acc: 98.000 [*]\n",
            "model_2: train loss: 0.081 - train acc: 98.812 - val loss: 0.104 - val acc: 98.000 [*]\n",
            "model_3: train loss: 0.078 - train acc: 99.258 - val loss: 0.096 - val acc: 98.667 [*]\n",
            "model_4: train loss: 0.122 - train acc: 97.996 - val loss: 0.143 - val acc: 98.333 [*]\n",
            "model_5: train loss: 0.077 - train acc: 99.035 - val loss: 0.112 - val acc: 97.667\n",
            "model_6: train loss: 0.075 - train acc: 99.258 - val loss: 0.093 - val acc: 98.667\n",
            "model_7: train loss: 0.074 - train acc: 99.220 - val loss: 0.097 - val acc: 98.667 [*]\n",
            "model_8: train loss: 0.090 - train acc: 98.589 - val loss: 0.121 - val acc: 98.000 [*]\n",
            "model_9: train loss: 0.082 - train acc: 98.812 - val loss: 0.099 - val acc: 98.333 [*]\n",
            "model_10: train loss: 0.077 - train acc: 99.072 - val loss: 0.100 - val acc: 97.667\n",
            "\n",
            "Epoch: 10/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.063 - model1_acc: 99.517: : 2694it [00:01, 2193.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.063 - train acc: 99.517 - val loss: 0.101 - val acc: 97.333\n",
            "model_2: train loss: 0.065 - train acc: 99.443 - val loss: 0.094 - val acc: 98.000\n",
            "model_3: train loss: 0.067 - train acc: 99.332 - val loss: 0.091 - val acc: 98.667\n",
            "model_4: train loss: 0.102 - train acc: 98.293 - val loss: 0.123 - val acc: 98.000\n",
            "model_5: train loss: 0.065 - train acc: 99.295 - val loss: 0.091 - val acc: 98.667 [*]\n",
            "model_6: train loss: 0.062 - train acc: 99.406 - val loss: 0.090 - val acc: 98.333\n",
            "model_7: train loss: 0.061 - train acc: 99.369 - val loss: 0.086 - val acc: 98.333\n",
            "model_8: train loss: 0.072 - train acc: 99.183 - val loss: 0.125 - val acc: 98.000\n",
            "model_9: train loss: 0.066 - train acc: 99.258 - val loss: 0.093 - val acc: 98.667 [*]\n",
            "model_10: train loss: 0.065 - train acc: 99.369 - val loss: 0.105 - val acc: 98.000\n",
            "\n",
            "Epoch: 11/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.058 - model1_acc: 99.517: : 2694it [00:01, 2197.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.058 - train acc: 99.517 - val loss: 0.082 - val acc: 98.333 [*]\n",
            "model_2: train loss: 0.059 - train acc: 99.629 - val loss: 0.076 - val acc: 98.333 [*]\n",
            "model_3: train loss: 0.057 - train acc: 99.555 - val loss: 0.091 - val acc: 98.333\n",
            "model_4: train loss: 0.091 - train acc: 98.961 - val loss: 0.119 - val acc: 98.000\n",
            "model_5: train loss: 0.058 - train acc: 99.592 - val loss: 0.076 - val acc: 99.000 [*]\n",
            "model_6: train loss: 0.057 - train acc: 99.517 - val loss: 0.079 - val acc: 98.667\n",
            "model_7: train loss: 0.059 - train acc: 99.406 - val loss: 0.080 - val acc: 98.667\n",
            "model_8: train loss: 0.066 - train acc: 99.258 - val loss: 0.090 - val acc: 98.333 [*]\n",
            "model_9: train loss: 0.061 - train acc: 99.555 - val loss: 0.087 - val acc: 98.333\n",
            "model_10: train loss: 0.062 - train acc: 99.406 - val loss: 0.108 - val acc: 98.000\n",
            "\n",
            "Epoch: 12/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.054 - model1_acc: 99.592: : 2694it [00:01, 2225.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.054 - train acc: 99.592 - val loss: 0.073 - val acc: 98.667 [*]\n",
            "model_2: train loss: 0.052 - train acc: 99.703 - val loss: 0.068 - val acc: 98.333\n",
            "model_3: train loss: 0.055 - train acc: 99.443 - val loss: 0.075 - val acc: 98.000\n",
            "model_4: train loss: 0.080 - train acc: 99.035 - val loss: 0.113 - val acc: 97.333\n",
            "model_5: train loss: 0.052 - train acc: 99.517 - val loss: 0.071 - val acc: 98.667\n",
            "model_6: train loss: 0.054 - train acc: 99.443 - val loss: 0.073 - val acc: 98.333\n",
            "model_7: train loss: 0.052 - train acc: 99.703 - val loss: 0.068 - val acc: 99.333 [*]\n",
            "model_8: train loss: 0.057 - train acc: 99.480 - val loss: 0.069 - val acc: 98.667 [*]\n",
            "model_9: train loss: 0.056 - train acc: 99.666 - val loss: 0.078 - val acc: 98.667\n",
            "model_10: train loss: 0.053 - train acc: 99.517 - val loss: 0.088 - val acc: 98.000\n",
            "\n",
            "Epoch: 13/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.050 - model1_acc: 99.666: : 2694it [00:01, 2269.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.050 - train acc: 99.666 - val loss: 0.077 - val acc: 98.667\n",
            "model_2: train loss: 0.049 - train acc: 99.666 - val loss: 0.070 - val acc: 98.333\n",
            "model_3: train loss: 0.049 - train acc: 99.666 - val loss: 0.075 - val acc: 98.667\n",
            "model_4: train loss: 0.063 - train acc: 99.406 - val loss: 0.083 - val acc: 98.333\n",
            "model_5: train loss: 0.048 - train acc: 99.777 - val loss: 0.067 - val acc: 98.667\n",
            "model_6: train loss: 0.046 - train acc: 99.666 - val loss: 0.076 - val acc: 98.000\n",
            "model_7: train loss: 0.047 - train acc: 99.740 - val loss: 0.066 - val acc: 99.000\n",
            "model_8: train loss: 0.052 - train acc: 99.629 - val loss: 0.066 - val acc: 99.000 [*]\n",
            "model_9: train loss: 0.052 - train acc: 99.517 - val loss: 0.094 - val acc: 98.000\n",
            "model_10: train loss: 0.049 - train acc: 99.480 - val loss: 0.074 - val acc: 98.333 [*]\n",
            "\n",
            "Epoch: 14/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.048 - model1_acc: 99.740: : 2694it [00:01, 2088.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.048 - train acc: 99.740 - val loss: 0.067 - val acc: 99.000 [*]\n",
            "model_2: train loss: 0.047 - train acc: 99.666 - val loss: 0.098 - val acc: 97.667\n",
            "model_3: train loss: 0.051 - train acc: 99.517 - val loss: 0.079 - val acc: 98.000\n",
            "model_4: train loss: 0.052 - train acc: 99.666 - val loss: 0.073 - val acc: 98.333\n",
            "model_5: train loss: 0.046 - train acc: 99.666 - val loss: 0.073 - val acc: 98.667\n",
            "model_6: train loss: 0.052 - train acc: 99.517 - val loss: 0.070 - val acc: 98.667\n",
            "model_7: train loss: 0.046 - train acc: 99.740 - val loss: 0.066 - val acc: 99.333\n",
            "model_8: train loss: 0.050 - train acc: 99.666 - val loss: 0.071 - val acc: 98.333\n",
            "model_9: train loss: 0.049 - train acc: 99.629 - val loss: 0.090 - val acc: 98.000\n",
            "model_10: train loss: 0.046 - train acc: 99.629 - val loss: 0.066 - val acc: 98.333\n",
            "\n",
            "Epoch: 15/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.046 - model1_acc: 99.703: : 2694it [00:01, 1907.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.046 - train acc: 99.703 - val loss: 0.074 - val acc: 98.667\n",
            "model_2: train loss: 0.046 - train acc: 99.740 - val loss: 0.074 - val acc: 98.333\n",
            "model_3: train loss: 0.056 - train acc: 99.480 - val loss: 0.060 - val acc: 99.000 [*]\n",
            "model_4: train loss: 0.048 - train acc: 99.740 - val loss: 0.068 - val acc: 98.333\n",
            "model_5: train loss: 0.046 - train acc: 99.555 - val loss: 0.065 - val acc: 99.000\n",
            "model_6: train loss: 0.048 - train acc: 99.703 - val loss: 0.067 - val acc: 98.667\n",
            "model_7: train loss: 0.047 - train acc: 99.740 - val loss: 0.063 - val acc: 99.333\n",
            "model_8: train loss: 0.049 - train acc: 99.629 - val loss: 0.065 - val acc: 98.667\n",
            "model_9: train loss: 0.054 - train acc: 99.555 - val loss: 0.096 - val acc: 97.667\n",
            "model_10: train loss: 0.044 - train acc: 99.814 - val loss: 0.064 - val acc: 99.000 [*]\n",
            "\n",
            "Epoch: 16/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.045 - model1_acc: 99.777: : 2694it [00:01, 2099.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.045 - train acc: 99.777 - val loss: 0.068 - val acc: 99.000\n",
            "model_2: train loss: 0.046 - train acc: 99.666 - val loss: 0.098 - val acc: 98.000\n",
            "model_3: train loss: 0.048 - train acc: 99.666 - val loss: 0.063 - val acc: 99.000\n",
            "model_4: train loss: 0.046 - train acc: 99.740 - val loss: 0.065 - val acc: 99.000 [*]\n",
            "model_5: train loss: 0.051 - train acc: 99.666 - val loss: 0.066 - val acc: 98.333\n",
            "model_6: train loss: 0.048 - train acc: 99.666 - val loss: 0.079 - val acc: 98.000\n",
            "model_7: train loss: 0.046 - train acc: 99.777 - val loss: 0.073 - val acc: 98.667\n",
            "model_8: train loss: 0.046 - train acc: 99.814 - val loss: 0.077 - val acc: 98.000\n",
            "model_9: train loss: 0.046 - train acc: 99.814 - val loss: 0.064 - val acc: 99.000 [*]\n",
            "model_10: train loss: 0.053 - train acc: 99.443 - val loss: 0.069 - val acc: 99.000\n",
            "\n",
            "Epoch: 17/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.042 - model1_acc: 99.740: : 2694it [00:01, 2071.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.042 - train acc: 99.740 - val loss: 0.061 - val acc: 99.000\n",
            "model_2: train loss: 0.044 - train acc: 99.814 - val loss: 0.078 - val acc: 97.667\n",
            "model_3: train loss: 0.042 - train acc: 99.777 - val loss: 0.059 - val acc: 99.000\n",
            "model_4: train loss: 0.044 - train acc: 99.740 - val loss: 0.060 - val acc: 98.333\n",
            "model_5: train loss: 0.043 - train acc: 99.814 - val loss: 0.062 - val acc: 98.333\n",
            "model_6: train loss: 0.043 - train acc: 99.777 - val loss: 0.062 - val acc: 98.333\n",
            "model_7: train loss: 0.042 - train acc: 99.703 - val loss: 0.060 - val acc: 99.000\n",
            "model_8: train loss: 0.043 - train acc: 99.703 - val loss: 0.073 - val acc: 99.000\n",
            "model_9: train loss: 0.040 - train acc: 99.777 - val loss: 0.064 - val acc: 99.000\n",
            "model_10: train loss: 0.046 - train acc: 99.666 - val loss: 0.079 - val acc: 98.000\n",
            "\n",
            "Epoch: 18/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.038 - model1_acc: 99.740: : 2694it [00:01, 2204.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.038 - train acc: 99.740 - val loss: 0.070 - val acc: 98.667\n",
            "model_2: train loss: 0.046 - train acc: 99.703 - val loss: 0.069 - val acc: 98.667 [*]\n",
            "model_3: train loss: 0.037 - train acc: 99.814 - val loss: 0.071 - val acc: 98.667\n",
            "model_4: train loss: 0.041 - train acc: 99.777 - val loss: 0.063 - val acc: 98.333\n",
            "model_5: train loss: 0.042 - train acc: 99.666 - val loss: 0.071 - val acc: 98.667\n",
            "model_6: train loss: 0.043 - train acc: 99.703 - val loss: 0.068 - val acc: 98.333\n",
            "model_7: train loss: 0.041 - train acc: 99.740 - val loss: 0.065 - val acc: 99.000\n",
            "model_8: train loss: 0.040 - train acc: 99.740 - val loss: 0.089 - val acc: 98.333\n",
            "model_9: train loss: 0.036 - train acc: 99.889 - val loss: 0.059 - val acc: 99.000\n",
            "model_10: train loss: 0.039 - train acc: 99.852 - val loss: 0.069 - val acc: 98.667\n",
            "\n",
            "Epoch: 19/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.038 - model1_acc: 99.777: : 2694it [00:01, 2182.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.038 - train acc: 99.777 - val loss: 0.063 - val acc: 99.000\n",
            "model_2: train loss: 0.043 - train acc: 99.814 - val loss: 0.073 - val acc: 98.333\n",
            "model_3: train loss: 0.038 - train acc: 99.814 - val loss: 0.056 - val acc: 99.000\n",
            "model_4: train loss: 0.040 - train acc: 99.814 - val loss: 0.060 - val acc: 98.667\n",
            "model_5: train loss: 0.038 - train acc: 99.852 - val loss: 0.076 - val acc: 98.333\n",
            "model_6: train loss: 0.041 - train acc: 99.703 - val loss: 0.072 - val acc: 98.667\n",
            "model_7: train loss: 0.038 - train acc: 99.926 - val loss: 0.067 - val acc: 98.333\n",
            "model_8: train loss: 0.040 - train acc: 99.777 - val loss: 0.067 - val acc: 99.000\n",
            "model_9: train loss: 0.035 - train acc: 99.926 - val loss: 0.061 - val acc: 98.667\n",
            "model_10: train loss: 0.042 - train acc: 99.740 - val loss: 0.071 - val acc: 99.000\n",
            "\n",
            "Epoch: 20/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.044 - model1_acc: 99.666: : 2694it [00:01, 2091.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.044 - train acc: 99.666 - val loss: 0.100 - val acc: 98.000\n",
            "model_2: train loss: 0.042 - train acc: 99.926 - val loss: 0.062 - val acc: 99.000 [*]\n",
            "model_3: train loss: 0.040 - train acc: 99.592 - val loss: 0.058 - val acc: 98.667\n",
            "model_4: train loss: 0.040 - train acc: 99.889 - val loss: 0.064 - val acc: 99.000\n",
            "model_5: train loss: 0.041 - train acc: 99.740 - val loss: 0.067 - val acc: 99.000\n",
            "model_6: train loss: 0.048 - train acc: 99.666 - val loss: 0.126 - val acc: 96.333\n",
            "model_7: train loss: 0.044 - train acc: 99.740 - val loss: 0.080 - val acc: 99.333\n",
            "model_8: train loss: 0.043 - train acc: 99.889 - val loss: 0.072 - val acc: 99.000\n",
            "model_9: train loss: 0.036 - train acc: 99.889 - val loss: 0.063 - val acc: 99.000\n",
            "model_10: train loss: 0.041 - train acc: 99.814 - val loss: 0.077 - val acc: 98.667\n",
            "\n",
            "Epoch: 21/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.057 - model1_acc: 99.555: : 2694it [00:01, 2204.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.057 - train acc: 99.555 - val loss: 0.091 - val acc: 98.333\n",
            "model_2: train loss: 0.041 - train acc: 99.926 - val loss: 0.065 - val acc: 99.333 [*]\n",
            "model_3: train loss: 0.044 - train acc: 99.740 - val loss: 0.066 - val acc: 99.000\n",
            "model_4: train loss: 0.046 - train acc: 99.740 - val loss: 0.069 - val acc: 98.333\n",
            "model_5: train loss: 0.054 - train acc: 99.592 - val loss: 0.068 - val acc: 98.333\n",
            "model_6: train loss: 0.063 - train acc: 99.295 - val loss: 0.067 - val acc: 99.000 [*]\n",
            "model_7: train loss: 0.048 - train acc: 99.740 - val loss: 0.067 - val acc: 99.000\n",
            "model_8: train loss: 0.047 - train acc: 99.703 - val loss: 0.058 - val acc: 99.000\n",
            "model_9: train loss: 0.040 - train acc: 99.889 - val loss: 0.062 - val acc: 99.000\n",
            "model_10: train loss: 0.050 - train acc: 99.703 - val loss: 0.078 - val acc: 99.000\n",
            "\n",
            "Epoch: 22/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.071 - model1_acc: 99.332: : 2694it [00:01, 2211.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.071 - train acc: 99.332 - val loss: 0.072 - val acc: 98.667\n",
            "model_2: train loss: 0.048 - train acc: 99.814 - val loss: 0.078 - val acc: 98.333\n",
            "model_3: train loss: 0.049 - train acc: 99.629 - val loss: 0.077 - val acc: 98.333\n",
            "model_4: train loss: 0.047 - train acc: 99.814 - val loss: 0.085 - val acc: 98.000\n",
            "model_5: train loss: 0.058 - train acc: 99.517 - val loss: 0.091 - val acc: 99.000\n",
            "model_6: train loss: 0.053 - train acc: 99.740 - val loss: 0.069 - val acc: 98.667\n",
            "model_7: train loss: 0.049 - train acc: 99.777 - val loss: 0.064 - val acc: 98.667\n",
            "model_8: train loss: 0.052 - train acc: 99.777 - val loss: 0.064 - val acc: 99.000\n",
            "model_9: train loss: 0.040 - train acc: 99.926 - val loss: 0.063 - val acc: 99.000\n",
            "model_10: train loss: 0.045 - train acc: 99.889 - val loss: 0.064 - val acc: 99.000\n",
            "\n",
            "Epoch: 23/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.048 - model1_acc: 99.666: : 2694it [00:01, 2099.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.048 - train acc: 99.666 - val loss: 0.054 - val acc: 99.333 [*]\n",
            "model_2: train loss: 0.043 - train acc: 99.777 - val loss: 0.077 - val acc: 98.000\n",
            "model_3: train loss: 0.042 - train acc: 99.852 - val loss: 0.059 - val acc: 99.000\n",
            "model_4: train loss: 0.046 - train acc: 99.703 - val loss: 0.069 - val acc: 98.667\n",
            "model_5: train loss: 0.048 - train acc: 99.740 - val loss: 0.076 - val acc: 98.333\n",
            "model_6: train loss: 0.046 - train acc: 99.703 - val loss: 0.087 - val acc: 99.000\n",
            "model_7: train loss: 0.044 - train acc: 99.777 - val loss: 0.061 - val acc: 98.667\n",
            "model_8: train loss: 0.051 - train acc: 99.555 - val loss: 0.095 - val acc: 97.667\n",
            "model_9: train loss: 0.041 - train acc: 99.889 - val loss: 0.058 - val acc: 99.333 [*]\n",
            "model_10: train loss: 0.044 - train acc: 99.666 - val loss: 0.063 - val acc: 99.333 [*]\n",
            "\n",
            "Epoch: 24/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.042 - model1_acc: 99.852: : 2694it [00:01, 1906.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.042 - train acc: 99.852 - val loss: 0.056 - val acc: 98.667\n",
            "model_2: train loss: 0.045 - train acc: 99.592 - val loss: 0.065 - val acc: 98.667\n",
            "model_3: train loss: 0.041 - train acc: 99.852 - val loss: 0.065 - val acc: 99.000\n",
            "model_4: train loss: 0.043 - train acc: 99.703 - val loss: 0.062 - val acc: 98.333\n",
            "model_5: train loss: 0.043 - train acc: 99.814 - val loss: 0.069 - val acc: 99.000\n",
            "model_6: train loss: 0.046 - train acc: 99.777 - val loss: 0.065 - val acc: 99.000\n",
            "model_7: train loss: 0.043 - train acc: 99.740 - val loss: 0.067 - val acc: 98.667\n",
            "model_8: train loss: 0.049 - train acc: 99.480 - val loss: 0.070 - val acc: 98.667\n",
            "model_9: train loss: 0.047 - train acc: 99.629 - val loss: 0.063 - val acc: 98.667\n",
            "model_10: train loss: 0.040 - train acc: 99.777 - val loss: 0.069 - val acc: 98.333\n",
            "\n",
            "Epoch: 25/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.040 - model1_acc: 99.814: : 2694it [00:01, 1986.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.040 - train acc: 99.814 - val loss: 0.078 - val acc: 98.000\n",
            "model_2: train loss: 0.040 - train acc: 99.814 - val loss: 0.065 - val acc: 99.000\n",
            "model_3: train loss: 0.046 - train acc: 99.666 - val loss: 0.066 - val acc: 99.000\n",
            "model_4: train loss: 0.044 - train acc: 99.703 - val loss: 0.063 - val acc: 98.333\n",
            "model_5: train loss: 0.043 - train acc: 99.852 - val loss: 0.066 - val acc: 98.667\n",
            "model_6: train loss: 0.042 - train acc: 99.814 - val loss: 0.063 - val acc: 98.667\n",
            "model_7: train loss: 0.046 - train acc: 99.814 - val loss: 0.061 - val acc: 99.000\n",
            "model_8: train loss: 0.039 - train acc: 99.889 - val loss: 0.061 - val acc: 98.667\n",
            "model_9: train loss: 0.044 - train acc: 99.814 - val loss: 0.060 - val acc: 99.000\n",
            "model_10: train loss: 0.042 - train acc: 99.814 - val loss: 0.067 - val acc: 98.667\n",
            "\n",
            "Epoch: 26/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.037 - model1_acc: 99.852: : 2694it [00:01, 2051.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.037 - train acc: 99.852 - val loss: 0.059 - val acc: 99.000\n",
            "model_2: train loss: 0.035 - train acc: 99.889 - val loss: 0.054 - val acc: 99.000\n",
            "model_3: train loss: 0.044 - train acc: 99.740 - val loss: 0.068 - val acc: 99.000\n",
            "model_4: train loss: 0.041 - train acc: 99.777 - val loss: 0.063 - val acc: 99.667 [*]\n",
            "model_5: train loss: 0.038 - train acc: 99.926 - val loss: 0.061 - val acc: 99.333 [*]\n",
            "model_6: train loss: 0.034 - train acc: 99.926 - val loss: 0.064 - val acc: 99.000\n",
            "model_7: train loss: 0.039 - train acc: 99.740 - val loss: 0.061 - val acc: 99.000\n",
            "model_8: train loss: 0.036 - train acc: 99.889 - val loss: 0.062 - val acc: 98.667\n",
            "model_9: train loss: 0.042 - train acc: 99.703 - val loss: 0.068 - val acc: 98.667\n",
            "model_10: train loss: 0.038 - train acc: 99.814 - val loss: 0.105 - val acc: 98.333\n",
            "\n",
            "Epoch: 27/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.038 - model1_acc: 99.889: : 2694it [00:01, 2032.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.038 - train acc: 99.889 - val loss: 0.065 - val acc: 99.000\n",
            "model_2: train loss: 0.037 - train acc: 99.814 - val loss: 0.067 - val acc: 98.667\n",
            "model_3: train loss: 0.047 - train acc: 99.777 - val loss: 0.095 - val acc: 98.000\n",
            "model_4: train loss: 0.043 - train acc: 99.814 - val loss: 0.061 - val acc: 98.667\n",
            "model_5: train loss: 0.038 - train acc: 99.889 - val loss: 0.076 - val acc: 98.333\n",
            "model_6: train loss: 0.038 - train acc: 99.889 - val loss: 0.069 - val acc: 98.667\n",
            "model_7: train loss: 0.046 - train acc: 99.777 - val loss: 0.081 - val acc: 99.000\n",
            "model_8: train loss: 0.036 - train acc: 99.926 - val loss: 0.064 - val acc: 98.667\n",
            "model_9: train loss: 0.042 - train acc: 99.852 - val loss: 0.072 - val acc: 99.000\n",
            "model_10: train loss: 0.050 - train acc: 99.517 - val loss: 0.129 - val acc: 97.667\n",
            "\n",
            "Epoch: 28/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.039 - model1_acc: 99.889: : 2694it [00:01, 2085.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.039 - train acc: 99.889 - val loss: 0.055 - val acc: 99.333\n",
            "model_2: train loss: 0.038 - train acc: 99.852 - val loss: 0.069 - val acc: 98.667\n",
            "model_3: train loss: 0.046 - train acc: 99.814 - val loss: 0.066 - val acc: 99.000\n",
            "model_4: train loss: 0.047 - train acc: 99.777 - val loss: 0.059 - val acc: 98.667\n",
            "model_5: train loss: 0.042 - train acc: 99.703 - val loss: 0.063 - val acc: 98.667\n",
            "model_6: train loss: 0.037 - train acc: 99.963 - val loss: 0.051 - val acc: 98.667\n",
            "model_7: train loss: 0.042 - train acc: 99.740 - val loss: 0.050 - val acc: 99.000\n",
            "model_8: train loss: 0.037 - train acc: 99.889 - val loss: 0.055 - val acc: 98.667\n",
            "model_9: train loss: 0.044 - train acc: 99.740 - val loss: 0.062 - val acc: 99.000\n",
            "model_10: train loss: 0.061 - train acc: 99.332 - val loss: 0.074 - val acc: 99.000\n",
            "\n",
            "Epoch: 29/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.042 - model1_acc: 99.740: : 2694it [00:01, 2158.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.042 - train acc: 99.740 - val loss: 0.079 - val acc: 97.667\n",
            "model_2: train loss: 0.040 - train acc: 99.889 - val loss: 0.062 - val acc: 99.000\n",
            "model_3: train loss: 0.037 - train acc: 99.852 - val loss: 0.057 - val acc: 98.667\n",
            "model_4: train loss: 0.042 - train acc: 99.740 - val loss: 0.066 - val acc: 98.667\n",
            "model_5: train loss: 0.035 - train acc: 99.963 - val loss: 0.061 - val acc: 99.000\n",
            "model_6: train loss: 0.034 - train acc: 99.852 - val loss: 0.062 - val acc: 98.667\n",
            "model_7: train loss: 0.049 - train acc: 99.592 - val loss: 0.064 - val acc: 99.000\n",
            "model_8: train loss: 0.035 - train acc: 99.963 - val loss: 0.064 - val acc: 99.000\n",
            "model_9: train loss: 0.037 - train acc: 99.926 - val loss: 0.068 - val acc: 98.667\n",
            "model_10: train loss: 0.040 - train acc: 99.889 - val loss: 0.063 - val acc: 99.000\n",
            "\n",
            "Epoch: 30/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.032 - model1_acc: 99.889: : 2694it [00:01, 2177.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.032 - train acc: 99.889 - val loss: 0.054 - val acc: 99.000\n",
            "model_2: train loss: 0.037 - train acc: 99.889 - val loss: 0.061 - val acc: 98.667\n",
            "model_3: train loss: 0.033 - train acc: 99.889 - val loss: 0.054 - val acc: 99.000\n",
            "model_4: train loss: 0.037 - train acc: 99.814 - val loss: 0.053 - val acc: 98.667\n",
            "model_5: train loss: 0.031 - train acc: 100.000 - val loss: 0.055 - val acc: 99.000\n",
            "model_6: train loss: 0.031 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "model_7: train loss: 0.037 - train acc: 99.814 - val loss: 0.063 - val acc: 99.000\n",
            "model_8: train loss: 0.033 - train acc: 99.889 - val loss: 0.054 - val acc: 99.000\n",
            "model_9: train loss: 0.035 - train acc: 99.926 - val loss: 0.076 - val acc: 98.000\n",
            "model_10: train loss: 0.032 - train acc: 99.926 - val loss: 0.051 - val acc: 99.000\n",
            "\n",
            "Epoch: 31/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.031 - model1_acc: 99.963: : 2694it [00:01, 2139.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.031 - train acc: 99.963 - val loss: 0.053 - val acc: 99.000\n",
            "model_2: train loss: 0.036 - train acc: 99.889 - val loss: 0.060 - val acc: 98.667\n",
            "model_3: train loss: 0.036 - train acc: 99.889 - val loss: 0.066 - val acc: 99.333 [*]\n",
            "model_4: train loss: 0.034 - train acc: 99.889 - val loss: 0.058 - val acc: 98.667\n",
            "model_5: train loss: 0.036 - train acc: 99.814 - val loss: 0.059 - val acc: 98.667\n",
            "model_6: train loss: 0.033 - train acc: 99.852 - val loss: 0.052 - val acc: 99.000\n",
            "model_7: train loss: 0.040 - train acc: 99.814 - val loss: 0.096 - val acc: 97.667\n",
            "model_8: train loss: 0.034 - train acc: 99.926 - val loss: 0.053 - val acc: 99.000\n",
            "model_9: train loss: 0.039 - train acc: 99.703 - val loss: 0.099 - val acc: 97.667\n",
            "model_10: train loss: 0.033 - train acc: 99.926 - val loss: 0.067 - val acc: 99.000\n",
            "\n",
            "Epoch: 32/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.031 - model1_acc: 99.963: : 2694it [00:01, 1961.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.031 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_2: train loss: 0.032 - train acc: 99.963 - val loss: 0.048 - val acc: 99.000\n",
            "model_3: train loss: 0.036 - train acc: 99.926 - val loss: 0.064 - val acc: 99.000\n",
            "model_4: train loss: 0.033 - train acc: 99.889 - val loss: 0.050 - val acc: 99.000\n",
            "model_5: train loss: 0.033 - train acc: 99.926 - val loss: 0.054 - val acc: 99.000\n",
            "model_6: train loss: 0.033 - train acc: 99.777 - val loss: 0.058 - val acc: 99.000\n",
            "model_7: train loss: 0.052 - train acc: 99.629 - val loss: 0.072 - val acc: 98.333\n",
            "model_8: train loss: 0.038 - train acc: 99.852 - val loss: 0.055 - val acc: 99.000\n",
            "model_9: train loss: 0.036 - train acc: 99.963 - val loss: 0.071 - val acc: 98.667\n",
            "model_10: train loss: 0.033 - train acc: 99.889 - val loss: 0.074 - val acc: 98.000\n",
            "\n",
            "Epoch: 33/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.5s - model1_loss: 0.029 - model1_acc: 100.000: : 2694it [00:01, 1857.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 100.000 - val loss: 0.060 - val acc: 98.667\n",
            "model_2: train loss: 0.031 - train acc: 99.963 - val loss: 0.051 - val acc: 99.000\n",
            "model_3: train loss: 0.036 - train acc: 99.777 - val loss: 0.059 - val acc: 98.667\n",
            "model_4: train loss: 0.032 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "model_5: train loss: 0.034 - train acc: 99.889 - val loss: 0.057 - val acc: 98.667\n",
            "model_6: train loss: 0.031 - train acc: 99.963 - val loss: 0.057 - val acc: 98.667\n",
            "model_7: train loss: 0.039 - train acc: 99.814 - val loss: 0.080 - val acc: 99.333\n",
            "model_8: train loss: 0.036 - train acc: 99.852 - val loss: 0.112 - val acc: 97.667\n",
            "model_9: train loss: 0.047 - train acc: 99.666 - val loss: 0.067 - val acc: 99.000\n",
            "model_10: train loss: 0.032 - train acc: 99.963 - val loss: 0.058 - val acc: 99.000\n",
            "\n",
            "Epoch: 34/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.031 - model1_acc: 99.926: : 2694it [00:01, 2019.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.031 - train acc: 99.926 - val loss: 0.058 - val acc: 98.667\n",
            "model_2: train loss: 0.031 - train acc: 99.963 - val loss: 0.054 - val acc: 99.333\n",
            "model_3: train loss: 0.036 - train acc: 99.814 - val loss: 0.056 - val acc: 99.000\n",
            "model_4: train loss: 0.031 - train acc: 99.926 - val loss: 0.063 - val acc: 98.667\n",
            "model_5: train loss: 0.033 - train acc: 99.814 - val loss: 0.064 - val acc: 98.333\n",
            "model_6: train loss: 0.037 - train acc: 99.777 - val loss: 0.059 - val acc: 99.000\n",
            "model_7: train loss: 0.041 - train acc: 99.852 - val loss: 0.052 - val acc: 99.000\n",
            "model_8: train loss: 0.045 - train acc: 99.703 - val loss: 0.075 - val acc: 98.667\n",
            "model_9: train loss: 0.044 - train acc: 99.740 - val loss: 0.076 - val acc: 98.667\n",
            "model_10: train loss: 0.036 - train acc: 99.889 - val loss: 0.066 - val acc: 99.000\n",
            "\n",
            "Epoch: 35/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.035 - model1_acc: 99.814: : 2694it [00:01, 2133.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.035 - train acc: 99.814 - val loss: 0.051 - val acc: 99.000\n",
            "model_2: train loss: 0.034 - train acc: 100.000 - val loss: 0.053 - val acc: 99.000\n",
            "model_3: train loss: 0.040 - train acc: 99.889 - val loss: 0.063 - val acc: 99.000\n",
            "model_4: train loss: 0.040 - train acc: 99.777 - val loss: 0.070 - val acc: 98.667\n",
            "model_5: train loss: 0.039 - train acc: 99.777 - val loss: 0.064 - val acc: 99.000\n",
            "model_6: train loss: 0.043 - train acc: 99.666 - val loss: 0.080 - val acc: 97.333\n",
            "model_7: train loss: 0.036 - train acc: 99.926 - val loss: 0.056 - val acc: 98.667\n",
            "model_8: train loss: 0.044 - train acc: 99.666 - val loss: 0.062 - val acc: 98.667\n",
            "model_9: train loss: 0.038 - train acc: 99.889 - val loss: 0.060 - val acc: 98.667\n",
            "model_10: train loss: 0.034 - train acc: 99.926 - val loss: 0.054 - val acc: 99.000\n",
            "\n",
            "Epoch: 36/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.034 - model1_acc: 99.889: : 2694it [00:01, 2009.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.034 - train acc: 99.889 - val loss: 0.066 - val acc: 98.333\n",
            "model_2: train loss: 0.032 - train acc: 99.889 - val loss: 0.057 - val acc: 99.000\n",
            "model_3: train loss: 0.037 - train acc: 99.963 - val loss: 0.066 - val acc: 99.333\n",
            "model_4: train loss: 0.038 - train acc: 99.777 - val loss: 0.057 - val acc: 98.667\n",
            "model_5: train loss: 0.041 - train acc: 99.703 - val loss: 0.066 - val acc: 99.000\n",
            "model_6: train loss: 0.039 - train acc: 99.889 - val loss: 0.072 - val acc: 97.667\n",
            "model_7: train loss: 0.036 - train acc: 99.814 - val loss: 0.064 - val acc: 98.667\n",
            "model_8: train loss: 0.037 - train acc: 99.814 - val loss: 0.057 - val acc: 98.667\n",
            "model_9: train loss: 0.033 - train acc: 99.889 - val loss: 0.059 - val acc: 99.000\n",
            "model_10: train loss: 0.033 - train acc: 99.926 - val loss: 0.051 - val acc: 98.667\n",
            "\n",
            "Epoch: 37/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.034 - model1_acc: 99.963: : 2694it [00:01, 2012.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.034 - train acc: 99.963 - val loss: 0.067 - val acc: 98.000\n",
            "model_2: train loss: 0.033 - train acc: 100.000 - val loss: 0.069 - val acc: 98.667\n",
            "model_3: train loss: 0.042 - train acc: 99.740 - val loss: 0.061 - val acc: 99.000\n",
            "model_4: train loss: 0.040 - train acc: 99.889 - val loss: 0.082 - val acc: 99.000\n",
            "model_5: train loss: 0.050 - train acc: 99.666 - val loss: 0.100 - val acc: 98.000\n",
            "model_6: train loss: 0.060 - train acc: 99.406 - val loss: 0.090 - val acc: 98.667\n",
            "model_7: train loss: 0.036 - train acc: 99.889 - val loss: 0.058 - val acc: 99.000\n",
            "model_8: train loss: 0.039 - train acc: 99.926 - val loss: 0.066 - val acc: 99.000\n",
            "model_9: train loss: 0.031 - train acc: 99.963 - val loss: 0.061 - val acc: 98.667\n",
            "model_10: train loss: 0.033 - train acc: 99.963 - val loss: 0.058 - val acc: 99.000\n",
            "\n",
            "Epoch: 38/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.037 - model1_acc: 99.814: : 2694it [00:01, 2042.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.037 - train acc: 99.814 - val loss: 0.064 - val acc: 98.333\n",
            "model_2: train loss: 0.040 - train acc: 99.777 - val loss: 0.075 - val acc: 98.333\n",
            "model_3: train loss: 0.038 - train acc: 99.852 - val loss: 0.061 - val acc: 99.000\n",
            "model_4: train loss: 0.040 - train acc: 99.852 - val loss: 0.058 - val acc: 99.000\n",
            "model_5: train loss: 0.050 - train acc: 99.703 - val loss: 0.062 - val acc: 98.333\n",
            "model_6: train loss: 0.043 - train acc: 99.852 - val loss: 0.054 - val acc: 99.333 [*]\n",
            "model_7: train loss: 0.033 - train acc: 99.889 - val loss: 0.047 - val acc: 99.000\n",
            "model_8: train loss: 0.035 - train acc: 99.963 - val loss: 0.059 - val acc: 99.000\n",
            "model_9: train loss: 0.034 - train acc: 99.889 - val loss: 0.058 - val acc: 98.667\n",
            "model_10: train loss: 0.035 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "\n",
            "Epoch: 39/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.035 - model1_acc: 99.926: : 2694it [00:01, 2035.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.035 - train acc: 99.926 - val loss: 0.077 - val acc: 99.000\n",
            "model_2: train loss: 0.035 - train acc: 99.852 - val loss: 0.054 - val acc: 98.667\n",
            "model_3: train loss: 0.035 - train acc: 99.889 - val loss: 0.061 - val acc: 99.000\n",
            "model_4: train loss: 0.033 - train acc: 99.963 - val loss: 0.056 - val acc: 99.000\n",
            "model_5: train loss: 0.044 - train acc: 99.740 - val loss: 0.100 - val acc: 98.667\n",
            "model_6: train loss: 0.036 - train acc: 99.814 - val loss: 0.072 - val acc: 98.333\n",
            "model_7: train loss: 0.030 - train acc: 99.926 - val loss: 0.053 - val acc: 99.000\n",
            "model_8: train loss: 0.032 - train acc: 99.889 - val loss: 0.058 - val acc: 98.667\n",
            "model_9: train loss: 0.028 - train acc: 100.000 - val loss: 0.053 - val acc: 99.000\n",
            "model_10: train loss: 0.030 - train acc: 99.963 - val loss: 0.053 - val acc: 99.000\n",
            "\n",
            "Epoch: 40/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.061 - model1_acc: 99.480: : 2694it [00:01, 2057.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.061 - train acc: 99.480 - val loss: 0.067 - val acc: 99.000\n",
            "model_2: train loss: 0.032 - train acc: 100.000 - val loss: 0.051 - val acc: 98.667\n",
            "model_3: train loss: 0.040 - train acc: 99.777 - val loss: 0.054 - val acc: 99.333\n",
            "model_4: train loss: 0.032 - train acc: 100.000 - val loss: 0.054 - val acc: 99.000\n",
            "model_5: train loss: 0.040 - train acc: 99.740 - val loss: 0.059 - val acc: 99.000\n",
            "model_6: train loss: 0.036 - train acc: 100.000 - val loss: 0.052 - val acc: 99.333\n",
            "model_7: train loss: 0.032 - train acc: 99.889 - val loss: 0.056 - val acc: 99.000\n",
            "model_8: train loss: 0.034 - train acc: 99.926 - val loss: 0.055 - val acc: 99.333 [*]\n",
            "model_9: train loss: 0.033 - train acc: 100.000 - val loss: 0.059 - val acc: 99.000\n",
            "model_10: train loss: 0.034 - train acc: 99.889 - val loss: 0.052 - val acc: 99.000\n",
            "\n",
            "Epoch: 41/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.037 - model1_acc: 99.814: : 2694it [00:01, 2038.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.037 - train acc: 99.814 - val loss: 0.055 - val acc: 99.333\n",
            "model_2: train loss: 0.029 - train acc: 100.000 - val loss: 0.050 - val acc: 99.000\n",
            "model_3: train loss: 0.039 - train acc: 99.666 - val loss: 0.072 - val acc: 99.333\n",
            "model_4: train loss: 0.030 - train acc: 99.963 - val loss: 0.052 - val acc: 98.667\n",
            "model_5: train loss: 0.034 - train acc: 99.926 - val loss: 0.057 - val acc: 99.000\n",
            "model_6: train loss: 0.029 - train acc: 99.963 - val loss: 0.046 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.889 - val loss: 0.067 - val acc: 98.667\n",
            "model_8: train loss: 0.028 - train acc: 99.963 - val loss: 0.052 - val acc: 98.667\n",
            "model_9: train loss: 0.030 - train acc: 99.926 - val loss: 0.057 - val acc: 98.333\n",
            "model_10: train loss: 0.035 - train acc: 99.889 - val loss: 0.056 - val acc: 99.000\n",
            "\n",
            "Epoch: 42/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.033 - model1_acc: 99.889: : 2694it [00:01, 1970.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.033 - train acc: 99.889 - val loss: 0.046 - val acc: 99.000\n",
            "model_2: train loss: 0.027 - train acc: 99.963 - val loss: 0.048 - val acc: 99.333\n",
            "model_3: train loss: 0.032 - train acc: 99.926 - val loss: 0.053 - val acc: 99.000\n",
            "model_4: train loss: 0.029 - train acc: 99.889 - val loss: 0.051 - val acc: 99.000\n",
            "model_5: train loss: 0.032 - train acc: 99.889 - val loss: 0.067 - val acc: 98.667\n",
            "model_6: train loss: 0.027 - train acc: 99.963 - val loss: 0.048 - val acc: 99.000\n",
            "model_7: train loss: 0.029 - train acc: 99.926 - val loss: 0.053 - val acc: 99.000\n",
            "model_8: train loss: 0.029 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_9: train loss: 0.026 - train acc: 100.000 - val loss: 0.052 - val acc: 99.000\n",
            "model_10: train loss: 0.031 - train acc: 99.963 - val loss: 0.068 - val acc: 98.000\n",
            "\n",
            "Epoch: 43/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.033 - model1_acc: 99.889: : 2694it [00:01, 2145.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.033 - train acc: 99.889 - val loss: 0.072 - val acc: 98.333\n",
            "model_2: train loss: 0.027 - train acc: 100.000 - val loss: 0.050 - val acc: 99.000\n",
            "model_3: train loss: 0.030 - train acc: 99.963 - val loss: 0.054 - val acc: 99.000\n",
            "model_4: train loss: 0.029 - train acc: 100.000 - val loss: 0.059 - val acc: 99.000\n",
            "model_5: train loss: 0.029 - train acc: 100.000 - val loss: 0.059 - val acc: 98.667\n",
            "model_6: train loss: 0.027 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "model_7: train loss: 0.035 - train acc: 99.852 - val loss: 0.076 - val acc: 98.667\n",
            "model_8: train loss: 0.027 - train acc: 100.000 - val loss: 0.051 - val acc: 99.000\n",
            "model_9: train loss: 0.030 - train acc: 99.889 - val loss: 0.061 - val acc: 99.000\n",
            "model_10: train loss: 0.040 - train acc: 99.777 - val loss: 0.090 - val acc: 98.000\n",
            "\n",
            "Epoch: 44/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.035 - model1_acc: 99.852: : 2694it [00:01, 2235.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.035 - train acc: 99.852 - val loss: 0.063 - val acc: 98.000\n",
            "model_2: train loss: 0.027 - train acc: 100.000 - val loss: 0.046 - val acc: 99.000\n",
            "model_3: train loss: 0.031 - train acc: 100.000 - val loss: 0.054 - val acc: 99.000\n",
            "model_4: train loss: 0.032 - train acc: 99.926 - val loss: 0.047 - val acc: 99.000\n",
            "model_5: train loss: 0.033 - train acc: 99.852 - val loss: 0.060 - val acc: 98.667\n",
            "model_6: train loss: 0.028 - train acc: 99.963 - val loss: 0.046 - val acc: 99.000\n",
            "model_7: train loss: 0.044 - train acc: 99.740 - val loss: 0.083 - val acc: 98.333\n",
            "model_8: train loss: 0.032 - train acc: 99.852 - val loss: 0.060 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 99.926 - val loss: 0.064 - val acc: 98.667\n",
            "model_10: train loss: 0.040 - train acc: 99.777 - val loss: 0.054 - val acc: 99.333\n",
            "\n",
            "Epoch: 45/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.036 - model1_acc: 99.926: : 2694it [00:01, 2218.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.036 - train acc: 99.926 - val loss: 0.056 - val acc: 99.333\n",
            "model_2: train loss: 0.028 - train acc: 100.000 - val loss: 0.049 - val acc: 98.667\n",
            "model_3: train loss: 0.033 - train acc: 99.926 - val loss: 0.054 - val acc: 99.333\n",
            "model_4: train loss: 0.032 - train acc: 100.000 - val loss: 0.055 - val acc: 99.000\n",
            "model_5: train loss: 0.034 - train acc: 99.926 - val loss: 0.071 - val acc: 98.667\n",
            "model_6: train loss: 0.033 - train acc: 99.963 - val loss: 0.062 - val acc: 99.333\n",
            "model_7: train loss: 0.045 - train acc: 99.629 - val loss: 0.058 - val acc: 99.000\n",
            "model_8: train loss: 0.040 - train acc: 99.814 - val loss: 0.059 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 99.963 - val loss: 0.091 - val acc: 97.667\n",
            "model_10: train loss: 0.038 - train acc: 99.889 - val loss: 0.071 - val acc: 98.667\n",
            "\n",
            "Epoch: 46/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.035 - model1_acc: 99.852: : 2694it [00:01, 2228.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.035 - train acc: 99.852 - val loss: 0.052 - val acc: 98.667\n",
            "model_2: train loss: 0.028 - train acc: 99.963 - val loss: 0.048 - val acc: 99.333\n",
            "model_3: train loss: 0.031 - train acc: 99.926 - val loss: 0.054 - val acc: 99.000\n",
            "model_4: train loss: 0.031 - train acc: 100.000 - val loss: 0.063 - val acc: 98.667\n",
            "model_5: train loss: 0.035 - train acc: 99.814 - val loss: 0.051 - val acc: 99.000\n",
            "model_6: train loss: 0.032 - train acc: 99.963 - val loss: 0.066 - val acc: 98.667\n",
            "model_7: train loss: 0.034 - train acc: 99.852 - val loss: 0.058 - val acc: 99.000\n",
            "model_8: train loss: 0.032 - train acc: 99.889 - val loss: 0.046 - val acc: 98.667\n",
            "model_9: train loss: 0.033 - train acc: 99.889 - val loss: 0.057 - val acc: 99.000\n",
            "model_10: train loss: 0.035 - train acc: 99.814 - val loss: 0.055 - val acc: 99.000\n",
            "\n",
            "Epoch: 47/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.029 - model1_acc: 100.000: : 2694it [00:01, 2236.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 100.000 - val loss: 0.047 - val acc: 99.000\n",
            "model_2: train loss: 0.028 - train acc: 100.000 - val loss: 0.050 - val acc: 98.667\n",
            "model_3: train loss: 0.032 - train acc: 99.852 - val loss: 0.051 - val acc: 98.667\n",
            "model_4: train loss: 0.029 - train acc: 99.963 - val loss: 0.054 - val acc: 99.000\n",
            "model_5: train loss: 0.031 - train acc: 99.889 - val loss: 0.054 - val acc: 99.000\n",
            "model_6: train loss: 0.037 - train acc: 99.814 - val loss: 0.052 - val acc: 99.333\n",
            "model_7: train loss: 0.031 - train acc: 99.963 - val loss: 0.068 - val acc: 98.667\n",
            "model_8: train loss: 0.031 - train acc: 99.963 - val loss: 0.056 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 99.926 - val loss: 0.063 - val acc: 98.667\n",
            "model_10: train loss: 0.032 - train acc: 99.852 - val loss: 0.056 - val acc: 99.000\n",
            "\n",
            "Epoch: 48/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.027 - model1_acc: 100.000: : 2694it [00:01, 2256.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.027 - train acc: 100.000 - val loss: 0.048 - val acc: 99.000\n",
            "model_2: train loss: 0.028 - train acc: 99.926 - val loss: 0.048 - val acc: 99.000\n",
            "model_3: train loss: 0.030 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "model_4: train loss: 0.030 - train acc: 99.889 - val loss: 0.057 - val acc: 98.667\n",
            "model_5: train loss: 0.030 - train acc: 99.926 - val loss: 0.057 - val acc: 98.667\n",
            "model_6: train loss: 0.035 - train acc: 99.926 - val loss: 0.050 - val acc: 98.667\n",
            "model_7: train loss: 0.031 - train acc: 99.814 - val loss: 0.055 - val acc: 99.000\n",
            "model_8: train loss: 0.034 - train acc: 99.852 - val loss: 0.052 - val acc: 99.000\n",
            "model_9: train loss: 0.027 - train acc: 100.000 - val loss: 0.057 - val acc: 98.667\n",
            "model_10: train loss: 0.030 - train acc: 99.926 - val loss: 0.065 - val acc: 98.333\n",
            "\n",
            "Epoch: 49/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2230.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.045 - val acc: 99.000\n",
            "model_2: train loss: 0.042 - train acc: 99.666 - val loss: 0.071 - val acc: 98.667\n",
            "model_3: train loss: 0.035 - train acc: 99.740 - val loss: 0.056 - val acc: 99.000\n",
            "model_4: train loss: 0.029 - train acc: 99.889 - val loss: 0.048 - val acc: 98.667\n",
            "model_5: train loss: 0.036 - train acc: 99.814 - val loss: 0.066 - val acc: 98.667\n",
            "model_6: train loss: 0.031 - train acc: 99.889 - val loss: 0.058 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.889 - val loss: 0.061 - val acc: 99.333\n",
            "model_8: train loss: 0.029 - train acc: 99.963 - val loss: 0.050 - val acc: 98.667\n",
            "model_9: train loss: 0.031 - train acc: 100.000 - val loss: 0.060 - val acc: 99.000\n",
            "model_10: train loss: 0.032 - train acc: 99.889 - val loss: 0.077 - val acc: 98.333\n",
            "\n",
            "Epoch: 50/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.029 - model1_acc: 99.963: : 2694it [00:01, 2211.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 99.963 - val loss: 0.054 - val acc: 98.667\n",
            "model_2: train loss: 0.046 - train acc: 99.777 - val loss: 0.070 - val acc: 98.667\n",
            "model_3: train loss: 0.037 - train acc: 99.963 - val loss: 0.057 - val acc: 99.333\n",
            "model_4: train loss: 0.031 - train acc: 99.926 - val loss: 0.048 - val acc: 99.000\n",
            "model_5: train loss: 0.038 - train acc: 99.963 - val loss: 0.068 - val acc: 99.333\n",
            "model_6: train loss: 0.033 - train acc: 99.963 - val loss: 0.064 - val acc: 98.333\n",
            "model_7: train loss: 0.033 - train acc: 99.926 - val loss: 0.050 - val acc: 99.000\n",
            "model_8: train loss: 0.035 - train acc: 99.889 - val loss: 0.067 - val acc: 98.667\n",
            "model_9: train loss: 0.042 - train acc: 99.777 - val loss: 0.073 - val acc: 98.333\n",
            "model_10: train loss: 0.037 - train acc: 99.889 - val loss: 0.065 - val acc: 99.000\n",
            "\n",
            "Epoch: 51/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.029 - model1_acc: 99.963: : 2694it [00:01, 2066.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 99.963 - val loss: 0.047 - val acc: 98.667\n",
            "model_2: train loss: 0.043 - train acc: 99.814 - val loss: 0.052 - val acc: 98.667\n",
            "model_3: train loss: 0.033 - train acc: 99.926 - val loss: 0.050 - val acc: 99.000\n",
            "model_4: train loss: 0.031 - train acc: 99.963 - val loss: 0.045 - val acc: 99.000\n",
            "model_5: train loss: 0.040 - train acc: 99.852 - val loss: 0.060 - val acc: 98.667\n",
            "model_6: train loss: 0.033 - train acc: 100.000 - val loss: 0.041 - val acc: 99.000\n",
            "model_7: train loss: 0.035 - train acc: 99.889 - val loss: 0.046 - val acc: 99.000\n",
            "model_8: train loss: 0.035 - train acc: 99.889 - val loss: 0.053 - val acc: 98.667\n",
            "model_9: train loss: 0.035 - train acc: 100.000 - val loss: 0.054 - val acc: 99.000\n",
            "model_10: train loss: 0.035 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "\n",
            "Epoch: 52/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.028 - model1_acc: 99.963: : 2694it [00:01, 2041.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.028 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "model_2: train loss: 0.041 - train acc: 99.740 - val loss: 0.064 - val acc: 99.333\n",
            "model_3: train loss: 0.031 - train acc: 99.963 - val loss: 0.048 - val acc: 99.000\n",
            "model_4: train loss: 0.029 - train acc: 99.963 - val loss: 0.051 - val acc: 99.000\n",
            "model_5: train loss: 0.030 - train acc: 99.926 - val loss: 0.053 - val acc: 99.000\n",
            "model_6: train loss: 0.029 - train acc: 99.926 - val loss: 0.051 - val acc: 99.333\n",
            "model_7: train loss: 0.030 - train acc: 99.889 - val loss: 0.053 - val acc: 98.667\n",
            "model_8: train loss: 0.030 - train acc: 99.926 - val loss: 0.044 - val acc: 98.667\n",
            "model_9: train loss: 0.041 - train acc: 99.740 - val loss: 0.061 - val acc: 99.000\n",
            "model_10: train loss: 0.029 - train acc: 99.852 - val loss: 0.050 - val acc: 99.333\n",
            "\n",
            "Epoch: 53/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.030 - model1_acc: 99.926: : 2694it [00:01, 2193.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.030 - train acc: 99.926 - val loss: 0.047 - val acc: 99.000\n",
            "model_2: train loss: 0.036 - train acc: 99.889 - val loss: 0.048 - val acc: 99.000\n",
            "model_3: train loss: 0.038 - train acc: 99.926 - val loss: 0.056 - val acc: 99.333\n",
            "model_4: train loss: 0.031 - train acc: 100.000 - val loss: 0.052 - val acc: 98.667\n",
            "model_5: train loss: 0.029 - train acc: 99.963 - val loss: 0.055 - val acc: 98.333\n",
            "model_6: train loss: 0.031 - train acc: 99.963 - val loss: 0.052 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_8: train loss: 0.031 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "model_9: train loss: 0.051 - train acc: 99.592 - val loss: 0.060 - val acc: 99.000\n",
            "model_10: train loss: 0.031 - train acc: 99.926 - val loss: 0.052 - val acc: 99.000\n",
            "\n",
            "Epoch: 54/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.030 - model1_acc: 99.852: : 2694it [00:01, 2201.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.030 - train acc: 99.852 - val loss: 0.054 - val acc: 98.667\n",
            "model_2: train loss: 0.031 - train acc: 99.926 - val loss: 0.059 - val acc: 99.000\n",
            "model_3: train loss: 0.034 - train acc: 99.852 - val loss: 0.061 - val acc: 99.000\n",
            "model_4: train loss: 0.032 - train acc: 99.963 - val loss: 0.065 - val acc: 98.667\n",
            "model_5: train loss: 0.028 - train acc: 99.926 - val loss: 0.055 - val acc: 98.667\n",
            "model_6: train loss: 0.029 - train acc: 99.926 - val loss: 0.044 - val acc: 98.667\n",
            "model_7: train loss: 0.029 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 99.926 - val loss: 0.048 - val acc: 99.000\n",
            "model_9: train loss: 0.033 - train acc: 99.889 - val loss: 0.074 - val acc: 97.667\n",
            "model_10: train loss: 0.027 - train acc: 99.926 - val loss: 0.056 - val acc: 99.333\n",
            "\n",
            "Epoch: 55/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.036 - model1_acc: 99.889: : 2694it [00:01, 2207.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.036 - train acc: 99.889 - val loss: 0.074 - val acc: 98.333\n",
            "model_2: train loss: 0.030 - train acc: 99.963 - val loss: 0.054 - val acc: 99.000\n",
            "model_3: train loss: 0.042 - train acc: 99.777 - val loss: 0.070 - val acc: 98.333\n",
            "model_4: train loss: 0.037 - train acc: 99.889 - val loss: 0.065 - val acc: 99.333\n",
            "model_5: train loss: 0.029 - train acc: 100.000 - val loss: 0.051 - val acc: 99.000\n",
            "model_6: train loss: 0.030 - train acc: 99.963 - val loss: 0.049 - val acc: 99.000\n",
            "model_7: train loss: 0.032 - train acc: 99.926 - val loss: 0.060 - val acc: 99.333\n",
            "model_8: train loss: 0.029 - train acc: 100.000 - val loss: 0.049 - val acc: 98.667\n",
            "model_9: train loss: 0.042 - train acc: 99.666 - val loss: 0.074 - val acc: 98.667\n",
            "model_10: train loss: 0.030 - train acc: 99.963 - val loss: 0.054 - val acc: 99.000\n",
            "\n",
            "Epoch: 56/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.041 - model1_acc: 99.629: : 2694it [00:01, 2181.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.041 - train acc: 99.629 - val loss: 0.048 - val acc: 99.000\n",
            "model_2: train loss: 0.029 - train acc: 99.963 - val loss: 0.045 - val acc: 99.000\n",
            "model_3: train loss: 0.045 - train acc: 99.740 - val loss: 0.062 - val acc: 99.333\n",
            "model_4: train loss: 0.036 - train acc: 99.777 - val loss: 0.048 - val acc: 99.000\n",
            "model_5: train loss: 0.029 - train acc: 100.000 - val loss: 0.045 - val acc: 99.000\n",
            "model_6: train loss: 0.031 - train acc: 99.963 - val loss: 0.049 - val acc: 98.667\n",
            "model_7: train loss: 0.031 - train acc: 99.889 - val loss: 0.050 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 100.000 - val loss: 0.042 - val acc: 98.667\n",
            "model_9: train loss: 0.037 - train acc: 99.852 - val loss: 0.069 - val acc: 98.333\n",
            "model_10: train loss: 0.029 - train acc: 100.000 - val loss: 0.049 - val acc: 98.667\n",
            "\n",
            "Epoch: 57/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.029 - model1_acc: 99.963: : 2694it [00:01, 2174.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 99.963 - val loss: 0.041 - val acc: 99.000\n",
            "model_2: train loss: 0.027 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_3: train loss: 0.044 - train acc: 99.443 - val loss: 0.074 - val acc: 99.000\n",
            "model_4: train loss: 0.031 - train acc: 99.926 - val loss: 0.054 - val acc: 98.667\n",
            "model_5: train loss: 0.029 - train acc: 99.963 - val loss: 0.050 - val acc: 98.667\n",
            "model_6: train loss: 0.028 - train acc: 99.926 - val loss: 0.047 - val acc: 99.000\n",
            "model_7: train loss: 0.027 - train acc: 99.926 - val loss: 0.043 - val acc: 99.000\n",
            "model_8: train loss: 0.027 - train acc: 100.000 - val loss: 0.051 - val acc: 99.000\n",
            "model_9: train loss: 0.031 - train acc: 99.926 - val loss: 0.067 - val acc: 98.333\n",
            "model_10: train loss: 0.028 - train acc: 100.000 - val loss: 0.058 - val acc: 99.000\n",
            "\n",
            "Epoch: 58/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.029 - model1_acc: 99.963: : 2694it [00:01, 2203.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 99.963 - val loss: 0.048 - val acc: 98.333\n",
            "model_2: train loss: 0.026 - train acc: 99.963 - val loss: 0.049 - val acc: 99.000\n",
            "model_3: train loss: 0.038 - train acc: 99.777 - val loss: 0.051 - val acc: 99.333\n",
            "model_4: train loss: 0.028 - train acc: 100.000 - val loss: 0.043 - val acc: 99.000\n",
            "model_5: train loss: 0.027 - train acc: 100.000 - val loss: 0.055 - val acc: 99.000\n",
            "model_6: train loss: 0.026 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_7: train loss: 0.027 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_8: train loss: 0.027 - train acc: 100.000 - val loss: 0.042 - val acc: 99.000\n",
            "model_9: train loss: 0.029 - train acc: 99.963 - val loss: 0.059 - val acc: 99.000\n",
            "model_10: train loss: 0.028 - train acc: 99.963 - val loss: 0.055 - val acc: 98.667\n",
            "\n",
            "Epoch: 59/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.024 - model1_acc: 100.000: : 2694it [00:01, 2227.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.024 - train acc: 100.000 - val loss: 0.047 - val acc: 99.000\n",
            "model_2: train loss: 0.024 - train acc: 100.000 - val loss: 0.043 - val acc: 99.000\n",
            "model_3: train loss: 0.027 - train acc: 99.926 - val loss: 0.050 - val acc: 99.000\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_5: train loss: 0.027 - train acc: 99.889 - val loss: 0.050 - val acc: 99.000\n",
            "model_6: train loss: 0.024 - train acc: 99.926 - val loss: 0.053 - val acc: 98.667\n",
            "model_7: train loss: 0.024 - train acc: 99.926 - val loss: 0.039 - val acc: 99.000\n",
            "model_8: train loss: 0.024 - train acc: 99.963 - val loss: 0.040 - val acc: 99.333\n",
            "model_9: train loss: 0.024 - train acc: 100.000 - val loss: 0.059 - val acc: 98.667\n",
            "model_10: train loss: 0.025 - train acc: 100.000 - val loss: 0.049 - val acc: 99.000\n",
            "\n",
            "Epoch: 60/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2088.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_2: train loss: 0.024 - train acc: 100.000 - val loss: 0.049 - val acc: 99.000\n",
            "model_3: train loss: 0.027 - train acc: 99.926 - val loss: 0.050 - val acc: 99.000\n",
            "model_4: train loss: 0.023 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_5: train loss: 0.031 - train acc: 99.889 - val loss: 0.047 - val acc: 98.667\n",
            "model_6: train loss: 0.029 - train acc: 99.777 - val loss: 0.045 - val acc: 99.000\n",
            "model_7: train loss: 0.024 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 100.000 - val loss: 0.067 - val acc: 98.333\n",
            "model_9: train loss: 0.026 - train acc: 99.963 - val loss: 0.054 - val acc: 98.667\n",
            "model_10: train loss: 0.025 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "\n",
            "Epoch: 61/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.024 - model1_acc: 100.000: : 2694it [00:01, 2022.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.024 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_2: train loss: 0.023 - train acc: 100.000 - val loss: 0.046 - val acc: 99.000\n",
            "model_3: train loss: 0.024 - train acc: 100.000 - val loss: 0.048 - val acc: 99.333\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.042 - val acc: 99.000\n",
            "model_5: train loss: 0.027 - train acc: 99.889 - val loss: 0.058 - val acc: 99.000\n",
            "model_6: train loss: 0.024 - train acc: 99.963 - val loss: 0.048 - val acc: 98.667\n",
            "model_7: train loss: 0.023 - train acc: 99.963 - val loss: 0.051 - val acc: 99.000\n",
            "model_8: train loss: 0.032 - train acc: 99.740 - val loss: 0.054 - val acc: 98.667\n",
            "model_9: train loss: 0.026 - train acc: 99.963 - val loss: 0.065 - val acc: 99.000\n",
            "model_10: train loss: 0.025 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "\n",
            "Epoch: 62/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.027 - model1_acc: 99.963: : 2694it [00:01, 2110.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.027 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_2: train loss: 0.027 - train acc: 100.000 - val loss: 0.046 - val acc: 99.000\n",
            "model_3: train loss: 0.029 - train acc: 100.000 - val loss: 0.049 - val acc: 99.000\n",
            "model_4: train loss: 0.026 - train acc: 100.000 - val loss: 0.045 - val acc: 99.000\n",
            "model_5: train loss: 0.043 - train acc: 99.629 - val loss: 0.052 - val acc: 98.667\n",
            "model_6: train loss: 0.029 - train acc: 99.926 - val loss: 0.050 - val acc: 99.000\n",
            "model_7: train loss: 0.032 - train acc: 99.814 - val loss: 0.049 - val acc: 99.000\n",
            "model_8: train loss: 0.046 - train acc: 99.740 - val loss: 0.050 - val acc: 99.333\n",
            "model_9: train loss: 0.028 - train acc: 100.000 - val loss: 0.054 - val acc: 98.667\n",
            "model_10: train loss: 0.028 - train acc: 100.000 - val loss: 0.044 - val acc: 99.333\n",
            "\n",
            "Epoch: 63/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2206.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.050 - val acc: 98.667\n",
            "model_2: train loss: 0.025 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_3: train loss: 0.028 - train acc: 100.000 - val loss: 0.055 - val acc: 99.333\n",
            "model_4: train loss: 0.025 - train acc: 100.000 - val loss: 0.041 - val acc: 99.000\n",
            "model_5: train loss: 0.031 - train acc: 99.889 - val loss: 0.050 - val acc: 98.667\n",
            "model_6: train loss: 0.027 - train acc: 99.963 - val loss: 0.042 - val acc: 99.000\n",
            "model_7: train loss: 0.032 - train acc: 99.889 - val loss: 0.052 - val acc: 99.333\n",
            "model_8: train loss: 0.039 - train acc: 99.926 - val loss: 0.052 - val acc: 98.667\n",
            "model_9: train loss: 0.027 - train acc: 99.963 - val loss: 0.049 - val acc: 99.000\n",
            "model_10: train loss: 0.027 - train acc: 100.000 - val loss: 0.051 - val acc: 99.000\n",
            "\n",
            "Epoch: 64/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.027 - model1_acc: 100.000: : 2694it [00:01, 2232.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.027 - train acc: 100.000 - val loss: 0.047 - val acc: 98.667\n",
            "model_2: train loss: 0.025 - train acc: 100.000 - val loss: 0.050 - val acc: 99.000\n",
            "model_3: train loss: 0.029 - train acc: 99.963 - val loss: 0.070 - val acc: 99.333\n",
            "model_4: train loss: 0.028 - train acc: 99.963 - val loss: 0.054 - val acc: 99.000\n",
            "model_5: train loss: 0.030 - train acc: 99.963 - val loss: 0.051 - val acc: 99.000\n",
            "model_6: train loss: 0.027 - train acc: 99.963 - val loss: 0.056 - val acc: 99.000\n",
            "model_7: train loss: 0.029 - train acc: 99.814 - val loss: 0.065 - val acc: 98.667\n",
            "model_8: train loss: 0.038 - train acc: 99.777 - val loss: 0.059 - val acc: 98.667\n",
            "model_9: train loss: 0.031 - train acc: 99.852 - val loss: 0.054 - val acc: 99.000\n",
            "model_10: train loss: 0.027 - train acc: 99.963 - val loss: 0.063 - val acc: 98.000\n",
            "\n",
            "Epoch: 65/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2223.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.046 - val acc: 98.333\n",
            "model_2: train loss: 0.028 - train acc: 99.963 - val loss: 0.045 - val acc: 99.000\n",
            "model_3: train loss: 0.032 - train acc: 100.000 - val loss: 0.060 - val acc: 99.333\n",
            "model_4: train loss: 0.029 - train acc: 99.926 - val loss: 0.049 - val acc: 99.000\n",
            "model_5: train loss: 0.027 - train acc: 99.963 - val loss: 0.049 - val acc: 98.667\n",
            "model_6: train loss: 0.028 - train acc: 100.000 - val loss: 0.046 - val acc: 99.000\n",
            "model_7: train loss: 0.036 - train acc: 99.740 - val loss: 0.062 - val acc: 99.000\n",
            "model_8: train loss: 0.033 - train acc: 99.852 - val loss: 0.054 - val acc: 98.667\n",
            "model_9: train loss: 0.036 - train acc: 99.814 - val loss: 0.076 - val acc: 98.333\n",
            "model_10: train loss: 0.030 - train acc: 99.889 - val loss: 0.064 - val acc: 98.667\n",
            "\n",
            "Epoch: 66/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.030 - model1_acc: 99.926: : 2694it [00:01, 2220.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.030 - train acc: 99.926 - val loss: 0.055 - val acc: 98.667\n",
            "model_2: train loss: 0.029 - train acc: 100.000 - val loss: 0.055 - val acc: 99.333\n",
            "model_3: train loss: 0.035 - train acc: 99.814 - val loss: 0.054 - val acc: 98.667\n",
            "model_4: train loss: 0.030 - train acc: 99.889 - val loss: 0.050 - val acc: 99.000\n",
            "model_5: train loss: 0.032 - train acc: 99.926 - val loss: 0.058 - val acc: 99.000\n",
            "model_6: train loss: 0.028 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_7: train loss: 0.039 - train acc: 99.814 - val loss: 0.068 - val acc: 98.667\n",
            "model_8: train loss: 0.030 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_9: train loss: 0.035 - train acc: 99.963 - val loss: 0.068 - val acc: 99.000\n",
            "model_10: train loss: 0.037 - train acc: 99.777 - val loss: 0.065 - val acc: 98.000\n",
            "\n",
            "Epoch: 67/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.032 - model1_acc: 99.926: : 2694it [00:01, 2193.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.032 - train acc: 99.926 - val loss: 0.047 - val acc: 98.667\n",
            "model_2: train loss: 0.030 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_3: train loss: 0.042 - train acc: 99.852 - val loss: 0.058 - val acc: 99.000\n",
            "model_4: train loss: 0.030 - train acc: 99.963 - val loss: 0.044 - val acc: 98.667\n",
            "model_5: train loss: 0.039 - train acc: 99.777 - val loss: 0.057 - val acc: 98.333\n",
            "model_6: train loss: 0.031 - train acc: 100.000 - val loss: 0.051 - val acc: 99.333\n",
            "model_7: train loss: 0.040 - train acc: 99.852 - val loss: 0.060 - val acc: 99.000\n",
            "model_8: train loss: 0.030 - train acc: 100.000 - val loss: 0.048 - val acc: 99.000\n",
            "model_9: train loss: 0.056 - train acc: 99.443 - val loss: 0.068 - val acc: 99.000\n",
            "model_10: train loss: 0.042 - train acc: 99.814 - val loss: 0.062 - val acc: 98.667\n",
            "\n",
            "Epoch: 68/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.032 - model1_acc: 100.000: : 2694it [00:01, 2186.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.032 - train acc: 100.000 - val loss: 0.049 - val acc: 98.667\n",
            "model_2: train loss: 0.033 - train acc: 99.963 - val loss: 0.056 - val acc: 98.667\n",
            "model_3: train loss: 0.041 - train acc: 99.740 - val loss: 0.065 - val acc: 99.000\n",
            "model_4: train loss: 0.033 - train acc: 99.852 - val loss: 0.053 - val acc: 99.000\n",
            "model_5: train loss: 0.036 - train acc: 99.889 - val loss: 0.059 - val acc: 99.000\n",
            "model_6: train loss: 0.033 - train acc: 99.963 - val loss: 0.050 - val acc: 99.000\n",
            "model_7: train loss: 0.052 - train acc: 99.629 - val loss: 0.070 - val acc: 99.000\n",
            "model_8: train loss: 0.032 - train acc: 99.963 - val loss: 0.049 - val acc: 99.000\n",
            "model_9: train loss: 0.037 - train acc: 99.814 - val loss: 0.053 - val acc: 98.667\n",
            "model_10: train loss: 0.047 - train acc: 99.852 - val loss: 0.057 - val acc: 99.333\n",
            "\n",
            "Epoch: 69/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.032 - model1_acc: 99.852: : 2694it [00:01, 2191.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.032 - train acc: 99.852 - val loss: 0.058 - val acc: 98.667\n",
            "model_2: train loss: 0.030 - train acc: 99.963 - val loss: 0.046 - val acc: 99.333\n",
            "model_3: train loss: 0.038 - train acc: 99.852 - val loss: 0.059 - val acc: 98.667\n",
            "model_4: train loss: 0.031 - train acc: 99.926 - val loss: 0.052 - val acc: 98.667\n",
            "model_5: train loss: 0.031 - train acc: 99.889 - val loss: 0.051 - val acc: 98.667\n",
            "model_6: train loss: 0.030 - train acc: 99.926 - val loss: 0.045 - val acc: 99.333\n",
            "model_7: train loss: 0.033 - train acc: 99.889 - val loss: 0.062 - val acc: 98.667\n",
            "model_8: train loss: 0.027 - train acc: 99.963 - val loss: 0.049 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 99.926 - val loss: 0.060 - val acc: 99.000\n",
            "model_10: train loss: 0.042 - train acc: 99.814 - val loss: 0.070 - val acc: 98.667\n",
            "\n",
            "Epoch: 70/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.032 - model1_acc: 99.926: : 2694it [00:01, 2006.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.032 - train acc: 99.926 - val loss: 0.050 - val acc: 99.333\n",
            "model_2: train loss: 0.033 - train acc: 99.926 - val loss: 0.044 - val acc: 99.333\n",
            "model_3: train loss: 0.036 - train acc: 99.963 - val loss: 0.082 - val acc: 98.000\n",
            "model_4: train loss: 0.029 - train acc: 100.000 - val loss: 0.046 - val acc: 98.667\n",
            "model_5: train loss: 0.031 - train acc: 99.926 - val loss: 0.055 - val acc: 99.000\n",
            "model_6: train loss: 0.037 - train acc: 99.852 - val loss: 0.057 - val acc: 99.000\n",
            "model_7: train loss: 0.032 - train acc: 99.926 - val loss: 0.051 - val acc: 99.000\n",
            "model_8: train loss: 0.029 - train acc: 99.926 - val loss: 0.048 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 99.926 - val loss: 0.048 - val acc: 99.000\n",
            "model_10: train loss: 0.039 - train acc: 99.814 - val loss: 0.057 - val acc: 99.000\n",
            "\n",
            "Epoch: 71/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.028 - model1_acc: 100.000: : 2694it [00:01, 2046.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.028 - train acc: 100.000 - val loss: 0.044 - val acc: 99.333\n",
            "model_2: train loss: 0.031 - train acc: 99.814 - val loss: 0.045 - val acc: 99.333\n",
            "model_3: train loss: 0.045 - train acc: 99.666 - val loss: 0.069 - val acc: 98.667\n",
            "model_4: train loss: 0.027 - train acc: 99.926 - val loss: 0.042 - val acc: 99.000\n",
            "model_5: train loss: 0.031 - train acc: 99.926 - val loss: 0.054 - val acc: 99.000\n",
            "model_6: train loss: 0.032 - train acc: 99.889 - val loss: 0.045 - val acc: 99.000\n",
            "model_7: train loss: 0.033 - train acc: 99.889 - val loss: 0.067 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "model_9: train loss: 0.027 - train acc: 100.000 - val loss: 0.053 - val acc: 99.000\n",
            "model_10: train loss: 0.032 - train acc: 99.889 - val loss: 0.049 - val acc: 99.000\n",
            "\n",
            "Epoch: 72/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.042 - model1_acc: 99.740: : 2694it [00:01, 2222.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.042 - train acc: 99.740 - val loss: 0.082 - val acc: 98.000\n",
            "model_2: train loss: 0.048 - train acc: 99.629 - val loss: 0.063 - val acc: 99.000\n",
            "model_3: train loss: 0.057 - train acc: 99.555 - val loss: 0.112 - val acc: 98.000\n",
            "model_4: train loss: 0.031 - train acc: 99.963 - val loss: 0.051 - val acc: 98.667\n",
            "model_5: train loss: 0.034 - train acc: 99.889 - val loss: 0.067 - val acc: 98.667\n",
            "model_6: train loss: 0.036 - train acc: 99.852 - val loss: 0.059 - val acc: 99.000\n",
            "model_7: train loss: 0.050 - train acc: 99.629 - val loss: 0.095 - val acc: 98.000\n",
            "model_8: train loss: 0.029 - train acc: 100.000 - val loss: 0.052 - val acc: 98.667\n",
            "model_9: train loss: 0.031 - train acc: 100.000 - val loss: 0.058 - val acc: 99.000\n",
            "model_10: train loss: 0.031 - train acc: 100.000 - val loss: 0.055 - val acc: 99.000\n",
            "\n",
            "Epoch: 73/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.050 - model1_acc: 99.666: : 2694it [00:01, 2100.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.050 - train acc: 99.666 - val loss: 0.065 - val acc: 99.000\n",
            "model_2: train loss: 0.040 - train acc: 99.852 - val loss: 0.050 - val acc: 99.000\n",
            "model_3: train loss: 0.053 - train acc: 99.777 - val loss: 0.067 - val acc: 99.333\n",
            "model_4: train loss: 0.035 - train acc: 99.852 - val loss: 0.047 - val acc: 99.000\n",
            "model_5: train loss: 0.038 - train acc: 99.926 - val loss: 0.074 - val acc: 98.333\n",
            "model_6: train loss: 0.042 - train acc: 99.852 - val loss: 0.056 - val acc: 99.000\n",
            "model_7: train loss: 0.056 - train acc: 99.480 - val loss: 0.075 - val acc: 99.333\n",
            "model_8: train loss: 0.037 - train acc: 99.963 - val loss: 0.046 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 100.000 - val loss: 0.062 - val acc: 98.333\n",
            "model_10: train loss: 0.034 - train acc: 100.000 - val loss: 0.048 - val acc: 99.000\n",
            "\n",
            "Epoch: 74/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.041 - model1_acc: 99.926: : 2694it [00:01, 2153.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.041 - train acc: 99.926 - val loss: 0.059 - val acc: 99.333\n",
            "model_2: train loss: 0.029 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_3: train loss: 0.035 - train acc: 99.926 - val loss: 0.062 - val acc: 99.000\n",
            "model_4: train loss: 0.030 - train acc: 100.000 - val loss: 0.042 - val acc: 99.000\n",
            "model_5: train loss: 0.037 - train acc: 99.852 - val loss: 0.065 - val acc: 98.667\n",
            "model_6: train loss: 0.033 - train acc: 99.963 - val loss: 0.049 - val acc: 99.000\n",
            "model_7: train loss: 0.042 - train acc: 99.703 - val loss: 0.053 - val acc: 98.667\n",
            "model_8: train loss: 0.033 - train acc: 99.889 - val loss: 0.047 - val acc: 99.333\n",
            "model_9: train loss: 0.028 - train acc: 100.000 - val loss: 0.052 - val acc: 98.667\n",
            "model_10: train loss: 0.028 - train acc: 99.963 - val loss: 0.051 - val acc: 99.000\n",
            "\n",
            "Epoch: 75/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.034 - model1_acc: 99.889: : 2694it [00:01, 2172.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.034 - train acc: 99.889 - val loss: 0.046 - val acc: 99.000\n",
            "model_2: train loss: 0.029 - train acc: 99.926 - val loss: 0.047 - val acc: 99.333\n",
            "model_3: train loss: 0.033 - train acc: 100.000 - val loss: 0.055 - val acc: 99.000\n",
            "model_4: train loss: 0.025 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_5: train loss: 0.041 - train acc: 99.777 - val loss: 0.068 - val acc: 99.000\n",
            "model_6: train loss: 0.029 - train acc: 99.926 - val loss: 0.042 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.926 - val loss: 0.040 - val acc: 99.000\n",
            "model_8: train loss: 0.036 - train acc: 99.777 - val loss: 0.066 - val acc: 98.333\n",
            "model_9: train loss: 0.027 - train acc: 99.963 - val loss: 0.047 - val acc: 98.667\n",
            "model_10: train loss: 0.027 - train acc: 99.963 - val loss: 0.042 - val acc: 99.333\n",
            "\n",
            "Epoch: 76/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.025 - model1_acc: 99.963: : 2694it [00:01, 2185.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.025 - train acc: 99.963 - val loss: 0.048 - val acc: 98.667\n",
            "model_2: train loss: 0.026 - train acc: 100.000 - val loss: 0.045 - val acc: 99.000\n",
            "model_3: train loss: 0.027 - train acc: 100.000 - val loss: 0.058 - val acc: 99.333\n",
            "model_4: train loss: 0.021 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_5: train loss: 0.036 - train acc: 99.740 - val loss: 0.058 - val acc: 98.667\n",
            "model_6: train loss: 0.025 - train acc: 99.926 - val loss: 0.046 - val acc: 99.000\n",
            "model_7: train loss: 0.023 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_8: train loss: 0.030 - train acc: 99.926 - val loss: 0.044 - val acc: 99.333\n",
            "model_9: train loss: 0.023 - train acc: 100.000 - val loss: 0.047 - val acc: 99.000\n",
            "model_10: train loss: 0.025 - train acc: 99.926 - val loss: 0.045 - val acc: 99.000\n",
            "\n",
            "Epoch: 77/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 99.889: : 2694it [00:01, 2183.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 99.889 - val loss: 0.046 - val acc: 99.000\n",
            "model_2: train loss: 0.027 - train acc: 99.926 - val loss: 0.047 - val acc: 99.333\n",
            "model_3: train loss: 0.026 - train acc: 99.963 - val loss: 0.046 - val acc: 99.000\n",
            "model_4: train loss: 0.021 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_5: train loss: 0.026 - train acc: 99.963 - val loss: 0.047 - val acc: 98.667\n",
            "model_6: train loss: 0.024 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_7: train loss: 0.025 - train acc: 99.889 - val loss: 0.043 - val acc: 99.000\n",
            "model_8: train loss: 0.025 - train acc: 99.926 - val loss: 0.041 - val acc: 99.000\n",
            "model_9: train loss: 0.022 - train acc: 100.000 - val loss: 0.049 - val acc: 99.000\n",
            "model_10: train loss: 0.025 - train acc: 99.926 - val loss: 0.047 - val acc: 99.000\n",
            "\n",
            "Epoch: 78/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 99.889: : 2694it [00:01, 2219.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 99.889 - val loss: 0.043 - val acc: 99.000\n",
            "model_2: train loss: 0.025 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_3: train loss: 0.025 - train acc: 99.889 - val loss: 0.049 - val acc: 99.000\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_5: train loss: 0.025 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "model_6: train loss: 0.024 - train acc: 99.963 - val loss: 0.042 - val acc: 99.333\n",
            "model_7: train loss: 0.023 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_8: train loss: 0.026 - train acc: 99.889 - val loss: 0.045 - val acc: 98.667\n",
            "model_9: train loss: 0.026 - train acc: 99.926 - val loss: 0.046 - val acc: 99.000\n",
            "model_10: train loss: 0.026 - train acc: 100.000 - val loss: 0.037 - val acc: 99.333\n",
            "\n",
            "Epoch: 79/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.4s - model1_loss: 0.023 - model1_acc: 99.963: : 2694it [00:01, 1951.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.023 - train acc: 99.963 - val loss: 0.039 - val acc: 99.000\n",
            "model_2: train loss: 0.025 - train acc: 99.852 - val loss: 0.038 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 100.000 - val loss: 0.046 - val acc: 99.000\n",
            "model_4: train loss: 0.021 - train acc: 100.000 - val loss: 0.036 - val acc: 99.000\n",
            "model_5: train loss: 0.023 - train acc: 100.000 - val loss: 0.039 - val acc: 99.000\n",
            "model_6: train loss: 0.022 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_7: train loss: 0.023 - train acc: 100.000 - val loss: 0.039 - val acc: 99.000\n",
            "model_8: train loss: 0.027 - train acc: 99.889 - val loss: 0.045 - val acc: 98.667\n",
            "model_9: train loss: 0.023 - train acc: 99.963 - val loss: 0.053 - val acc: 99.000\n",
            "model_10: train loss: 0.023 - train acc: 99.963 - val loss: 0.037 - val acc: 99.000\n",
            "\n",
            "Epoch: 80/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.023 - model1_acc: 100.000: : 2694it [00:01, 2097.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.023 - train acc: 100.000 - val loss: 0.039 - val acc: 99.333\n",
            "model_2: train loss: 0.026 - train acc: 99.963 - val loss: 0.041 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 100.000 - val loss: 0.051 - val acc: 99.000\n",
            "model_4: train loss: 0.024 - train acc: 99.926 - val loss: 0.039 - val acc: 98.667\n",
            "model_5: train loss: 0.023 - train acc: 100.000 - val loss: 0.048 - val acc: 99.000\n",
            "model_6: train loss: 0.023 - train acc: 99.889 - val loss: 0.040 - val acc: 99.000\n",
            "model_7: train loss: 0.021 - train acc: 99.963 - val loss: 0.038 - val acc: 99.000\n",
            "model_8: train loss: 0.027 - train acc: 99.926 - val loss: 0.046 - val acc: 98.667\n",
            "model_9: train loss: 0.027 - train acc: 99.926 - val loss: 0.050 - val acc: 98.667\n",
            "model_10: train loss: 0.022 - train acc: 100.000 - val loss: 0.048 - val acc: 98.667\n",
            "\n",
            "Epoch: 81/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.022 - model1_acc: 100.000: : 2694it [00:01, 2162.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.022 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_2: train loss: 0.025 - train acc: 99.963 - val loss: 0.041 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 100.000 - val loss: 0.060 - val acc: 99.333\n",
            "model_4: train loss: 0.028 - train acc: 99.889 - val loss: 0.040 - val acc: 99.667\n",
            "model_5: train loss: 0.025 - train acc: 100.000 - val loss: 0.056 - val acc: 99.000\n",
            "model_6: train loss: 0.026 - train acc: 99.926 - val loss: 0.037 - val acc: 99.333\n",
            "model_7: train loss: 0.025 - train acc: 99.926 - val loss: 0.041 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 99.889 - val loss: 0.047 - val acc: 99.000\n",
            "model_9: train loss: 0.024 - train acc: 100.000 - val loss: 0.051 - val acc: 99.000\n",
            "model_10: train loss: 0.027 - train acc: 99.889 - val loss: 0.051 - val acc: 99.000\n",
            "\n",
            "Epoch: 82/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.024 - model1_acc: 99.926: : 2694it [00:01, 2158.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.024 - train acc: 99.926 - val loss: 0.042 - val acc: 99.000\n",
            "model_2: train loss: 0.026 - train acc: 100.000 - val loss: 0.047 - val acc: 99.333\n",
            "model_3: train loss: 0.027 - train acc: 100.000 - val loss: 0.064 - val acc: 99.333\n",
            "model_4: train loss: 0.029 - train acc: 99.926 - val loss: 0.056 - val acc: 98.667\n",
            "model_5: train loss: 0.026 - train acc: 100.000 - val loss: 0.061 - val acc: 98.667\n",
            "model_6: train loss: 0.027 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_7: train loss: 0.026 - train acc: 99.889 - val loss: 0.046 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "model_9: train loss: 0.025 - train acc: 100.000 - val loss: 0.052 - val acc: 99.000\n",
            "model_10: train loss: 0.028 - train acc: 99.926 - val loss: 0.046 - val acc: 99.333\n",
            "\n",
            "Epoch: 83/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 99.963: : 2694it [00:01, 2189.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 99.963 - val loss: 0.042 - val acc: 98.667\n",
            "model_2: train loss: 0.030 - train acc: 99.963 - val loss: 0.064 - val acc: 99.333\n",
            "model_3: train loss: 0.031 - train acc: 99.926 - val loss: 0.049 - val acc: 99.333\n",
            "model_4: train loss: 0.040 - train acc: 99.777 - val loss: 0.074 - val acc: 98.000\n",
            "model_5: train loss: 0.029 - train acc: 99.926 - val loss: 0.049 - val acc: 98.667\n",
            "model_6: train loss: 0.027 - train acc: 99.926 - val loss: 0.042 - val acc: 99.000\n",
            "model_7: train loss: 0.030 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "model_8: train loss: 0.035 - train acc: 99.889 - val loss: 0.051 - val acc: 99.000\n",
            "model_9: train loss: 0.029 - train acc: 99.889 - val loss: 0.059 - val acc: 98.667\n",
            "model_10: train loss: 0.027 - train acc: 100.000 - val loss: 0.047 - val acc: 99.000\n",
            "\n",
            "Epoch: 84/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.025 - model1_acc: 100.000: : 2694it [00:01, 2237.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.025 - train acc: 100.000 - val loss: 0.044 - val acc: 99.333\n",
            "model_2: train loss: 0.032 - train acc: 99.926 - val loss: 0.060 - val acc: 99.000\n",
            "model_3: train loss: 0.036 - train acc: 99.926 - val loss: 0.052 - val acc: 99.000\n",
            "model_4: train loss: 0.049 - train acc: 99.592 - val loss: 0.082 - val acc: 98.333\n",
            "model_5: train loss: 0.033 - train acc: 99.852 - val loss: 0.052 - val acc: 99.000\n",
            "model_6: train loss: 0.027 - train acc: 99.963 - val loss: 0.046 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.852 - val loss: 0.051 - val acc: 99.000\n",
            "model_8: train loss: 0.034 - train acc: 99.814 - val loss: 0.046 - val acc: 98.667\n",
            "model_9: train loss: 0.031 - train acc: 99.963 - val loss: 0.053 - val acc: 99.000\n",
            "model_10: train loss: 0.036 - train acc: 99.814 - val loss: 0.091 - val acc: 97.667\n",
            "\n",
            "Epoch: 85/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.027 - model1_acc: 100.000: : 2694it [00:01, 2205.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.027 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_2: train loss: 0.034 - train acc: 99.889 - val loss: 0.041 - val acc: 99.000\n",
            "model_3: train loss: 0.034 - train acc: 99.926 - val loss: 0.060 - val acc: 98.667\n",
            "model_4: train loss: 0.051 - train acc: 99.777 - val loss: 0.054 - val acc: 99.667\n",
            "model_5: train loss: 0.031 - train acc: 100.000 - val loss: 0.068 - val acc: 99.333\n",
            "model_6: train loss: 0.028 - train acc: 99.963 - val loss: 0.050 - val acc: 99.000\n",
            "model_7: train loss: 0.034 - train acc: 99.963 - val loss: 0.043 - val acc: 99.000\n",
            "model_8: train loss: 0.034 - train acc: 99.926 - val loss: 0.046 - val acc: 99.000\n",
            "model_9: train loss: 0.029 - train acc: 100.000 - val loss: 0.049 - val acc: 98.667\n",
            "model_10: train loss: 0.049 - train acc: 99.517 - val loss: 0.052 - val acc: 99.333\n",
            "\n",
            "Epoch: 86/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2170.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_2: train loss: 0.035 - train acc: 99.889 - val loss: 0.065 - val acc: 98.000\n",
            "model_3: train loss: 0.039 - train acc: 99.814 - val loss: 0.058 - val acc: 99.333\n",
            "model_4: train loss: 0.041 - train acc: 99.777 - val loss: 0.042 - val acc: 99.333\n",
            "model_5: train loss: 0.038 - train acc: 99.852 - val loss: 0.055 - val acc: 99.000\n",
            "model_6: train loss: 0.030 - train acc: 99.963 - val loss: 0.043 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.889 - val loss: 0.045 - val acc: 99.000\n",
            "model_8: train loss: 0.032 - train acc: 99.889 - val loss: 0.060 - val acc: 99.000\n",
            "model_9: train loss: 0.032 - train acc: 99.963 - val loss: 0.055 - val acc: 99.000\n",
            "model_10: train loss: 0.038 - train acc: 99.926 - val loss: 0.056 - val acc: 99.000\n",
            "\n",
            "Epoch: 87/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2139.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_2: train loss: 0.038 - train acc: 99.814 - val loss: 0.062 - val acc: 99.000\n",
            "model_3: train loss: 0.036 - train acc: 99.889 - val loss: 0.057 - val acc: 99.000\n",
            "model_4: train loss: 0.029 - train acc: 99.889 - val loss: 0.049 - val acc: 99.333\n",
            "model_5: train loss: 0.038 - train acc: 99.889 - val loss: 0.048 - val acc: 99.000\n",
            "model_6: train loss: 0.032 - train acc: 99.926 - val loss: 0.043 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "model_8: train loss: 0.036 - train acc: 99.926 - val loss: 0.052 - val acc: 98.667\n",
            "model_9: train loss: 0.035 - train acc: 99.852 - val loss: 0.055 - val acc: 99.333\n",
            "model_10: train loss: 0.043 - train acc: 99.777 - val loss: 0.060 - val acc: 99.000\n",
            "\n",
            "Epoch: 88/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.027 - model1_acc: 99.963: : 2694it [00:01, 2067.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.027 - train acc: 99.963 - val loss: 0.045 - val acc: 99.000\n",
            "model_2: train loss: 0.036 - train acc: 99.814 - val loss: 0.067 - val acc: 99.000\n",
            "model_3: train loss: 0.032 - train acc: 99.852 - val loss: 0.059 - val acc: 99.000\n",
            "model_4: train loss: 0.029 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_5: train loss: 0.035 - train acc: 99.889 - val loss: 0.068 - val acc: 98.333\n",
            "model_6: train loss: 0.033 - train acc: 99.814 - val loss: 0.049 - val acc: 99.000\n",
            "model_7: train loss: 0.027 - train acc: 99.963 - val loss: 0.055 - val acc: 99.000\n",
            "model_8: train loss: 0.040 - train acc: 99.740 - val loss: 0.079 - val acc: 99.000\n",
            "model_9: train loss: 0.030 - train acc: 99.889 - val loss: 0.067 - val acc: 98.333\n",
            "model_10: train loss: 0.033 - train acc: 99.889 - val loss: 0.058 - val acc: 99.000\n",
            "\n",
            "Epoch: 89/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.025 - model1_acc: 99.963: : 2694it [00:01, 2043.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.025 - train acc: 99.963 - val loss: 0.040 - val acc: 99.000\n",
            "model_2: train loss: 0.035 - train acc: 99.926 - val loss: 0.048 - val acc: 99.000\n",
            "model_3: train loss: 0.032 - train acc: 100.000 - val loss: 0.048 - val acc: 99.000\n",
            "model_4: train loss: 0.027 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_5: train loss: 0.038 - train acc: 99.852 - val loss: 0.050 - val acc: 99.000\n",
            "model_6: train loss: 0.038 - train acc: 99.740 - val loss: 0.048 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.963 - val loss: 0.043 - val acc: 99.000\n",
            "model_8: train loss: 0.037 - train acc: 99.814 - val loss: 0.047 - val acc: 99.333\n",
            "model_9: train loss: 0.030 - train acc: 99.963 - val loss: 0.055 - val acc: 98.667\n",
            "model_10: train loss: 0.028 - train acc: 100.000 - val loss: 0.041 - val acc: 99.000\n",
            "\n",
            "Epoch: 90/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.023 - model1_acc: 100.000: : 2694it [00:01, 2141.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.023 - train acc: 100.000 - val loss: 0.042 - val acc: 99.000\n",
            "model_2: train loss: 0.028 - train acc: 99.926 - val loss: 0.039 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 99.963 - val loss: 0.054 - val acc: 99.000\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.039 - val acc: 99.333\n",
            "model_5: train loss: 0.033 - train acc: 99.889 - val loss: 0.066 - val acc: 98.667\n",
            "model_6: train loss: 0.033 - train acc: 99.889 - val loss: 0.050 - val acc: 99.000\n",
            "model_7: train loss: 0.028 - train acc: 99.852 - val loss: 0.040 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 99.926 - val loss: 0.044 - val acc: 99.000\n",
            "model_9: train loss: 0.025 - train acc: 99.963 - val loss: 0.046 - val acc: 99.000\n",
            "model_10: train loss: 0.023 - train acc: 100.000 - val loss: 0.039 - val acc: 99.000\n",
            "\n",
            "Epoch: 91/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.027 - model1_acc: 99.963: : 2694it [00:01, 2219.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.027 - train acc: 99.963 - val loss: 0.043 - val acc: 99.000\n",
            "model_2: train loss: 0.025 - train acc: 100.000 - val loss: 0.039 - val acc: 99.000\n",
            "model_3: train loss: 0.027 - train acc: 99.963 - val loss: 0.051 - val acc: 99.333\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_5: train loss: 0.034 - train acc: 99.889 - val loss: 0.047 - val acc: 99.000\n",
            "model_6: train loss: 0.034 - train acc: 99.814 - val loss: 0.056 - val acc: 98.667\n",
            "model_7: train loss: 0.029 - train acc: 99.889 - val loss: 0.039 - val acc: 99.000\n",
            "model_8: train loss: 0.028 - train acc: 99.852 - val loss: 0.039 - val acc: 99.000\n",
            "model_9: train loss: 0.027 - train acc: 99.926 - val loss: 0.042 - val acc: 99.000\n",
            "model_10: train loss: 0.023 - train acc: 100.000 - val loss: 0.041 - val acc: 99.333\n",
            "\n",
            "Epoch: 92/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.026 - model1_acc: 99.963: : 2694it [00:01, 2232.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 99.963 - val loss: 0.042 - val acc: 99.333\n",
            "model_2: train loss: 0.024 - train acc: 100.000 - val loss: 0.042 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 100.000 - val loss: 0.048 - val acc: 99.333\n",
            "model_4: train loss: 0.021 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_5: train loss: 0.032 - train acc: 99.777 - val loss: 0.047 - val acc: 99.000\n",
            "model_6: train loss: 0.034 - train acc: 99.703 - val loss: 0.062 - val acc: 99.000\n",
            "model_7: train loss: 0.024 - train acc: 99.963 - val loss: 0.043 - val acc: 99.000\n",
            "model_8: train loss: 0.030 - train acc: 99.852 - val loss: 0.060 - val acc: 99.000\n",
            "model_9: train loss: 0.024 - train acc: 99.963 - val loss: 0.045 - val acc: 98.667\n",
            "model_10: train loss: 0.022 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "\n",
            "Epoch: 93/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.029 - model1_acc: 99.889: : 2694it [00:01, 2223.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.029 - train acc: 99.889 - val loss: 0.066 - val acc: 99.000\n",
            "model_2: train loss: 0.024 - train acc: 100.000 - val loss: 0.044 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 99.926 - val loss: 0.050 - val acc: 99.000\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_5: train loss: 0.030 - train acc: 99.889 - val loss: 0.074 - val acc: 98.667\n",
            "model_6: train loss: 0.038 - train acc: 99.777 - val loss: 0.060 - val acc: 99.000\n",
            "model_7: train loss: 0.025 - train acc: 99.963 - val loss: 0.042 - val acc: 99.000\n",
            "model_8: train loss: 0.030 - train acc: 99.814 - val loss: 0.048 - val acc: 99.333\n",
            "model_9: train loss: 0.026 - train acc: 100.000 - val loss: 0.054 - val acc: 99.000\n",
            "model_10: train loss: 0.023 - train acc: 99.963 - val loss: 0.047 - val acc: 99.000\n",
            "\n",
            "Epoch: 94/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.049 - model1_acc: 99.592: : 2694it [00:01, 2235.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.049 - train acc: 99.592 - val loss: 0.045 - val acc: 99.000\n",
            "model_2: train loss: 0.026 - train acc: 99.889 - val loss: 0.039 - val acc: 99.000\n",
            "model_3: train loss: 0.029 - train acc: 100.000 - val loss: 0.046 - val acc: 99.000\n",
            "model_4: train loss: 0.024 - train acc: 100.000 - val loss: 0.043 - val acc: 99.333\n",
            "model_5: train loss: 0.038 - train acc: 99.814 - val loss: 0.050 - val acc: 99.000\n",
            "model_6: train loss: 0.031 - train acc: 99.963 - val loss: 0.050 - val acc: 99.000\n",
            "model_7: train loss: 0.027 - train acc: 100.000 - val loss: 0.052 - val acc: 98.667\n",
            "model_8: train loss: 0.025 - train acc: 100.000 - val loss: 0.041 - val acc: 99.333\n",
            "model_9: train loss: 0.031 - train acc: 99.889 - val loss: 0.071 - val acc: 98.667\n",
            "model_10: train loss: 0.026 - train acc: 100.000 - val loss: 0.044 - val acc: 99.333\n",
            "\n",
            "Epoch: 95/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.035 - model1_acc: 99.889: : 2694it [00:01, 2204.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.035 - train acc: 99.889 - val loss: 0.044 - val acc: 99.333\n",
            "model_2: train loss: 0.026 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 100.000 - val loss: 0.054 - val acc: 98.667\n",
            "model_4: train loss: 0.024 - train acc: 100.000 - val loss: 0.039 - val acc: 99.000\n",
            "model_5: train loss: 0.029 - train acc: 99.889 - val loss: 0.044 - val acc: 99.000\n",
            "model_6: train loss: 0.025 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_7: train loss: 0.028 - train acc: 99.814 - val loss: 0.043 - val acc: 99.000\n",
            "model_8: train loss: 0.024 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_9: train loss: 0.038 - train acc: 99.852 - val loss: 0.081 - val acc: 99.000\n",
            "model_10: train loss: 0.024 - train acc: 100.000 - val loss: 0.050 - val acc: 98.667\n",
            "\n",
            "Epoch: 96/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.025 - model1_acc: 99.926: : 2694it [00:01, 2189.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.025 - train acc: 99.926 - val loss: 0.040 - val acc: 99.333\n",
            "model_2: train loss: 0.021 - train acc: 99.963 - val loss: 0.035 - val acc: 99.000\n",
            "model_3: train loss: 0.024 - train acc: 99.963 - val loss: 0.045 - val acc: 99.000\n",
            "model_4: train loss: 0.020 - train acc: 100.000 - val loss: 0.036 - val acc: 99.000\n",
            "model_5: train loss: 0.023 - train acc: 99.963 - val loss: 0.042 - val acc: 99.000\n",
            "model_6: train loss: 0.020 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "model_7: train loss: 0.022 - train acc: 99.963 - val loss: 0.042 - val acc: 99.333\n",
            "model_8: train loss: 0.022 - train acc: 100.000 - val loss: 0.036 - val acc: 99.000\n",
            "model_9: train loss: 0.031 - train acc: 99.852 - val loss: 0.071 - val acc: 98.667\n",
            "model_10: train loss: 0.022 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "\n",
            "Epoch: 97/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.022 - model1_acc: 100.000: : 2694it [00:01, 2167.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.022 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_2: train loss: 0.022 - train acc: 100.000 - val loss: 0.043 - val acc: 99.000\n",
            "model_3: train loss: 0.025 - train acc: 99.926 - val loss: 0.047 - val acc: 99.000\n",
            "model_4: train loss: 0.021 - train acc: 100.000 - val loss: 0.034 - val acc: 99.000\n",
            "model_5: train loss: 0.025 - train acc: 99.926 - val loss: 0.043 - val acc: 98.667\n",
            "model_6: train loss: 0.021 - train acc: 100.000 - val loss: 0.039 - val acc: 99.000\n",
            "model_7: train loss: 0.023 - train acc: 99.963 - val loss: 0.044 - val acc: 99.000\n",
            "model_8: train loss: 0.022 - train acc: 99.963 - val loss: 0.038 - val acc: 99.333\n",
            "model_9: train loss: 0.033 - train acc: 99.777 - val loss: 0.058 - val acc: 99.000\n",
            "model_10: train loss: 0.022 - train acc: 100.000 - val loss: 0.038 - val acc: 99.333\n",
            "\n",
            "Epoch: 98/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.022 - model1_acc: 99.963: : 2694it [00:01, 2008.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.022 - train acc: 99.963 - val loss: 0.037 - val acc: 99.000\n",
            "model_2: train loss: 0.024 - train acc: 100.000 - val loss: 0.037 - val acc: 99.000\n",
            "model_3: train loss: 0.024 - train acc: 99.963 - val loss: 0.050 - val acc: 99.333\n",
            "model_4: train loss: 0.022 - train acc: 100.000 - val loss: 0.042 - val acc: 99.333\n",
            "model_5: train loss: 0.025 - train acc: 99.926 - val loss: 0.050 - val acc: 98.667\n",
            "model_6: train loss: 0.023 - train acc: 99.963 - val loss: 0.036 - val acc: 99.000\n",
            "model_7: train loss: 0.023 - train acc: 99.963 - val loss: 0.039 - val acc: 99.000\n",
            "model_8: train loss: 0.023 - train acc: 99.926 - val loss: 0.041 - val acc: 98.667\n",
            "model_9: train loss: 0.040 - train acc: 99.703 - val loss: 0.047 - val acc: 99.000\n",
            "model_10: train loss: 0.022 - train acc: 100.000 - val loss: 0.040 - val acc: 99.000\n",
            "\n",
            "Epoch: 99/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.3s - model1_loss: 0.026 - model1_acc: 100.000: : 2694it [00:01, 2040.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.026 - train acc: 100.000 - val loss: 0.038 - val acc: 99.000\n",
            "model_2: train loss: 0.025 - train acc: 99.963 - val loss: 0.039 - val acc: 99.000\n",
            "model_3: train loss: 0.028 - train acc: 99.963 - val loss: 0.045 - val acc: 99.000\n",
            "model_4: train loss: 0.024 - train acc: 100.000 - val loss: 0.042 - val acc: 99.000\n",
            "model_5: train loss: 0.026 - train acc: 99.963 - val loss: 0.039 - val acc: 99.000\n",
            "model_6: train loss: 0.026 - train acc: 99.926 - val loss: 0.040 - val acc: 99.000\n",
            "model_7: train loss: 0.026 - train acc: 99.963 - val loss: 0.038 - val acc: 99.000\n",
            "model_8: train loss: 0.024 - train acc: 100.000 - val loss: 0.036 - val acc: 99.333\n",
            "model_9: train loss: 0.036 - train acc: 99.889 - val loss: 0.050 - val acc: 98.667\n",
            "model_10: train loss: 0.025 - train acc: 99.963 - val loss: 0.048 - val acc: 99.000\n",
            "\n",
            "Epoch: 100/100 - LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.2s - model1_loss: 0.025 - model1_acc: 100.000: : 2694it [00:01, 2191.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1: train loss: 0.025 - train acc: 100.000 - val loss: 0.046 - val acc: 99.333\n",
            "model_2: train loss: 0.027 - train acc: 99.926 - val loss: 0.041 - val acc: 99.000\n",
            "model_3: train loss: 0.026 - train acc: 100.000 - val loss: 0.057 - val acc: 99.333\n",
            "model_4: train loss: 0.024 - train acc: 99.963 - val loss: 0.039 - val acc: 98.667\n",
            "model_5: train loss: 0.025 - train acc: 99.926 - val loss: 0.043 - val acc: 99.000\n",
            "model_6: train loss: 0.025 - train acc: 99.963 - val loss: 0.039 - val acc: 99.000\n",
            "model_7: train loss: 0.031 - train acc: 99.852 - val loss: 0.054 - val acc: 99.000\n",
            "model_8: train loss: 0.026 - train acc: 99.963 - val loss: 0.041 - val acc: 99.333\n",
            "model_9: train loss: 0.031 - train acc: 99.889 - val loss: 0.053 - val acc: 98.667\n",
            "model_10: train loss: 0.032 - train acc: 99.814 - val loss: 0.043 - val acc: 99.333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./dml_1_ckpt.pth.tar\"\n",
        "model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "model.load_state_dict(torch.load(model_path)['model_state'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANYxTV239Fsf",
        "outputId": "ba9219dd-9f36-4854-c221-f07e7ac982fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(cuda_device)\n",
        "t_acc_1, t_acc_5, t_loss = test_dml(test_dataloader, model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jpwsAb19FvP",
        "outputId": "b07d063e-1aa1-4f04-b512-e7063141ce74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 99.223 Acc@5 99.995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(t_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiyog2wu9FyP",
        "outputId": "75fac13e-7bad-434f-f551-785ad598bf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 99.223\n",
            "Teacher top-5 test accuracy: 99.995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of teacher parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbaEtESA9F1I",
        "outputId": "0d5bb5d5-9678-4618-d9e2-f9b6abd87d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of teacher parameters: 49104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZITpQdTG0gxJ",
        "outputId": "9e7e65af-e15b-4a78-c214-e55e484812a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 15000\n",
            "conv1.bias 15\n",
            "conv2.weight 11250\n",
            "conv2.bias 30\n",
            "fc1.weight 22500\n",
            "fc1.bias 30\n",
            "fc2.weight 270\n",
            "fc2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Online distillation: ONE**"
      ],
      "metadata": {
        "id": "iShv5NDa-2DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper with code: https://github.com/DefangChen/OKDDip-AAAI2020\n",
        "num_branches = 3\n",
        "alpha = 0.5\n",
        "T = 4\n",
        "ind = False\n",
        "avg = True\n",
        "bpscale=True\n",
        "pdist = nn.PairwiseDistance(p=2)\n",
        "num_epochs = 200\n",
        "start_consistency = 0.5\n",
        "\n",
        "length = 80\n",
        "num_classes = len(np.unique(y_train))\n",
        "learning_rate = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "soyQxd_hCr3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KL_Loss(nn.Module):\n",
        "    def __init__(self, temperature = 1):\n",
        "        super(KL_Loss, self).__init__()\n",
        "        self.T = temperature\n",
        "    def forward(self, output_batch, teacher_outputs):\n",
        "        # output_batch  -> B X num_classes            \n",
        "        # teacher_outputs -> B X num_classes\n",
        "        \n",
        "        # loss_2 = -torch.sum(torch.sum(torch.mul(F.log_softmax(teacher_outputs,dim=1), F.softmax(teacher_outputs,dim=1)+10**(-7))))/teacher_outputs.size(0)\n",
        "        # print('loss H:',loss_2)\n",
        "        \n",
        "        output_batch = F.log_softmax(output_batch/self.T, dim = 1)    \n",
        "        teacher_outputs = F.softmax(teacher_outputs/self.T, dim = 1) + 10**(-7)\n",
        "        # print(output_batch.shape, teacher_outputs.shape)\n",
        "    \n",
        "        loss = self.T * self.T * nn.KLDivLoss(reduction='batchmean')(output_batch, teacher_outputs) \n",
        "        \n",
        "        # Same result KL-loss implementation\n",
        "        # loss = T * T * torch.sum(torch.sum(torch.mul(teacher_outputs, torch.log(teacher_outputs) - output_batch)))/teacher_outputs.size(0)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "DKhqpbx0HmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ILR(torch.autograd.Function):\n",
        "   \n",
        "    @staticmethod\n",
        "    def forward(ctx, input, num_branches):\n",
        "        ctx.num_branches = num_branches\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        num_branches = ctx.num_branches\n",
        "        return grad_output/num_branches, None"
      ],
      "metadata": {
        "id": "hIJr1zb_-66N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ONEcnn(nn.Module):\n",
        "    def __init__(self, n_classes=16, num_branches=3, bpscale = False, avg = False, ind = False, cfg=s_cfg, fc=s_fc):\n",
        "        super(ONEcnn, self).__init__()\n",
        "        self.avg = avg\n",
        "        self.bpscale = bpscale\n",
        "        self.num_branches = num_branches\n",
        "        self.conv1 = nn.Conv2d(num_components, cfg[0], kernel_size=(5, 5))\n",
        "        # self.bn1 = nn.BatchNorm2d(self.inplances)\n",
        "        self.conv2 = nn.Conv2d(cfg[0], cfg[1], kernel_size=(5, 5))\n",
        "        #self.bn2 = nn.BatchNorm2d(self.inplances)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d((2, 2))\n",
        "        self.ind = ind\n",
        "        \n",
        "        # self.layer2 = self._make_layers(256, num_layer)\n",
        "        # self.layer3 = self._make_layers(512, num_layer)\n",
        "        for i in range(num_branches):\n",
        "            # setattr(self, 'layer3_'+str(i), self._make_layers(512, num_layer))\n",
        "            setattr(self, 'classifier3_'+str(i), nn.Sequential(\n",
        "            nn.Linear(int(25*cfg[1]), fc),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(fc, n_classes)))\n",
        "            \n",
        "        if self.avg == False:\n",
        "            self.avgpool_c = nn.AdaptiveAvgPool2d((1,1))\n",
        "            self.control_v1 = nn.Linear(cfg[1], self.num_branches)\n",
        "            self.bn_v1 = nn.BatchNorm1d(self.num_branches)\n",
        "        if self.bpscale:\n",
        "            self.layer_ILR = ILR.apply\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        # x = self.layer1(x)\n",
        "        # x = self.layer2(x)\n",
        "        # x = self.layer3(x)\n",
        "        if self.bpscale:\n",
        "            x = self.layer_ILR(x, self.num_branches) # Backprop rescaling\n",
        "            \n",
        "        #x_3 = getattr(self,'layer3_0')(x)   # B x 64 x 8 x 8\n",
        "        x_3 = x.view(x.size(0), -1)     # B x 64\n",
        "        x_3_1 = getattr(self, 'classifier3_0')(x_3)     # B x num_classes\n",
        "        pro = x_3_1.unsqueeze(-1)        \n",
        "        for i in range(1, self.num_branches):\n",
        "            temp = x\n",
        "            temp = temp.view(temp.size(0), -1)   \n",
        "            temp_1 = getattr(self, 'classifier3_' + str(i))(temp)\n",
        "            temp_1 = temp_1.unsqueeze(-1)\n",
        "            pro = torch.cat([pro,temp_1],-1)        # B x num_classes x num_branches\n",
        "        \n",
        "        if self.ind:\n",
        "            return pro, None\n",
        "        # CL\n",
        "        else:\n",
        "            if self.avg:\n",
        "                x_m = 0\n",
        "                for i in range(1, self.num_branches):\n",
        "                    x_m += 1/(self.num_branches-1) * pro[:,:,i]\n",
        "                x_m = x_m.unsqueeze(-1)\n",
        "                for i in range(1, self.num_branches):\n",
        "                    temp = 0\n",
        "                    for j in range(0, self.num_branches):\n",
        "                        if j != i:\n",
        "                            temp += 1/(self.num_branches-1) * pro[:,:,j]       # B x num_classes\n",
        "                    temp = temp.unsqueeze(-1)\n",
        "                    x_m = torch.cat([x_m, temp],-1)                            # B x num_classes x num_branches\n",
        "            # ONE\n",
        "            else:\n",
        "                x_c=self.avgpool_c(x)\n",
        "                x_c = x_c.view(x_c.size(0), -1) # B x 32 \n",
        "                x_c=self.control_v1(x_c)    # B x 3\n",
        "                x_c=self.bn_v1(x_c)  \n",
        "                x_c=F.relu(x_c)      \n",
        "                x_c = F.softmax(x_c, dim=1) # B x 3  \n",
        "            \n",
        "                #x_3 = getattr(self,'layer3_0')(x)   # B x 64 x 8 x 8\n",
        "                x_3 = x.view(x.size(0), -1)     # B x 64\n",
        "                x_3_1 = getattr(self, 'classifier3_0')(x_3)     # B x num_classes\n",
        "                x_m = x_c[:,0].view(-1, 1).repeat(1, x_3_1.size(1)) * x_3_1\n",
        "                pro = x_3_1.unsqueeze(-1) \n",
        "                for i in range(1, self.num_branches):\n",
        "                    temp = x\n",
        "                    temp = temp.view(temp.size(0), -1)   \n",
        "                    temp_1 = getattr(self, 'classifier3_' + str(i))(temp)\n",
        "                    x_m += x_c[:,i].view(-1, 1).repeat(1, temp_1.size(1)) * temp_1       # B x num_classes\n",
        "                    temp_1 = temp_1.unsqueeze(-1)\n",
        "                    pro = torch.cat([pro,temp_1],-1)        # B x num_classes x num_branches\n",
        "              \n",
        "            return pro, x_m"
      ],
      "metadata": {
        "id": "fCfJ5cB5-681"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ONE(train_loader, model, optimizer, criterion, criterion_T, accuracy, consistency_weight):\n",
        "    \n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # set running average object for loss and accuracy\n",
        "    accTop1_avg = list(range(num_branches+1))\n",
        "    accTop5_avg = list(range(num_branches+1))\n",
        "    for i in range(num_branches + 1):\n",
        "        accTop1_avg[i] = RunningAverage()\n",
        "        accTop5_avg[i] = RunningAverage()\n",
        "    loss_true_avg = RunningAverage()\n",
        "    loss_group_avg = RunningAverage()\n",
        "    loss_avg = RunningAverage()    \n",
        "    end = time.time()\n",
        "    \n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(train_loader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(train_loader):\n",
        "            train_batch = train_batch.cuda(non_blocking=True)\n",
        "            labels_batch = labels_batch.cuda(non_blocking=True)\n",
        "            \n",
        "            # compute model output and loss\n",
        "            output_batch, x_m = model(train_batch) \n",
        "            loss_true = 0\n",
        "            loss_group = 0    \n",
        "            if ind:\n",
        "                for i in range(num_branches):\n",
        "                    loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                loss_group += torch.zeros(1).cuda()\n",
        "            else:\n",
        "                if avg:\n",
        "                    for i in range(num_branches):\n",
        "                        loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                        loss_group += criterion_T(output_batch[:,:,i], x_m[:,:,i])\n",
        "                else:\n",
        "                    for i in range(num_branches):\n",
        "                        loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                        loss_group += criterion_T(output_batch[:,:,i], x_m)\n",
        "                    loss_true += criterion(x_m, labels_batch)\n",
        "            \n",
        "            loss = loss_true + alpha * consistency_weight * loss_group\n",
        "        \n",
        "            loss_true_avg.update(loss_true.item())\n",
        "            loss_group_avg.update(loss_group.item())\n",
        "            loss_avg.update(loss.item())\n",
        "            \n",
        "            # Update average loss and accuracy\n",
        "            for i in range(num_branches):\n",
        "                metrics = accuracy(output_batch[:,:,i], labels_batch, topk=(1,5))\n",
        "                accTop1_avg[i].update(metrics[0].item())\n",
        "                accTop5_avg[i].update(metrics[1].item())\n",
        "                # when num_branches = 4 \n",
        "                # 0,1,2 peer branches\n",
        "            \n",
        "            e_metrics = accuracy(torch.mean(output_batch, dim=2), labels_batch, topk=(1,5)) # need to test after softmax\n",
        "            accTop1_avg[num_branches].update(e_metrics[0].item())\n",
        "            accTop5_avg[num_branches].update(e_metrics[1].item())            \n",
        "            # 4 ensemble of 0,1,2\n",
        "            \n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # performs updates using calculated gradients\n",
        "            optimizer.step()\n",
        "            \n",
        "            t.update()\n",
        "            \n",
        "    mean_train_accTop1 = 0\n",
        "    mean_train_accTop5 = 0\n",
        "    for i in range(num_branches):\n",
        "        mean_train_accTop1 += accTop1_avg[i].value()\n",
        "        mean_train_accTop5 += accTop5_avg[i].value()\n",
        "    mean_train_accTop1 /= (num_branches)\n",
        "    mean_train_accTop5 /= (num_branches)\n",
        "    \n",
        "    # compute mean of all metrics in summary     \n",
        "    \n",
        "    train_metrics = {'train_loss': loss_avg.value(),\n",
        "                     'train_true_loss': loss_true_avg.value(),\n",
        "                     'train_group_loss': loss_group_avg.value(),\n",
        "                     'mean_train_accTop1': mean_train_accTop1,\n",
        "                     'mean_train_accTop5': mean_train_accTop1,\n",
        "                     'train_accTop1': accTop1_avg[num_branches].value(),\n",
        "                     'train_accTop5': accTop5_avg[num_branches].value(),\n",
        "                     'time': time.time() - end}\n",
        "   \n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in train_metrics.items())\n",
        "    # logging.info(\"- Train metrics: \" + metrics_string)\n",
        "    return train_metrics"
      ],
      "metadata": {
        "id": "s0tT3Uqs-7DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ONE(test_loader, model, criterion, criterion_T, accuracy, consistency_weight):\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # set running average object for loss   \n",
        "    \n",
        "    accTop1_avg = list(range(num_branches + 1))\n",
        "    accTop5_avg = list(range(num_branches + 1))\n",
        "    for i in range(num_branches + 1):\n",
        "        accTop1_avg[i] = RunningAverage()\n",
        "        accTop5_avg[i] = RunningAverage()\n",
        "    \n",
        "    loss_true_avg = RunningAverage()\n",
        "    loss_group_avg = RunningAverage()\n",
        "    loss_avg = RunningAverage()\n",
        "    dist_avg = RunningAverage()\n",
        "    end = time.time()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _, (test_batch, labels_batch) in enumerate(test_loader):\n",
        "            test_batch = test_batch.cuda(non_blocking=True)\n",
        "            labels_batch = labels_batch.cuda(non_blocking=True)\n",
        "            \n",
        "            # compute model output and loss\n",
        "            output_batch, x_m = model(test_batch)\n",
        "            loss_true = 0 \n",
        "            loss_group = 0\n",
        "            if ind:\n",
        "                for i in range(num_branches):\n",
        "                    loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                loss_group += torch.zeros(1).cuda()\n",
        "            else:\n",
        "                if avg:\n",
        "                    for i in range(num_branches):\n",
        "                        loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                        loss_group += criterion_T(output_batch[:,:,i],x_m[:,:,i])\n",
        "                else:            \n",
        "                    for i in range(num_branches):\n",
        "                        loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                        loss_group += criterion_T(output_batch[:,:,i],x_m)\n",
        "                    loss_true += criterion(x_m, labels_batch)\n",
        "                \n",
        "            loss = loss_true + alpha * consistency_weight * loss_group\n",
        "            \n",
        "            loss_true_avg.update(loss_true.item())\n",
        "            loss_group_avg.update(loss_group.item())\n",
        "            loss_avg.update(loss.item())\n",
        "            \n",
        "            # Update average loss and accuracy\n",
        "            for i in range(num_branches):\n",
        "                metrics = accuracy(output_batch[:,:,i], labels_batch, topk=(1,5))\n",
        "                accTop1_avg[i].update(metrics[0].item())\n",
        "                accTop5_avg[i].update(metrics[1].item())\n",
        "            \n",
        "            e_metrics = accuracy(torch.mean(output_batch, dim=2), labels_batch, topk=(1,5))\n",
        "            accTop1_avg[num_branches].update(e_metrics[0].item())\n",
        "            accTop5_avg[num_branches].update(e_metrics[1].item()) \n",
        "            \n",
        "            len_kk = output_batch.size(0)\n",
        "            output_batch = F.softmax(output_batch, dim=1)    \n",
        "            for kk in range(len_kk):\n",
        "                ret = output_batch[kk,:,:]\n",
        "#                 ret = ret.squeeze(0)           \n",
        "                ret = ret.t()                  # branches x classes\n",
        "                sim = 0\n",
        "                for j in range(num_branches-1):\n",
        "                    for k in range(j+1, num_branches-1):\n",
        "                        sim += pdist(ret[j:j+1,:],ret[k:k+1,:])    \n",
        "                #sim = 2 * sim / (num_branches*(num_branches-1))\n",
        "                sim = sim / 3\n",
        "                dist_avg.update(sim.item())\n",
        "\n",
        "    mean_test_accTop1 = 0\n",
        "    mean_test_accTop5 = 0\n",
        "    for i in range(num_branches):\n",
        "        mean_test_accTop1 += accTop1_avg[i].value()\n",
        "        mean_test_accTop5 += accTop5_avg[i].value()\n",
        "    mean_test_accTop1 /= (num_branches)\n",
        "    mean_test_accTop5 /= (num_branches)\n",
        "    # compute mean of all metrics in summary\n",
        "        \n",
        "    test_metrics = { 'test_loss': loss_avg.value(),\n",
        "                     'test_true_loss': loss_true_avg.value(),\n",
        "                     'test_group_loss': loss_group_avg.value(),\n",
        "                     'mean_test_accTop1': mean_test_accTop1,\n",
        "                     'mean_test_accTop5': mean_test_accTop5,\n",
        "                     'test_accTop1': accTop1_avg[num_branches].value(),\n",
        "                     'test_accTop5': accTop5_avg[num_branches].value(),\n",
        "                     'dist': dist_avg.value(),\n",
        "                     'time': time.time() - end}\n",
        "    \n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in test_metrics.items())\n",
        "    # logging.info(\"- Test metrics: \" + metrics_string)\n",
        "    return test_metrics"
      ],
      "metadata": {
        "id": "Bf7pSCMZ-7JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dict_to_json(d, json_path):\n",
        "    \"\"\"Saves dict of floats in json file\n",
        "    Args:\n",
        "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
        "        json_path: (string) path to json file\n",
        "    \"\"\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
        "        d = {k: v for k, v in d.items()}\n",
        "        json.dump(d, f, indent=4)"
      ],
      "metadata": {
        "id": "9yE2ZSuDFk5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, criterion_T, accuracy, model_dir):\n",
        "    \n",
        "    start_epoch = 0\n",
        "    best_acc = 0.\n",
        "        \n",
        "    # # learning rate schedulers for different models:\n",
        "    # scheduler = MultiStepLR(optimizer, milestones=args.schedule, gamma=0.1)\n",
        "    \n",
        "    # # TensorboardX setup\n",
        "    # writer = SummaryWriter(log_dir = model_dir) # ensemble\n",
        "    \n",
        "    # Save best ensemble or average accTop1\n",
        "    choose_E = False\n",
        "    \n",
        "    # Save the parameters for export\n",
        "    result_train_metrics = list(range(num_epochs))\n",
        "    result_test_metrics = list(range(num_epochs))\n",
        "        \n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        \n",
        "        #scheduler.step()\n",
        "     \n",
        "        # Run one epoch\n",
        "        #logging.info(\"Epoch {}/{}\".format(epoch + 1, args.num_epochs))\n",
        "        \n",
        "        # Set consistency_weight or originial temperature scale \n",
        "        consistency_epoch = start_consistency * num_epochs \n",
        "        if epoch < consistency_epoch:\n",
        "            consistency_weight = 1\n",
        "        else:\n",
        "            consistency_weight = get_current_consistency_weight(epoch - consistency_epoch, length)\n",
        "        \n",
        "        # compute number of batches in one epoch (one full pass over the training set)\n",
        "        train_metrics = train_ONE(train_loader, model, optimizer, criterion, criterion_T, accuracy, consistency_weight)\n",
        "\t\t\n",
        "        # writer.add_scalar('Train/Loss', train_metrics['train_loss'], epoch+1)\n",
        "        # writer.add_scalar('Train/AccTop1', train_metrics['train_accTop1'], epoch+1)\n",
        "        \n",
        "        # Evaluate for one epoch on validation set\n",
        "        test_metrics = evaluate_ONE(test_loader, model, criterion, criterion_T, accuracy, consistency_weight) \n",
        "        \n",
        "        # Find the best accTop1 for Branch1.\n",
        "        if choose_E:\n",
        "            test_acc = test_metrics['test_accTop1']\n",
        "        else:\n",
        "            test_acc = test_metrics['mean_test_accTop1']\n",
        "            \n",
        "        #writer.add_scalar('Test/Loss', test_metrics['test_loss'], epoch+1)\n",
        "        #writer.add_scalar('Test/AccTop1', test_metrics['test_accTop1'], epoch+1)\n",
        "        \n",
        "        result_train_metrics[epoch] = train_metrics\n",
        "        result_test_metrics[epoch] = test_metrics\n",
        "        \n",
        "        # Save latest train/test metrics\n",
        "        torch.save(result_train_metrics, os.path.join(model_dir, 'train_metrics'))\n",
        "        torch.save(result_test_metrics, os.path.join(model_dir, 'test_metrics'))\n",
        "\n",
        "        last_path = os.path.join(model_dir, 'last.pth')        \n",
        "        # Save latest model weights, optimizer and accuracy\n",
        "        torch.save({    'state_dict': model.state_dict(),\n",
        "                        'epoch': epoch + 1,\n",
        "                        'optim_dict': optimizer.state_dict(),\n",
        "                        'test_accTop1': test_metrics['test_accTop1'],\n",
        "                        'mean_test_accTop1': test_metrics['mean_test_accTop1']}, last_path)\n",
        "        # If best_eval, best_save_path\n",
        "        is_best = test_acc >= best_acc\n",
        "        if is_best:\n",
        "            #logging.info(\"- Found better accuracy\")            \n",
        "            best_acc = test_acc            \n",
        "            # Save best metrics in a json file in the model directory\n",
        "            test_metrics['epoch'] = epoch + 1\n",
        "            save_dict_to_json(test_metrics, os.path.join(model_dir, \"test_best_metrics.json\"))\n",
        "        \n",
        "            # Save model and optimizer\n",
        "            shutil.copyfile(last_path, os.path.join(model_dir, 'best.pth'))\n",
        "    # writer.close()   "
      ],
      "metadata": {
        "id": "3z9vAaVB-7Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_consistency_weight(current, rampup_length = length):\n",
        "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current, 0.0, rampup_length)\n",
        "        phase = 1.0 - current / rampup_length\n",
        "        return float(np.exp(-5.0 * phase * phase))"
      ],
      "metadata": {
        "id": "yjSgtAiL-7Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ONEcnn(n_classes=num_classes, num_branches=num_branches, bpscale = bpscale, avg = avg, ind = ind, cfg=s_cfg, fc=s_fc).to(cuda_device)"
      ],
      "metadata": {
        "id": "20qDT7ogIl1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = (sum(p.numel() for p in model.parameters()))\n",
        "print(num_params)\n",
        "\n",
        "# Loss and optimizer(SGD with 0.9 momentum)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion_T = KL_Loss(T).to(cuda_device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGSUrQtBGujj",
        "outputId": "4b9ebb99-b244-44a4-fd36-2a79c2493137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, criterion, criterion_T, accuracy, model_dir='./student/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O27EM7LFF5_7",
        "outputId": "a301a109-a392-439e-b8b8-87f2e5466e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 73.99it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.80it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.63it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.24it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 83.57it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.59it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.97it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.67it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.50it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.17it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.22it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.78it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.76it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.08it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.87it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.96it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.34it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.74it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.81it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.59it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.24it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.96it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.46it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.06it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 93.10it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 93.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.50it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.14it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.60it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.50it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.02it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.09it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.06it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.10it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.45it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.23it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.56it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.80it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.28it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.17it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.14it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.25it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.25it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.08it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.71it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.40it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.57it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.09it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 84.16it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.81it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.18it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.67it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.09it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.12it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.00it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.89it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.86it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 84.70it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 84.76it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.56it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.97it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.47it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.67it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.09it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.72it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.98it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.25it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.62it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.78it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.24it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.82it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.32it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.19it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.29it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.53it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.83it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.24it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.90it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.64it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.33it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.16it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.87it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.65it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.85it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.30it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.19it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.42it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.33it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.22it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.15it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.07it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.06it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.58it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.28it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.93it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.22it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.71it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.20it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.02it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.18it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.26it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.98it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.46it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.97it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.72it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.82it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.67it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.28it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.71it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.54it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.08it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.90it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 83.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 83.17it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.00it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.51it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.79it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.71it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.82it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 95.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.71it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.61it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.89it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.62it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.49it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.81it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.31it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.77it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.32it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.88it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 93.22it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.68it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 92.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.12it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.34it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.54it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.94it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.42it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.68it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 82.74it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 81.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 83.32it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.61it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.48it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.80it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.42it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.23it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 85.57it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.46it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 84.51it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.07it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.02it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.93it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 86.99it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 89.70it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.75it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.64it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 87.55it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 93.53it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 88.72it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.48it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.13it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.56it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 90.51it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 91.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consistency_weight = get_current_consistency_weight(50, length)\n",
        "test_metrics = evaluate_ONE(test_dataloader, model, criterion, criterion_T, accuracy, consistency_weight)"
      ],
      "metadata": {
        "id": "_KZT1JK2F6HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWNwNsEqF6J0",
        "outputId": "7b80ddf8-477d-4b84-bded-aefe23007472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.07544255258524837,\n",
              " 'test_true_loss': 0.07472101972443075,\n",
              " 'test_group_loss': 0.0029150726859827514,\n",
              " 'mean_test_accTop1': 99.5678391959799,\n",
              " 'mean_test_accTop5': 99.98157453936348,\n",
              " 'test_accTop1': 99.57035175879398,\n",
              " 'test_accTop5': 99.97738693467336,\n",
              " 'dist': 0.0002689911008378352,\n",
              " 'time': 6.880849123001099}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msuWFrMYF6MX",
        "outputId": "0ab1da17-8d0a-4161-846a-4d8e915897c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 15000\n",
            "conv1.bias 15\n",
            "conv2.weight 11250\n",
            "conv2.bias 30\n",
            "classifier3_0.0.weight 22500\n",
            "classifier3_0.0.bias 30\n",
            "classifier3_0.2.weight 270\n",
            "classifier3_0.2.bias 9\n",
            "classifier3_1.0.weight 22500\n",
            "classifier3_1.0.bias 30\n",
            "classifier3_1.2.weight 270\n",
            "classifier3_1.2.bias 9\n",
            "classifier3_2.0.weight 22500\n",
            "classifier3_2.0.bias 30\n",
            "classifier3_2.2.weight 270\n",
            "classifier3_2.2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Online Distillation: OKDDip**"
      ],
      "metadata": {
        "id": "08Zg2R-vyLpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper with code: https://github.com/DefangChen/OKDDip-AAAI2020\n",
        "num_branches = 4\n",
        "alpha = 1\n",
        "T = 4\n",
        "\n",
        "pdist = nn.PairwiseDistance(p=2)\n",
        "num_epochs = 200\n",
        "start_consistency = 0.5\n",
        "\n",
        "length = 80\n",
        "num_classes = len(np.unique(y_train))\n",
        "learning_rate = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "Hm4VwVWoySrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KL_Loss(nn.Module):\n",
        "    def __init__(self, temperature = 1):\n",
        "        super(KL_Loss, self).__init__()\n",
        "        self.T = temperature\n",
        "    def forward(self, output_batch, teacher_outputs):\n",
        "        # output_batch  -> B X num_classes            \n",
        "        # teacher_outputs -> B X num_classes\n",
        "        \n",
        "        # loss_2 = -torch.sum(torch.sum(torch.mul(F.log_softmax(teacher_outputs,dim=1), F.softmax(teacher_outputs,dim=1)+10**(-7))))/teacher_outputs.size(0)\n",
        "        # print('loss H:',loss_2)\n",
        "        \n",
        "        output_batch = F.log_softmax(output_batch/self.T, dim = 1)    \n",
        "        teacher_outputs = F.softmax(teacher_outputs/self.T, dim = 1) + 10**(-7)\n",
        "        # print(output_batch.shape, teacher_outputs.shape)\n",
        "    \n",
        "        loss = self.T * self.T * nn.KLDivLoss(reduction='batchmean')(output_batch, teacher_outputs) \n",
        "        \n",
        "        # Same result KL-loss implementation\n",
        "        # loss = T * T * torch.sum(torch.sum(torch.mul(teacher_outputs, torch.log(teacher_outputs) - output_batch)))/teacher_outputs.size(0)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "6RUd0w443WJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OKDDcnn(nn.Module):\n",
        "    def __init__(self, n_classes=16, num_branches=3, factor=8, en=False, cfg=s_cfg, fc=s_fc):\n",
        "        super(OKDDcnn, self).__init__()\n",
        "        self.en = en\n",
        "        self.num_branches = num_branches\n",
        "        self.conv1 = nn.Conv2d(num_components, cfg[0], kernel_size=(5, 5))\n",
        "        # self.bn1 = nn.BatchNorm2d(self.inplances)\n",
        "        self.conv2 = nn.Conv2d(cfg[0], cfg[1], kernel_size=(5, 5))\n",
        "        #self.bn2 = nn.BatchNorm2d(self.inplances)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d((2, 2))\n",
        "        \n",
        "        for i in range(num_branches):\n",
        "            # setattr(self, 'layer3_'+str(i), self._make_layers(512, num_layer))\n",
        "            setattr(self, 'classifier3_'+str(i), nn.Sequential(\n",
        "            nn.Linear(int(25*cfg[1]), fc),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(fc, n_classes)))\n",
        "            \n",
        "        input_channel = int(25*cfg[1])\n",
        "        self.query_weight = nn.Linear(input_channel, input_channel//factor, bias = False)\n",
        "        self.key_weight = nn.Linear(input_channel, input_channel//factor, bias = False)   \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "             \n",
        "        #x_3 = getattr(self,'layer3_0')(x)   # B x 64 x 8 x 8\n",
        "        x_3 = x.view(x.size(0), -1)     # B x 64\n",
        "        proj_q = self.query_weight(x_3)     # B x 64\n",
        "        proj_q = proj_q[:, None, :]\n",
        "        proj_k = self.key_weight(x_3)       # B x 64 \n",
        "        proj_k = proj_k[:, None, :]\n",
        "        x_3_1 = getattr(self, 'classifier3_0')(x_3)     # B x num_classes\n",
        "        pro = x_3_1.unsqueeze(-1)        \n",
        "        if self.en:\n",
        "            for i in range(1, self.num_branches):\n",
        "                temp = x\n",
        "                temp = temp.view(temp.size(0), -1)  \n",
        "                temp_q = self.query_weight(temp)\n",
        "                temp_k = self.key_weight(temp)\n",
        "                temp_q = temp_q[:, None, :]\n",
        "                temp_k = temp_k[:, None, :]\n",
        "                temp_1 = getattr(self, 'classifier3_' + str(i))(temp)\n",
        "                temp_1 = temp_1.unsqueeze(-1)\n",
        "                pro = torch.cat([pro,temp_1],-1)        # B x num_classes x num_branches\n",
        "                proj_q = torch.cat([proj_q, temp_q], 1) # B x num_branches x 8\n",
        "                proj_k = torch.cat([proj_k, temp_k], 1) \n",
        "            \n",
        "            energy = torch.bmm(proj_q, proj_k.permute(0,2,1)) \n",
        "            attention = F.softmax(energy, dim = -1) \n",
        "            x_m = torch.bmm(pro, attention.permute(0,2,1))\n",
        "            return pro, x_m\n",
        "        else:\n",
        "            for i in range(1, self.num_branches - 1):\n",
        "                temp = x\n",
        "                temp = temp.view(temp.size(0), -1)   \n",
        "                temp_q = self.query_weight(temp)\n",
        "                temp_k = self.key_weight(temp)\n",
        "                temp_q = temp_q[:, None, :]\n",
        "                temp_k = temp_k[:, None, :]\n",
        "                temp_1 = getattr(self, 'classifier3_' + str(i))(temp)\n",
        "                temp_1 = temp_1.unsqueeze(-1)\n",
        "                pro = torch.cat([pro,temp_1],-1)        # B x num_classes x num_branches\n",
        "                proj_q = torch.cat([proj_q, temp_q], 1) # B x num_branches x 8\n",
        "                proj_k = torch.cat([proj_k, temp_k], 1) \n",
        "            \n",
        "            energy =  torch.bmm(proj_q, proj_k.permute(0,2,1)) \n",
        "            attention = F.softmax(energy, dim = -1) \n",
        "            x_m = torch.bmm(pro, attention.permute(0,2,1))\n",
        "            \n",
        "            temp = x\n",
        "            temp = temp.view(temp.size(0), -1)   \n",
        "            temp_out = getattr(self, 'classifier3_' + str(self.num_branches - 1))(temp)\n",
        "            return pro, x_m, temp_out"
      ],
      "metadata": {
        "id": "rjaeMfKV2A5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_okddip(train_loader, model, optimizer, criterion, criterion_T, accuracy, consistency_weight):\n",
        "    \n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # set running average object for loss and accuracy\n",
        "    accTop1_avg = list(range(num_branches + 1))\n",
        "    accTop5_avg = list(range(num_branches + 1))\n",
        "    for i in range(num_branches + 1):\n",
        "        accTop1_avg[i] = RunningAverage()\n",
        "        accTop5_avg[i] = RunningAverage()\n",
        "    loss_true_avg = RunningAverage()\n",
        "    loss_group_avg = RunningAverage()\n",
        "    loss_avg = RunningAverage()    \n",
        "    end = time.time()\n",
        "    \n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(train_loader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(train_loader):\n",
        "            train_batch = train_batch.cuda(non_blocking=True)\n",
        "            labels_batch = labels_batch.cuda(non_blocking=True)\n",
        "            \n",
        "            # compute model output and loss\n",
        "            output_batch, x_m, x_stu = model(train_batch) \n",
        "            loss_true = 0\n",
        "            loss_group = 0    \n",
        "            for i in range(num_branches - 1):\n",
        "                loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                loss_group += criterion_T(output_batch[:,:,i], x_m[:,:,i])\n",
        "            # loss_true = loss_true / args.num_branches        \n",
        "            # loss_group = loss_group / args.num_branches\n",
        "            loss = loss_true + criterion(x_stu, labels_batch) + alpha * consistency_weight * (loss_group + criterion_T(x_stu, torch.mean(output_batch, dim = 2)))\n",
        "        \n",
        "            loss_true_avg.update(loss_true.item())\n",
        "            loss_group_avg.update(loss_group.item())\n",
        "            loss_avg.update(loss.item())\n",
        "            \n",
        "            # Update average loss and accuracy\n",
        "            for i in range(num_branches - 1):\n",
        "                metrics = accuracy(output_batch[:,:,i], labels_batch, topk=(1,5))\n",
        "                accTop1_avg[i].update(metrics[0].item())\n",
        "                accTop5_avg[i].update(metrics[1].item())\n",
        "                # when num_branches = 4 \n",
        "                # 0,1,2 peer branches\n",
        "                \n",
        "            metrics = accuracy(x_stu, labels_batch, topk=(1,5))\n",
        "            accTop1_avg[num_branches - 1].update(metrics[0].item())\n",
        "            accTop5_avg[num_branches - 1].update(metrics[1].item())\n",
        "            # 3 leader branches\n",
        "        \n",
        "            e_metrics = accuracy(torch.mean(output_batch, dim=2), labels_batch, topk=(1,5)) # need to test after softmax\n",
        "            accTop1_avg[num_branches].update(e_metrics[0].item())\n",
        "            accTop5_avg[num_branches].update(e_metrics[1].item())            \n",
        "            # 4 ensemble of 0,1,2\n",
        "            \n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # performs updates using calculated gradients\n",
        "            optimizer.step()\n",
        "            \n",
        "            t.update()\n",
        "            \n",
        "    mean_train_accTop1 = 0\n",
        "    mean_train_accTop5 = 0\n",
        "    for i in range(num_branches - 1):\n",
        "        mean_train_accTop1 += accTop1_avg[i].value()\n",
        "        mean_train_accTop5 += accTop5_avg[i].value()\n",
        "    mean_train_accTop1 /= (num_branches-1)\n",
        "    mean_train_accTop5 /= (num_branches-1)\n",
        "    \n",
        "    # compute mean of all metrics in summary     \n",
        "    \n",
        "    train_metrics = {'train_loss': loss_avg.value(),\n",
        "                     'train_true_loss': loss_true_avg.value(),\n",
        "                     'train_group_loss': loss_group_avg.value(),\n",
        "                     'mean_train_accTop1': mean_train_accTop1,\n",
        "                     'mean_train_accTop5': mean_train_accTop1,\n",
        "                     'stu_train_accTop1': accTop1_avg[num_branches - 1].value(),\n",
        "                     'stu_train_accTop5': accTop5_avg[num_branches - 1].value(),\n",
        "                     'train_accTop1': accTop1_avg[num_branches].value(),\n",
        "                     'train_accTop5': accTop5_avg[num_branches].value(),\n",
        "                     'time': time.time() - end}\n",
        "                     \n",
        "    for i in range(num_branches - 1):\n",
        "        train_metrics.update({'stu'+str(i)+'train_accTop1' : accTop1_avg[i].value()})\n",
        "        train_metrics.update({'stu'+str(i)+'train_accTop5' : accTop5_avg[i].value()})\n",
        "        \n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in train_metrics.items())\n",
        "\n",
        "    return train_metrics"
      ],
      "metadata": {
        "id": "QrTPjXNQySuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_okddip(test_loader, model, criterion, criterion_T, accuracy, consistency_weight):\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # set running average object for loss   \n",
        "    \n",
        "    accTop1_avg = list(range(num_branches + 1))\n",
        "    accTop5_avg = list(range(num_branches + 1))\n",
        "    for i in range(num_branches + 1):\n",
        "        accTop1_avg[i] = RunningAverage()\n",
        "        accTop5_avg[i] = RunningAverage()\n",
        "    \n",
        "    loss_true_avg = RunningAverage()\n",
        "    loss_group_avg = RunningAverage()\n",
        "    loss_avg = RunningAverage()\n",
        "    dist_avg = RunningAverage()\n",
        "    end = time.time()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _, (test_batch, labels_batch) in enumerate(test_loader):\n",
        "            test_batch = test_batch.cuda(non_blocking=True)\n",
        "            labels_batch = labels_batch.cuda(non_blocking=True)\n",
        "            \n",
        "            # compute model output and loss\n",
        "            loss_true = 0\n",
        "            loss_group = 0\n",
        "    \n",
        "            output_batch, x_m, x_stu = model(test_batch)\n",
        "            for i in range(num_branches - 1):\n",
        "                loss_true += criterion(output_batch[:,:,i], labels_batch)\n",
        "                loss_group += criterion_T(output_batch[:,:,i], x_m[:,:,i])\n",
        "            # loss_true = loss_true / args.num_branches        \n",
        "            # loss_group = loss_group / args.num_branches\n",
        "            loss = loss_true + criterion(x_stu, labels_batch) + alpha * consistency_weight * (loss_group + criterion_T(x_stu, torch.mean(output_batch, dim = 2)))\n",
        "    \n",
        "            loss_true_avg.update(loss_true.item())\n",
        "            loss_group_avg.update(loss_group.item())\n",
        "            loss_avg.update(loss.item())\n",
        "            \n",
        "            # Update average loss and accuracy\n",
        "            for i in range(num_branches - 1):\n",
        "                metrics = accuracy(output_batch[:,:,i], labels_batch, topk=(1,5))\n",
        "                accTop1_avg[i].update(metrics[0].item())\n",
        "                accTop5_avg[i].update(metrics[1].item())\n",
        "                                \n",
        "            metrics = accuracy(x_stu, labels_batch, topk=(1,5))\n",
        "            accTop1_avg[num_branches - 1].update(metrics[0].item())\n",
        "            accTop5_avg[num_branches - 1].update(metrics[1].item())\n",
        "                \n",
        "            e_metrics = accuracy(torch.mean(output_batch, dim=2), labels_batch, topk=(1,5))\n",
        "            accTop1_avg[num_branches].update(e_metrics[0].item())\n",
        "            accTop5_avg[num_branches].update(e_metrics[1].item()) \n",
        "            \n",
        "            len_kk = output_batch.size(0)\n",
        "            output_batch = F.softmax(output_batch, dim=1)    \n",
        "            for kk in range(len_kk):\n",
        "                ret = output_batch[kk,:,:]\n",
        "                # ret = ret.squeeze(0)           \n",
        "                ret = ret.t()                  # branches x classes\n",
        "                sim = 0\n",
        "                for j in range(num_branches-1):\n",
        "                    for k in range(j+1, num_branches-1):\n",
        "                        sim += pdist(ret[j:j+1,:],ret[k:k+1,:])    \n",
        "                sim = sim / 3\n",
        "                dist_avg.update(sim.item())\n",
        "\n",
        "    mean_test_accTop1 = 0\n",
        "    mean_test_accTop5 = 0\n",
        "    for i in range(num_branches - 1):\n",
        "        mean_test_accTop1 += accTop1_avg[i].value()\n",
        "        mean_test_accTop5 += accTop5_avg[i].value()\n",
        "    mean_test_accTop1 /= (num_branches - 1)\n",
        "    mean_test_accTop5 /= (num_branches - 1)\n",
        "    # compute mean of all metrics in summary\n",
        "        \n",
        "    test_metrics = { 'test_loss': loss_avg.value(),\n",
        "                     'test_true_loss': loss_true_avg.value(),\n",
        "                     'test_group_loss': loss_group_avg.value(),\n",
        "                     'mean_test_accTop1': mean_test_accTop1,\n",
        "                     'mean_test_accTop5': mean_test_accTop5,\n",
        "                     'test_accTop1': accTop1_avg[num_branches].value(),\n",
        "                     'test_accTop5': accTop5_avg[num_branches].value(),\n",
        "                     'stu_test_accTop1': accTop1_avg[num_branches - 1].value(),\n",
        "                     'stu_test_accTop5': accTop5_avg[num_branches - 1].value(),\n",
        "                     'dist': dist_avg.value(),\n",
        "                     'time': time.time() - end}\n",
        "    for i in range(num_branches - 1):\n",
        "        test_metrics.update({'stu'+str(i)+'test_accTop1' : accTop1_avg[i].value()})\n",
        "        test_metrics.update({'stu'+str(i)+'test_accTop5' : accTop5_avg[i].value()})\n",
        "    \n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in test_metrics.items())\n",
        "\n",
        "    return test_metrics"
      ],
      "metadata": {
        "id": "x3hrzh4UySwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dict_to_json(d, json_path):\n",
        "    \"\"\"Saves dict of floats in json file\n",
        "    Args:\n",
        "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
        "        json_path: (string) path to json file\n",
        "    \"\"\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
        "        d = {k: v for k, v in d.items()}\n",
        "        json.dump(d, f, indent=4)"
      ],
      "metadata": {
        "id": "1ejLRTfB05gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_okddip(model, train_loader, test_loader, optimizer, criterion, criterion_T, accuracy, model_dir):\n",
        "    \n",
        "    start_epoch = 0\n",
        "    best_acc = 0.\n",
        "           \n",
        "    # Save best ensemble or average accTop1\n",
        "    choose_E = False\n",
        "    \n",
        "    # Save the parameters for export\n",
        "    result_train_metrics = list(range(num_epochs))\n",
        "    result_test_metrics = list(range(num_epochs))\n",
        "        \n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "               \n",
        "        # Set consistency_weight or originial temperature scale \n",
        "        consistency_epoch = start_consistency * num_epochs \n",
        "        if epoch < consistency_epoch:\n",
        "            consistency_weight = 1\n",
        "        else:\n",
        "            consistency_weight = get_current_consistency_weight(epoch - consistency_epoch, length)\n",
        "        \n",
        "        # compute number of batches in one epoch (one full pass over the training set)\n",
        "        train_metrics = train_okddip(train_loader, model, optimizer, criterion, criterion_T, accuracy, consistency_weight)\n",
        "    \n",
        "        # Evaluate for one epoch on validation set\n",
        "        test_metrics = evaluate_okddip(test_loader, model, criterion, criterion_T, accuracy, consistency_weight) \n",
        "        \n",
        "        # Find the best accTop1 for Branch1.\n",
        "        if choose_E:\n",
        "            test_acc = test_metrics['test_accTop1']\n",
        "        else:\n",
        "            test_acc = test_metrics['stu_test_accTop1']\n",
        "        \n",
        "        result_train_metrics[epoch] = train_metrics\n",
        "        result_test_metrics[epoch] = test_metrics\n",
        "        \n",
        "        # Save latest train/test metrics\n",
        "        torch.save(result_train_metrics, os.path.join(model_dir, 'train_metrics'))\n",
        "        torch.save(result_test_metrics, os.path.join(model_dir, 'test_metrics'))\n",
        "\n",
        "        last_path = os.path.join(model_dir, 'last.pth')        \n",
        "        # Save latest model weights, optimizer and accuracy\n",
        "        torch.save({    'state_dict': model.state_dict(),\n",
        "                        'epoch': epoch + 1,\n",
        "                        'optim_dict': optimizer.state_dict(),\n",
        "                        'test_accTop1': test_metrics['test_accTop1'],\n",
        "                        'mean_test_accTop1': test_metrics['mean_test_accTop1'],\n",
        "                        'stu_test_accTop1': test_metrics['stu_test_accTop1']}, last_path)\n",
        "        # If best_eval, best_save_path\n",
        "        is_best = test_acc >= best_acc\n",
        "        if is_best:\n",
        "            #logging.info(\"- Found better accuracy\")            \n",
        "            best_acc = test_acc            \n",
        "            # Save best metrics in a json file in the model directory\n",
        "            test_metrics['epoch'] = epoch + 1\n",
        "            save_dict_to_json(test_metrics, os.path.join(model_dir, \"test_best_metrics.json\"))\n",
        "        \n",
        "            # Save model and optimizer\n",
        "            shutil.copyfile(last_path, os.path.join(model_dir, 'best.pth'))"
      ],
      "metadata": {
        "id": "NYBql-ldySzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_consistency_weight(current, rampup_length = length):\n",
        "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current, 0.0, rampup_length)\n",
        "        phase = 1.0 - current / rampup_length\n",
        "        return float(np.exp(-5.0 * phase * phase))"
      ],
      "metadata": {
        "id": "lEyk3jvMyS2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = OKDDcnn(n_classes=num_classes, num_branches=num_branches, factor=8, en=False, cfg=s_cfg, fc=s_fc).to(cuda_device)"
      ],
      "metadata": {
        "id": "SVySqdvtyS4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = (sum(p.numel() for p in model.parameters()))\n",
        "print(num_params)\n",
        "\n",
        "# Loss and optimizer(SGD with 0.9 momentum)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion_T = KL_Loss(T).to(cuda_device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68bkW32pyS7h",
        "outputId": "ed70b1e5-57b6-4c39-d58e-fdcbffd1d4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_okddip(model, train_dataloader, val_dataloader, optimizer, criterion, criterion_T, accuracy, model_dir='./student/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlWj6Z_J30ko",
        "outputId": "81ed58a0-ce7d-4c2a-d9a3-5174be5f196e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 66.09it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.83it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 64.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 67.61it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.34it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 69.18it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 70.94it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.10it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 70.10it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.13it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.39it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.31it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.81it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.57it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.92it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.74it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 64.37it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 67.58it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 66.08it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.51it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.99it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.25it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.33it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.62it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.12it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.68it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.44it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.15it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.43it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.52it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.19it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.47it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.19it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.78it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.89it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.00it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.94it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 68.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 68.40it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 67.28it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 65.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 70.88it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.83it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.74it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.22it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.78it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.34it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.42it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.70it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.67it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.20it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.76it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.66it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.28it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.32it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 70.83it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.79it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.38it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.94it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.56it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 64.88it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 44.19it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 52.08it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 55.62it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 50.12it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 59.54it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 58.92it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 51.12it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 61.52it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 58.88it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 54.65it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 60.19it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 60.03it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.00it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.81it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.20it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.17it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.94it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.66it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 68.92it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.33it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.08it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.90it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 70.99it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.37it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.00it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.06it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.75it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.79it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.79it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.48it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.35it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.84it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.26it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.01it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.90it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.96it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.84it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.83it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.66it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.58it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.77it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.39it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.57it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 67.75it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 65.77it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.82it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.63it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 72.99it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.16it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.52it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.53it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.37it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.25it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.06it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.83it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.45it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.73it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.15it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.51it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.68it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.64it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.64it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.91it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.02it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.82it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.20it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.23it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 68.91it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 67.74it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.75it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.81it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.61it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.62it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 74.63it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.92it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.69it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.45it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.33it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.84it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.53it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.07it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.66it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.18it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.11it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.77it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.71it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.20it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.72it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.41it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 71.36it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.04it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 70.48it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 69.17it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.75it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.77it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.79it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.96it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 80.65it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 79.37it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.21it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.67it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.80it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.05it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.49it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.95it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 77.38it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 78.77it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.84it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 73.27it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 68.87it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 76.88it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.07it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 75.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consistency_weight = get_current_consistency_weight(50, length)\n",
        "test_metrics = evaluate_okddip(test_dataloader, model, criterion, criterion_T, accuracy, consistency_weight)"
      ],
      "metadata": {
        "id": "RK6Fa9yg30nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srLomOQZ30qH",
        "outputId": "ef9df4f2-50ad-4425-9b3d-fcaed9355c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.07673324106364095,\n",
              " 'test_true_loss': 0.05624389749291779,\n",
              " 'test_group_loss': 0.0019687666922185254,\n",
              " 'mean_test_accTop1': 99.60971524288108,\n",
              " 'mean_test_accTop5': 99.98994974874371,\n",
              " 'test_accTop1': 99.60552763819095,\n",
              " 'test_accTop5': 99.98994974874371,\n",
              " 'stu_test_accTop1': 99.60552763819095,\n",
              " 'stu_test_accTop5': 99.98994974874371,\n",
              " 'dist': 0.0008984793681963235,\n",
              " 'time': 11.825973510742188,\n",
              " 'stu0test_accTop1': 99.6105527638191,\n",
              " 'stu0test_accTop5': 99.98994974874371,\n",
              " 'stu1test_accTop1': 99.61557788944724,\n",
              " 'stu1test_accTop5': 99.98994974874371,\n",
              " 'stu2test_accTop1': 99.60301507537689,\n",
              " 'stu2test_accTop5': 99.98994974874371}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx06AjaF3_bp",
        "outputId": "07e86abd-bd06-42ef-b399-385f462fb43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 15000\n",
            "conv1.bias 15\n",
            "conv2.weight 11250\n",
            "conv2.bias 30\n",
            "classifier3_0.0.weight 22500\n",
            "classifier3_0.0.bias 30\n",
            "classifier3_0.2.weight 270\n",
            "classifier3_0.2.bias 9\n",
            "classifier3_1.0.weight 22500\n",
            "classifier3_1.0.bias 30\n",
            "classifier3_1.2.weight 270\n",
            "classifier3_1.2.bias 9\n",
            "classifier3_2.0.weight 22500\n",
            "classifier3_2.0.bias 30\n",
            "classifier3_2.2.weight 270\n",
            "classifier3_2.2.bias 9\n",
            "classifier3_3.0.weight 22500\n",
            "classifier3_3.0.bias 30\n",
            "classifier3_3.2.weight 270\n",
            "classifier3_3.2.bias 9\n",
            "query_weight.weight 69750\n",
            "key_weight.weight 69750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self distillaiton: class-wise predictions (CS-KD)**"
      ],
      "metadata": {
        "id": "7sUAChzltIwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/alinlab/cs-kd\n",
        "T = 10\n",
        "lamda = 1\n",
        "learning_rate = 1e-3\n",
        "\n",
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.89  # prune ratio of conv layers\n",
        "linear_r = 0.89 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "FS-iftScto45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KDLoss(nn.Module):\n",
        "    def __init__(self, temp_factor):\n",
        "        super(KDLoss, self).__init__()\n",
        "        self.temp_factor = temp_factor\n",
        "        self.kl_div = nn.KLDivLoss(reduction=\"sum\")\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        log_p = torch.log_softmax(input/self.temp_factor, dim=1)\n",
        "        q = torch.softmax(target/self.temp_factor, dim=1)\n",
        "        loss = self.kl_div(log_p, q)*(self.temp_factor**2)/input.size(0)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "MAXLI5nFONeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kdloss = KDLoss(T)"
      ],
      "metadata": {
        "id": "JrLNmaTgtQh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"validation\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input = input.float()\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # if idx % print_freq == 0:\n",
        "            #     print('Test: [{0}/{1}]\\t'\n",
        "            #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "            #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "            #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "            #            idx, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "            #            top1=top1, top5=top5))\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "yJj3CZ1Ax-JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vanilla(epoch, train_loader, model, criterion, optimizer):\n",
        "    \"\"\"vanilla training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.float()\n",
        "        batch_size = input.size(0)\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # ===================forward=====================\n",
        "        targets_ = target[:batch_size//2]\n",
        "        outputs = model(input[:batch_size//2])\n",
        "        loss = torch.mean(criterion(outputs, targets_))\n",
        "        #train_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_cls = model(input[batch_size//2:])\n",
        "        cls_loss = kdloss(outputs, outputs_cls.detach())\n",
        "        loss += lamda * cls_loss\n",
        "        #train_cls_loss += cls_loss.item()\n",
        "\n",
        "        acc1, acc5 = accuracy(outputs, targets_, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "\n",
        "        # ===================backward=====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ===================meters=====================\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # tensorboard logger\n",
        "        pass\n",
        "\n",
        "        # print info\n",
        "        # if idx % print_freq == 0:\n",
        "        #     print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "        #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "        #            epoch, idx, len(train_loader), batch_time=batch_time,\n",
        "        #            data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "        #     sys.stdout.flush()\n",
        "\n",
        "    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, losses.avg"
      ],
      "metadata": {
        "id": "2-JqqTNBtQna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_distillation(train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './teacher/t_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./teacher/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './teacher/t_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "XvqakoRPtQqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_distillation(train_dataloader, val_dataloader, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oezK4MPHtQsj",
        "outputId": "6aa5309f-db52-4e10-b020-d63be43086e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * Acc@1 41.054 Acc@5 81.589\n",
            "epoch 1, total time 0.29\n",
            " * Acc@1 40.667 Acc@5 85.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 44.098 Acc@5 86.488\n",
            "epoch 2, total time 0.16\n",
            " * Acc@1 40.667 Acc@5 88.667\n",
            "==> training...\n",
            " * Acc@1 48.107 Acc@5 90.052\n",
            "epoch 3, total time 0.16\n",
            " * Acc@1 52.000 Acc@5 89.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 53.898 Acc@5 90.572\n",
            "epoch 4, total time 0.16\n",
            " * Acc@1 54.667 Acc@5 89.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 58.426 Acc@5 90.720\n",
            "epoch 5, total time 0.16\n",
            " * Acc@1 55.333 Acc@5 89.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 58.426 Acc@5 89.829\n",
            "epoch 6, total time 0.17\n",
            " * Acc@1 55.667 Acc@5 89.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 58.500 Acc@5 88.938\n",
            "epoch 7, total time 0.16\n",
            " * Acc@1 55.667 Acc@5 89.333\n",
            "==> training...\n",
            " * Acc@1 59.540 Acc@5 89.978\n",
            "epoch 8, total time 0.16\n",
            " * Acc@1 63.333 Acc@5 91.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 68.597 Acc@5 91.834\n",
            "epoch 9, total time 0.16\n",
            " * Acc@1 66.000 Acc@5 93.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 71.715 Acc@5 93.764\n",
            "epoch 10, total time 0.16\n",
            " * Acc@1 76.333 Acc@5 93.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 78.545 Acc@5 94.135\n",
            "epoch 11, total time 0.16\n",
            " * Acc@1 80.667 Acc@5 94.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 79.733 Acc@5 94.655\n",
            "epoch 12, total time 0.16\n",
            " * Acc@1 81.000 Acc@5 95.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 80.549 Acc@5 94.952\n",
            "epoch 13, total time 0.16\n",
            " * Acc@1 82.333 Acc@5 94.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 82.405 Acc@5 95.397\n",
            "epoch 14, total time 0.16\n",
            " * Acc@1 83.667 Acc@5 95.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 83.593 Acc@5 96.511\n",
            "epoch 15, total time 0.16\n",
            " * Acc@1 84.333 Acc@5 96.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 84.410 Acc@5 96.659\n",
            "epoch 16, total time 0.16\n",
            " * Acc@1 85.000 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 84.855 Acc@5 96.956\n",
            "epoch 17, total time 0.16\n",
            " * Acc@1 85.333 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 85.375 Acc@5 97.179\n",
            "epoch 18, total time 0.16\n",
            " * Acc@1 85.667 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 85.449 Acc@5 97.476\n",
            "epoch 19, total time 0.17\n",
            " * Acc@1 86.000 Acc@5 97.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 85.820 Acc@5 97.996\n",
            "epoch 20, total time 0.16\n",
            " * Acc@1 86.000 Acc@5 97.667\n",
            "==> training...\n",
            " * Acc@1 86.266 Acc@5 97.996\n",
            "epoch 21, total time 0.16\n",
            " * Acc@1 86.333 Acc@5 97.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 86.711 Acc@5 98.070\n",
            "epoch 22, total time 0.16\n",
            " * Acc@1 86.667 Acc@5 97.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 86.860 Acc@5 98.367\n",
            "epoch 23, total time 0.16\n",
            " * Acc@1 87.333 Acc@5 97.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 87.231 Acc@5 98.293\n",
            "epoch 24, total time 0.17\n",
            " * Acc@1 88.000 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 87.379 Acc@5 98.367\n",
            "epoch 25, total time 0.16\n",
            " * Acc@1 88.000 Acc@5 96.667\n",
            "==> training...\n",
            " * Acc@1 87.528 Acc@5 98.218\n",
            "epoch 26, total time 0.16\n",
            " * Acc@1 88.000 Acc@5 96.667\n",
            "==> training...\n",
            " * Acc@1 87.899 Acc@5 98.367\n",
            "epoch 27, total time 0.16\n",
            " * Acc@1 88.333 Acc@5 96.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 88.419 Acc@5 98.144\n",
            "epoch 28, total time 0.16\n",
            " * Acc@1 88.333 Acc@5 97.667\n",
            "==> training...\n",
            " * Acc@1 88.790 Acc@5 98.218\n",
            "epoch 29, total time 0.16\n",
            " * Acc@1 89.000 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 89.161 Acc@5 98.218\n",
            "epoch 30, total time 0.17\n",
            " * Acc@1 89.000 Acc@5 97.333\n",
            "==> training...\n",
            " * Acc@1 89.681 Acc@5 98.293\n",
            "epoch 31, total time 0.17\n",
            " * Acc@1 89.000 Acc@5 97.333\n",
            "==> training...\n",
            " * Acc@1 90.275 Acc@5 98.367\n",
            "epoch 32, total time 0.17\n",
            " * Acc@1 89.000 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 90.423 Acc@5 98.367\n",
            "epoch 33, total time 0.16\n",
            " * Acc@1 88.667 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 90.720 Acc@5 98.441\n",
            "epoch 34, total time 0.16\n",
            " * Acc@1 89.333 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 91.017 Acc@5 98.441\n",
            "epoch 35, total time 0.16\n",
            " * Acc@1 89.000 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 91.314 Acc@5 98.664\n",
            "epoch 36, total time 0.17\n",
            " * Acc@1 89.333 Acc@5 97.667\n",
            "==> training...\n",
            " * Acc@1 91.388 Acc@5 98.664\n",
            "epoch 37, total time 0.16\n",
            " * Acc@1 89.333 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 91.908 Acc@5 98.664\n",
            "epoch 38, total time 0.17\n",
            " * Acc@1 89.000 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 91.982 Acc@5 98.738\n",
            "epoch 39, total time 0.20\n",
            " * Acc@1 89.667 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 91.982 Acc@5 98.812\n",
            "epoch 40, total time 0.18\n",
            " * Acc@1 90.000 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 92.056 Acc@5 98.812\n",
            "epoch 41, total time 0.19\n",
            " * Acc@1 90.333 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 92.279 Acc@5 98.812\n",
            "epoch 42, total time 0.17\n",
            " * Acc@1 90.333 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 92.502 Acc@5 98.886\n",
            "epoch 43, total time 0.18\n",
            " * Acc@1 91.000 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 92.576 Acc@5 98.961\n",
            "epoch 44, total time 0.17\n",
            " * Acc@1 91.333 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 92.799 Acc@5 99.035\n",
            "epoch 45, total time 0.16\n",
            " * Acc@1 91.000 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 93.022 Acc@5 98.961\n",
            "epoch 46, total time 0.17\n",
            " * Acc@1 91.333 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 93.170 Acc@5 99.035\n",
            "epoch 47, total time 0.17\n",
            " * Acc@1 91.333 Acc@5 98.000\n",
            "==> training...\n",
            " * Acc@1 93.318 Acc@5 98.961\n",
            "epoch 48, total time 0.17\n",
            " * Acc@1 91.667 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 93.690 Acc@5 98.961\n",
            "epoch 49, total time 0.16\n",
            " * Acc@1 91.667 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 93.764 Acc@5 98.961\n",
            "epoch 50, total time 0.18\n",
            " * Acc@1 92.000 Acc@5 98.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 93.838 Acc@5 99.035\n",
            "epoch 51, total time 0.17\n",
            " * Acc@1 92.000 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 93.987 Acc@5 99.109\n",
            "epoch 52, total time 0.16\n",
            " * Acc@1 92.333 Acc@5 98.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 94.061 Acc@5 99.109\n",
            "epoch 53, total time 0.16\n",
            " * Acc@1 92.000 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 94.061 Acc@5 99.183\n",
            "epoch 54, total time 0.16\n",
            " * Acc@1 92.000 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 94.284 Acc@5 99.183\n",
            "epoch 55, total time 0.16\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 94.358 Acc@5 99.183\n",
            "epoch 56, total time 0.15\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 94.506 Acc@5 99.332\n",
            "epoch 57, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 94.581 Acc@5 99.332\n",
            "epoch 58, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 94.581 Acc@5 99.406\n",
            "epoch 59, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 94.878 Acc@5 99.555\n",
            "epoch 60, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 94.952 Acc@5 99.406\n",
            "epoch 61, total time 0.17\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 94.952 Acc@5 99.480\n",
            "epoch 62, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.100 Acc@5 99.629\n",
            "epoch 63, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.249 Acc@5 99.629\n",
            "epoch 64, total time 0.16\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.323 Acc@5 99.703\n",
            "epoch 65, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.323 Acc@5 99.703\n",
            "epoch 66, total time 0.15\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.397 Acc@5 99.703\n",
            "epoch 67, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.620 Acc@5 99.629\n",
            "epoch 68, total time 0.16\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.546 Acc@5 99.629\n",
            "epoch 69, total time 0.17\n",
            " * Acc@1 93.333 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 95.694 Acc@5 99.629\n",
            "epoch 70, total time 0.17\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.620 Acc@5 99.629\n",
            "epoch 71, total time 0.17\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.843 Acc@5 99.629\n",
            "epoch 72, total time 0.16\n",
            " * Acc@1 93.667 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 95.843 Acc@5 99.629\n",
            "epoch 73, total time 0.16\n",
            " * Acc@1 92.333 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.917 Acc@5 99.629\n",
            "epoch 74, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 95.843 Acc@5 99.629\n",
            "epoch 75, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.065 Acc@5 99.629\n",
            "epoch 76, total time 0.16\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.629\n",
            "epoch 77, total time 0.16\n",
            " * Acc@1 93.333 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.065 Acc@5 99.703\n",
            "epoch 78, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.214 Acc@5 99.629\n",
            "epoch 79, total time 0.17\n",
            " * Acc@1 93.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.629\n",
            "epoch 80, total time 0.16\n",
            " * Acc@1 93.333 Acc@5 98.667\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.629\n",
            "epoch 81, total time 0.17\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.629\n",
            "epoch 82, total time 0.16\n",
            " * Acc@1 92.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.629\n",
            "epoch 83, total time 0.16\n",
            " * Acc@1 93.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.629\n",
            "epoch 84, total time 0.16\n",
            " * Acc@1 93.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.140 Acc@5 99.703\n",
            "epoch 85, total time 0.16\n",
            " * Acc@1 93.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.214 Acc@5 99.629\n",
            "epoch 86, total time 0.16\n",
            " * Acc@1 93.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.362 Acc@5 99.629\n",
            "epoch 87, total time 0.18\n",
            " * Acc@1 94.333 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 96.511 Acc@5 99.629\n",
            "epoch 88, total time 0.17\n",
            " * Acc@1 94.000 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 96.511 Acc@5 99.629\n",
            "epoch 89, total time 0.16\n",
            " * Acc@1 93.667 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 96.585 Acc@5 99.629\n",
            "epoch 90, total time 0.16\n",
            " * Acc@1 94.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.511 Acc@5 99.555\n",
            "epoch 91, total time 0.16\n",
            " * Acc@1 94.000 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 96.511 Acc@5 99.777\n",
            "epoch 92, total time 0.17\n",
            " * Acc@1 93.667 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 96.511 Acc@5 99.777\n",
            "epoch 93, total time 0.16\n",
            " * Acc@1 94.000 Acc@5 98.333\n",
            "==> training...\n",
            " * Acc@1 96.808 Acc@5 99.777\n",
            "epoch 94, total time 0.16\n",
            " * Acc@1 94.667 Acc@5 98.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 96.511 Acc@5 99.777\n",
            "epoch 95, total time 0.16\n",
            " * Acc@1 94.333 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.882 Acc@5 99.703\n",
            "epoch 96, total time 0.16\n",
            " * Acc@1 94.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.956 Acc@5 99.852\n",
            "epoch 97, total time 0.16\n",
            " * Acc@1 94.000 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.882 Acc@5 99.926\n",
            "epoch 98, total time 0.16\n",
            " * Acc@1 94.333 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 96.808 Acc@5 99.852\n",
            "epoch 99, total time 0.16\n",
            " * Acc@1 94.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 97.030 Acc@5 99.926\n",
            "epoch 100, total time 0.17\n",
            " * Acc@1 95.333 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 96.882 Acc@5 99.926\n",
            "epoch 101, total time 0.15\n",
            " * Acc@1 95.333 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 97.030 Acc@5 99.926\n",
            "epoch 102, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 98.667\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 97.030 Acc@5 99.926\n",
            "epoch 103, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 97.105 Acc@5 99.852\n",
            "epoch 104, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 98.667\n",
            "==> training...\n",
            " * Acc@1 97.253 Acc@5 99.852\n",
            "epoch 105, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.327 Acc@5 99.926\n",
            "epoch 106, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.327 Acc@5 99.926\n",
            "epoch 107, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.327 Acc@5 99.852\n",
            "epoch 108, total time 0.17\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.327 Acc@5 99.926\n",
            "epoch 109, total time 0.18\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.402 Acc@5 99.926\n",
            "epoch 110, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.402 Acc@5 99.926\n",
            "epoch 111, total time 0.17\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.476 Acc@5 99.926\n",
            "epoch 112, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.476 Acc@5 99.926\n",
            "epoch 113, total time 0.18\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.550 Acc@5 99.926\n",
            "epoch 114, total time 0.17\n",
            " * Acc@1 95.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.550 Acc@5 100.000\n",
            "epoch 115, total time 0.18\n",
            " * Acc@1 96.333 Acc@5 99.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 97.624 Acc@5 100.000\n",
            "epoch 116, total time 0.17\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.550 Acc@5 100.000\n",
            "epoch 117, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.699 Acc@5 100.000\n",
            "epoch 118, total time 0.18\n",
            " * Acc@1 96.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.699 Acc@5 100.000\n",
            "epoch 119, total time 0.17\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.921 Acc@5 100.000\n",
            "epoch 120, total time 0.18\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 97.847 Acc@5 100.000\n",
            "epoch 121, total time 0.17\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.996 Acc@5 100.000\n",
            "epoch 122, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 97.996 Acc@5 100.000\n",
            "epoch 123, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.293 Acc@5 100.000\n",
            "epoch 124, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.218 Acc@5 100.000\n",
            "epoch 125, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.515 Acc@5 100.000\n",
            "epoch 126, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.367 Acc@5 100.000\n",
            "epoch 127, total time 0.17\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.441 Acc@5 100.000\n",
            "epoch 128, total time 0.15\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 98.441 Acc@5 100.000\n",
            "epoch 129, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.664 Acc@5 100.000\n",
            "epoch 130, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.589 Acc@5 100.000\n",
            "epoch 131, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.589 Acc@5 100.000\n",
            "epoch 132, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.664 Acc@5 100.000\n",
            "epoch 133, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 134, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.664 Acc@5 100.000\n",
            "epoch 135, total time 0.17\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 136, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.812 Acc@5 100.000\n",
            "epoch 137, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 138, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.812 Acc@5 100.000\n",
            "epoch 139, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 140, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.812 Acc@5 100.000\n",
            "epoch 141, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.738 Acc@5 100.000\n",
            "epoch 142, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.812 Acc@5 100.000\n",
            "epoch 143, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.812 Acc@5 100.000\n",
            "epoch 144, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.812 Acc@5 100.000\n",
            "epoch 145, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.886 Acc@5 100.000\n",
            "epoch 146, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.886 Acc@5 100.000\n",
            "epoch 147, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.886 Acc@5 100.000\n",
            "epoch 148, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 98.961 Acc@5 100.000\n",
            "epoch 149, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 150, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 151, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 152, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 153, total time 0.17\n",
            " * Acc@1 95.667 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 154, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 155, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 156, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 157, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 158, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 159, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.000\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 160, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> Saving...\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 161, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 162, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 163, total time 0.16\n",
            " * Acc@1 96.667 Acc@5 99.333\n",
            "saving the best model!\n",
            "==> training...\n",
            " * Acc@1 99.035 Acc@5 100.000\n",
            "epoch 164, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 165, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 166, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 167, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 168, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 169, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 170, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 171, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.333\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 172, total time 0.17\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 173, total time 0.17\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 174, total time 0.15\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 175, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 176, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 177, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 178, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 179, total time 0.17\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 180, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 181, total time 0.18\n",
            " * Acc@1 95.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 182, total time 0.17\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 183, total time 0.17\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 184, total time 0.18\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 185, total time 0.16\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 186, total time 0.17\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 187, total time 0.17\n",
            " * Acc@1 96.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 188, total time 0.17\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 189, total time 0.17\n",
            " * Acc@1 95.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 190, total time 0.18\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.258 Acc@5 100.000\n",
            "epoch 191, total time 0.16\n",
            " * Acc@1 96.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 192, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 193, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 194, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 195, total time 0.16\n",
            " * Acc@1 95.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 196, total time 0.16\n",
            " * Acc@1 95.000 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.183 Acc@5 100.000\n",
            "epoch 197, total time 0.15\n",
            " * Acc@1 95.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 198, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 199, total time 0.16\n",
            " * Acc@1 95.333 Acc@5 99.667\n",
            "==> training...\n",
            " * Acc@1 99.109 Acc@5 100.000\n",
            "epoch 200, total time 0.16\n",
            " * Acc@1 95.667 Acc@5 99.667\n",
            "==> Saving...\n",
            "best accuracy: tensor(96.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "self_model.load_state_dict(torch.load('./teacher/t_best.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjotXazzyK-B",
        "outputId": "1529425a-60d4-4fe8-83d7-35a785e0a92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "self_model = self_model.to(cuda_device)\n",
        "\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, self_model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4C1TDoIyLAd",
        "outputId": "e24f50a0-0ef5-42c5-a616-acda48f778e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 95.955 Acc@5 99.708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(t_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtBtedu9yLDJ",
        "outputId": "648f78f9-12a8-4072-bdbc-dd23c83a6e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 95.955\n",
            "Teacher top-5 test accuracy: 99.708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of teacher parameters:', sum(p.numel() for p in self_model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuUgSaIayLFq",
        "outputId": "45484ab1-6b0d-4ead-8e9c-1cf4b5be67eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of teacher parameters: 8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-distillation: TF-KD**"
      ],
      "metadata": {
        "id": "IqeRDI2XDR6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper with code: https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation\n",
        "alpha = 0.5\n",
        "T = 4\n",
        "multiplier = 50\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "learning_rate = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "QEfN2GPMSlSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_kd_regularization(outputs, labels):\n",
        "    \"\"\"\n",
        "    loss function for mannually-designed regularization: Tf-KD_{reg}\n",
        "    \"\"\"\n",
        "    correct_prob = 0.99    # the probability for correct class in u(k)\n",
        "    loss_CE = F.cross_entropy(outputs, labels)\n",
        "    K = outputs.size(1)\n",
        "\n",
        "    teacher_soft = torch.ones_like(outputs).cuda()\n",
        "    teacher_soft = teacher_soft*(1-correct_prob)/(K-1)  # p^d(k)\n",
        "    for i in range(outputs.shape[0]):\n",
        "        teacher_soft[i ,labels[i]] = correct_prob\n",
        "    loss_soft_regu = nn.KLDivLoss()(F.log_softmax(outputs, dim=1), F.softmax(teacher_soft/T, dim=1))*multiplier\n",
        "\n",
        "    KD_loss = (1. - alpha)*loss_CE + alpha*loss_soft_regu\n",
        "\n",
        "    return KD_loss"
      ],
      "metadata": {
        "id": "8klWiJorDRPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_distillation(train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_tf_kd(model, optimizer, train_loader, epoch)\n",
        "  \n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './teacher/t_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./teacher/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './teacher/t_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "OTu9_U0Zzt0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining train_kd functions\n",
        "def train_tf_kd(model, optimizer, dataloader, epoch):\n",
        "    \"\"\"\n",
        "    KD Train the model on `num_steps` batches\n",
        "    \"\"\"\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "    # teacher_model.eval()\n",
        "    loss_avg = RunningAverage()\n",
        "    losses = AverageMeter()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "\n",
        "            train_batch, labels_batch = train_batch.cuda(), labels_batch.cuda()\n",
        "            # convert to torch Variables\n",
        "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "            # compute model output, fetch teacher output, and compute KD loss\n",
        "            output_batch = model(train_batch)\n",
        "\n",
        "            # get one batch output from teacher model\n",
        "            #output_teacher_batch = teacher_model(train_batch).cuda()\n",
        "            #output_teacher_batch = Variable(output_teacher_batch, requires_grad=False)\n",
        "\n",
        "            loss = loss_kd_regularization(output_batch, labels_batch)\n",
        "\n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # performs updates using calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = output_batch.max(1)\n",
        "            total += labels_batch.size(0)\n",
        "            correct += predicted.eq(labels_batch).sum().item()\n",
        "            # update the average loss\n",
        "            loss_avg.update(loss.data)\n",
        "            losses.update(loss.item(), train_batch.size(0))\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg.value()), lr='{:05.6f}'.format(optimizer.param_groups[0]['lr']))\n",
        "            t.update()\n",
        "\n",
        "    acc = 100.*correct/total\n",
        "    print(\"- Train accuracy: {acc:.4f}, training loss: {loss:.4f}\".format(acc = acc, loss = losses.avg))\n",
        "    return acc, losses.avg\n"
      ],
      "metadata": {
        "id": "14OSIENLiclw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_distillation(train_dataloader, val_dataloader, epochs=200)"
      ],
      "metadata": {
        "id": "gwIouTMVicqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "cc094b02-7265-4dfc-b3fe-a5c2bc2fb810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-64b7c90ff214>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself_distillation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-2b1d0eb872ca>\u001b[0m in \u001b[0;36mself_distillation\u001b[0;34m(train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tf_kd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-f58d1a048001>\u001b[0m in \u001b[0;36mtrain_tf_kd\u001b[0;34m(model, optimizer, dataloader, epoch)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# convert to torch Variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cuda'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "self_model.load_state_dict(torch.load('./teacher/t_best.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veRf4ChK6CaN",
        "outputId": "9cdd8e50-f22b-4380-8098-a0e403408e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "self_model = self_model.to(cuda_device)\n",
        "\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, self_model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMfYSQBd7v4m",
        "outputId": "490ebc69-bfac-4b7a-aef7-51db003dfe33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 99.251 Acc@5 100.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(t_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp85CAoj71an",
        "outputId": "ee031e85-64d1-4edf-f091-f5c96a9330a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 99.251\n",
            "Teacher top-5 test accuracy: 100.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulIAxQmH78U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-distillation: PS-KD**"
      ],
      "metadata": {
        "id": "72j9CmLXpol1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code: https://github.com/lgcnsai/PS-KD-Pytorch\n",
        "lmbda = 0.5\n",
        "mu = 0.4\n",
        "T = 4\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "learning_rate = 8e-4\n",
        "alpha_T = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "FGxQjc-Ermdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.dataset = train_dataset\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        data, target = self.dataset[index]\n",
        "        \n",
        "        # Your transformations here (or set it in CIFAR10)\n",
        "        \n",
        "        return data, target, index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "98gRHMmX48jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Custom_Dataset()\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size) # create your dataloader"
      ],
      "metadata": {
        "id": "0a4qcpIm5XYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_CrossEntropy_PSKD(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Custom_CrossEntropy_PSKD, self).__init__()\n",
        "\t\tself.logsoftmax = nn.LogSoftmax(dim=1).cuda()\n",
        "\n",
        "\tdef forward(self, output, targets):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tinputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
        "\t\t\ttargets: ground truth labels with shape (num_classes)\n",
        "\t\t\"\"\"\n",
        "\t\tlog_probs = self.logsoftmax(output)\n",
        "\t\tloss = (- targets * log_probs).mean(0).sum()\n",
        "\t\treturn loss "
      ],
      "metadata": {
        "id": "kvRYCQloe1xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_distillation(train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_CE_pskd = Custom_CrossEntropy_PSKD().cuda()\n",
        "    all_predictions = torch.zeros(len(train_loader.dataset), num_classes, dtype=torch.float32)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        alpha_t = alpha_T * ((epoch + 1) / num_epochs)\n",
        "        alpha_t = max(0, alpha_t)\n",
        "\n",
        "        all_predictions = train_ps_kd(\n",
        "                                all_predictions,\n",
        "                                model,\n",
        "                                criterion_CE_pskd,\n",
        "                                optimizer,\n",
        "                                train_loader,\n",
        "                                epoch,\n",
        "                                alpha_t)\n",
        "\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './teacher/t_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./teacher/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './teacher/t_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "2BEwaNKpyHYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining train_kd functions\n",
        "def train_ps_kd(all_predictions, model, criterion_CE_pskd, optimizer, train_loader, epoch, alpha_t):\n",
        "    \"\"\"\n",
        "    KD Train the model on `num_steps` batches\n",
        "    \"\"\"\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "    train_top1 = AverageMeter()\n",
        "    train_top5 = AverageMeter()\n",
        "    train_losses = AverageMeter()\n",
        "\n",
        "    loss_avg = RunningAverage()\n",
        "    losses = AverageMeter()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(train_loader)) as t:\n",
        "        for batch_idx, (inputs, targets, input_indices) in enumerate(train_loader):\n",
        "\n",
        "            targets_numpy = targets.cpu().detach().numpy()\n",
        "            identity_matrix = torch.eye(num_classes) \n",
        "            targets_one_hot = identity_matrix[targets_numpy]\n",
        "            \n",
        "            if epoch == 0:\n",
        "                all_predictions[input_indices] = targets_one_hot\n",
        "\n",
        "            # create new soft-targets\n",
        "            soft_targets = ((1 - alpha_t) * targets_one_hot) + (alpha_t * all_predictions[input_indices])\n",
        "            soft_targets = soft_targets.cuda()\n",
        "                \n",
        "            # student model\n",
        "            # compute output\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # convert to torch Variables\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = model(inputs)\n",
        "            softmax_output = F.softmax(outputs, dim=1) \n",
        "            loss = criterion_CE_pskd(outputs, soft_targets)\n",
        "\n",
        "            train_losses.update(loss.item(), inputs.size(0))\n",
        "            err1, err5 = accuracy(outputs.data, targets, topk=(1, 5))\n",
        "            train_top1.update(err1.item(), inputs.size(0))\n",
        "            train_top5.update(err5.item(), inputs.size(0))\n",
        "\n",
        "            # compute gradient and do SGD step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            all_predictions[input_indices] = softmax_output.cpu().detach()\n",
        "          \n",
        "            # update the average loss\n",
        "            loss_avg.update(loss.data)\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg.value()), lr='{:05.6f}'.format(optimizer.param_groups[0]['lr']))\n",
        "            t.update()\n",
        "\n",
        "    acc = 100.*correct/total\n",
        "    print(\"- Train accuracy: {acc:.4f}, training loss: {loss:.4f}\".format(acc = acc, loss = losses.avg))\n",
        "    return all_predictions\n"
      ],
      "metadata": {
        "id": "dASC8t70lzn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_distillation(train_dataloader, val_dataloader, num_epochs)"
      ],
      "metadata": {
        "id": "IujC0hZglzqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d55123a-a842-4064-b57f-ddfaa05880c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 57.89it/s, loss=1.645, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 42.5761, training loss: 1.6455\n",
            " * Acc@1 41.333 Acc@5 85.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 102.70it/s, loss=1.206, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 54.0831, training loss: 1.2061\n",
            " * Acc@1 58.000 Acc@5 89.333\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 102.68it/s, loss=0.839, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 67.7431, training loss: 0.8391\n",
            " * Acc@1 70.333 Acc@5 93.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.09it/s, loss=0.588, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 75.7610, training loss: 0.5885\n",
            " * Acc@1 73.333 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.30it/s, loss=0.473, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 80.5865, training loss: 0.4734\n",
            " * Acc@1 85.667 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.75it/s, loss=0.336, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 87.4165, training loss: 0.3360\n",
            " * Acc@1 91.000 Acc@5 99.333\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.53it/s, loss=0.233, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 91.2027, training loss: 0.2329\n",
            " * Acc@1 93.333 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.84it/s, loss=0.173, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 94.4692, training loss: 0.1735\n",
            " * Acc@1 94.333 Acc@5 99.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.38it/s, loss=0.127, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 96.3623, training loss: 0.1266\n",
            " * Acc@1 95.333 Acc@5 99.333\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.75it/s, loss=0.093, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 97.5872, training loss: 0.0932\n",
            " * Acc@1 96.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.13it/s, loss=0.073, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 98.2925, training loss: 0.0730\n",
            " * Acc@1 96.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.49it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 98.9607, training loss: 0.0581\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.35it/s, loss=0.044, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.5174, training loss: 0.0441\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.51it/s, loss=0.037, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8144, training loss: 0.0370\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.66it/s, loss=0.035, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.7402, training loss: 0.0352\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 114.54it/s, loss=0.033, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8515, training loss: 0.0330\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.58it/s, loss=0.031, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8515, training loss: 0.0313\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.53it/s, loss=0.030, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0296\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.83it/s, loss=0.028, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0283\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.17it/s, loss=0.027, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0270\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.95it/s, loss=0.026, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0262\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.95it/s, loss=0.026, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0257\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.97it/s, loss=0.024, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0243\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.53it/s, loss=0.024, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0236\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.19it/s, loss=0.023, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0232\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 112.34it/s, loss=0.023, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0235\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.95it/s, loss=0.022, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0223\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.78it/s, loss=0.021, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0208\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.27it/s, loss=0.022, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0215\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 113.84it/s, loss=0.022, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8515, training loss: 0.0223\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.35it/s, loss=0.024, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0239\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.69it/s, loss=0.025, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.7030, training loss: 0.0253\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.50it/s, loss=0.019, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0185\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.06it/s, loss=0.018, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0183\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.40it/s, loss=0.018, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0178\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.02it/s, loss=0.018, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0177\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.63it/s, loss=0.018, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0176\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.81it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0175\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.81it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0173\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.17it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0173\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.97it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0171\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 112.25it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0171\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.92it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0164\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.19it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0159\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.81it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0158\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.86it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0161\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 95.61it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0161\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.07it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0165\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 101.83it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0166\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.94it/s, loss=0.020, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8515, training loss: 0.0196\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 102.99it/s, loss=0.023, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0232\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 101.92it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0141\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 101.85it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0142\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 101.55it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0140\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.78it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0146\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.74it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0139\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.54it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0141\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.29it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0140\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.10it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0142\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.32it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0144\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.50it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0151\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.83it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0138\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.84it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0139\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.70it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0138\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.88it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0143\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.66it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0141\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.48it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0150\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.90it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0158\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.81it/s, loss=0.018, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0182\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.52it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0151\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.13it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0140\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.71it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0148\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.21it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0136\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.52it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0131\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.13it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0129\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.39it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0129\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.27it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0127\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.35it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0129\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.97it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0133\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.84it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0132\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.83it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0142\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 112.38it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0125\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.20it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0128\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.42it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0126\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.06it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0138\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.84it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0129\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.44it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0138\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.63it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0129\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.71it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0132\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.33it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0126\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.21it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0128\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.89it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0140\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.37it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 97.41it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0126\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 102.73it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.24it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0125\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 99.05it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0125\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.13it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0130\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.76it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0117\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 102.98it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 101.38it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0124\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.84it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0128\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.36it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0128\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.28it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0142\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.03it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0121\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.75it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.16it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0122\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.85it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0120\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.56it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0118\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.73it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0120\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.29it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0126\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.84it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.06it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0131\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.95it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0149\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.23it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0126\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.24it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0125\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.58it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.06it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0121\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.58it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.87it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0127\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.76it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.81it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0117\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.05it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0121\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.23it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0119\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.96it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0119\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.03it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0119\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.83it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0124\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.41it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0122\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.12it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0137\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.78it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0141\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 111.59it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0138\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.56it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0129\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.55it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0126\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.38it/s, loss=0.016, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8515, training loss: 0.0160\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.91it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0141\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.13it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0135\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.90it/s, loss=0.019, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.7030, training loss: 0.0187\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.87it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0139\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 99.42it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 98.01it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0127\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.83it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.54it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9629, training loss: 0.0131\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 99.79it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.94it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0120\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.88it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 101.80it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0118\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.62it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 94.56it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0117\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 95.31it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.15it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.93it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.45it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.81it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.36it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.91it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.26it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.36it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.54it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.17it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.08it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.28it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.60it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.20it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.37it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.34it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0120\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.37it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.28it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0115\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.76it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0117\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.75it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0121\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.26it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0123\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.57it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 106.39it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0122\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.05it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0126\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.01it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0124\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 110.44it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0127\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.45it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8515, training loss: 0.0147\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 105.48it/s, loss=0.014, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0141\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.93it/s, loss=0.017, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.8886, training loss: 0.0166\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.56it/s, loss=0.025, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.4803, training loss: 0.0250\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.19it/s, loss=0.015, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 99.9258, training loss: 0.0145\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 109.14it/s, loss=0.013, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0132\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 108.60it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.14it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.37it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.65it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0112\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 98.63it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.40it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 99.77it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 99.59it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0117\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 96.47it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 99.15it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0118\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 100.39it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0111\n",
            " * Acc@1 99.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 95.80it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0120\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 96.25it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0111\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 98.60it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0119\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 97.19it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0112\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.29it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0117\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 104.46it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0113\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 103.24it/s, loss=0.012, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0116\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 107.69it/s, loss=0.011, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0114\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "self_model.load_state_dict(torch.load('./teacher/t_best.pth')['model'])"
      ],
      "metadata": {
        "id": "z_YtoRXQlztI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fcb2ba-40aa-4a97-c148-0eb9ed1ea138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "self_model = self_model.to(cuda_device)\n",
        "\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, self_model, criterion)"
      ],
      "metadata": {
        "id": "9a_U0Md4ppIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b37068-84c8-4a38-e4f0-77094a772d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 99.364 Acc@5 99.997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(t_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VctRraUL8N3U",
        "outputId": "398012a2-e00d-4426-8e2c-14adb573841c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 99.364\n",
            "Teacher top-5 test accuracy: 99.997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2TG-QMv8VJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DDGSD**"
      ],
      "metadata": {
        "id": "sqhiETCtZdX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper with code: https://github.com/youngerous/ddgsd-pytorch\n",
        "T = 4\n",
        "lmbda = 1\n",
        "mu = 1\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "learning_rate = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers\n",
        "\n",
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "VoWg5ziXBM9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBF(nn.Module):\n",
        "\n",
        "    def __init__(self, n_kernels=5, mul_factor=2.0, bandwidth=10):\n",
        "        super().__init__()\n",
        "        self.bandwidth_multipliers = mul_factor ** (torch.arange(n_kernels) - n_kernels // 2)\n",
        "        self.bandwidth = bandwidth\n",
        "\n",
        "    def get_bandwidth(self, L2_distances):\n",
        "        if self.bandwidth is None:\n",
        "            n_samples = L2_distances.shape[0]\n",
        "            return L2_distances.data.sum() / (n_samples ** 2 - n_samples)\n",
        "\n",
        "        return self.bandwidth\n",
        "\n",
        "    def forward(self, X):\n",
        "        L2_distances = torch.cdist(X, X) ** 2\n",
        "        L2_distances = L2_distances.cpu()\n",
        "        return torch.exp(-L2_distances[None, ...] / (self.get_bandwidth(L2_distances) * self.bandwidth_multipliers)[:, None, None]).sum(dim=0)\n",
        "\n",
        "\n",
        "class MMDLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel=RBF()):\n",
        "        super().__init__()\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        K = self.kernel(torch.vstack([X, Y]))\n",
        "\n",
        "        X_size = X.shape[0]\n",
        "        XX = K[:X_size, :X_size].mean()\n",
        "        XY = K[:X_size, X_size:].mean()\n",
        "        YY = K[X_size:, X_size:].mean()\n",
        "        return XX - 2 * XY + YY"
      ],
      "metadata": {
        "id": "9BKlK7NjeOye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[i] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(d) for d in self.datasets)"
      ],
      "metadata": {
        "id": "8CDVfqm6Zvvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temp: float):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.T = temp\n",
        "\n",
        "    def forward(self, out1, out2):\n",
        "        loss = F.kl_div(\n",
        "            F.log_softmax(out1 / self.T, dim=1),\n",
        "            F.softmax(out2 / self.T, dim=1),\n",
        "            reduction=\"mean\",\n",
        "        )\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ubsQm1-N0gnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform_flip = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor()\n",
        "            ]\n",
        "        )\n",
        "train_transform_crop = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(19, padding=4),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "2htPYKYxZv8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "X_train_flip = torch.zeros(X_train.shape)\n",
        "#set a seed so the same transforms are applied to each channel\n",
        "for i in range(len(X_train)):\n",
        "    x = []\n",
        "    seed = np.random.randint(2023)\n",
        "    for ch in X_train[i]:\n",
        "        random.seed(seed)\n",
        "        x.append(train_transform_flip(Image.fromarray(ch)))\n",
        "\n",
        "    #this is the multichannel transformed image (a torch tensor)\n",
        "    img_tfm = torch.cat(x)\n",
        "    X_train_flip[i] = img_tfm"
      ],
      "metadata": {
        "id": "DcF-wI0UZv_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_crop = torch.zeros(X_train.shape)\n",
        "#set a seed so the same transforms are applied to each channel\n",
        "for i in range(len(X_train)):\n",
        "    x = []\n",
        "    seed = np.random.randint(2023)\n",
        "    for ch in X_train[i]:\n",
        "        random.seed(seed)\n",
        "        x.append(train_transform_crop(Image.fromarray(ch)))\n",
        "\n",
        "    #this is the multichannel transformed image (a torch tensor)\n",
        "    img_tfm = torch.cat(x)\n",
        "    X_train_crop[i] = img_tfm"
      ],
      "metadata": {
        "id": "bFPykWDolEzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_y = torch.Tensor(y_train)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "train_dataset_flip = TensorDataset(X_train_flip,tensor_y) # create your datset\n",
        "train_dataset_crop = TensorDataset(X_train_crop,tensor_y) # create your datset\n",
        "train_dataloader = DataLoader(ConcatDataset(train_dataset_flip, train_dataset_crop), batch_size=batch_size) # create your dataloader"
      ],
      "metadata": {
        "id": "m84kCvRBlE14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining train_kd functions\n",
        "def train_ddgsd(model, optimizer, dataloader, epoch, ce_loss, kd_loss, mmd_loss):\n",
        "    \"\"\"\n",
        "    KD Train the model on `num_steps` batches\n",
        "    \"\"\"\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "    # teacher_model.eval()\n",
        "    loss_avg = RunningAverage()\n",
        "    losses = AverageMeter()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            batch_flip, batch_crop = batch\n",
        "            img_a, label = map(lambda x: x.to(cuda_device), batch_flip)\n",
        "            img_b, same_label = map(lambda x: x.to(cuda_device), batch_crop)\n",
        "            assert torch.equal(label, same_label), \"label not mathing\"\n",
        "\n",
        "            logit_a, logit_b = model(img_a), model(img_b)\n",
        "            ce_loss_ = ce_loss(logit_a, label) + ce_loss(logit_b, label)\n",
        "            kd_loss_ = kd_loss(logit_a, logit_b) + kd_loss(logit_b, logit_a)\n",
        "            mmd_loss_ = mmd_loss(logit_a, logit_b)\n",
        "\n",
        "            loss = (ce_loss_+ lmbda * kd_loss_ + mu * mmd_loss_).mean()\n",
        "\n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            optimizer.zero_grad()                \n",
        "            loss.backward()\n",
        "\n",
        "            # performs updates using calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_avg.update(loss.data)\n",
        "            losses.update(loss.item(), logit_a.size(0))\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg.value()), lr='{:05.6f}'.format(optimizer.param_groups[0]['lr']))\n",
        "            t.update()\n",
        "\n",
        "    acc = 100\n",
        "    print(\"- Train accuracy: {acc:.4f}, training loss: {loss:.4f}\".format(acc = acc, loss = losses.avg))\n",
        "    return acc, losses.avg\n"
      ],
      "metadata": {
        "id": "meYNYWeplE4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_distillation(train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    kd_loss = DistillationLoss(temp=T)\n",
        "    #mmd_loss = nn.MSELoss()\n",
        "    mmd_loss = MMDLoss()\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_ddgsd(model, optimizer, train_loader, epoch, ce_loss, kd_loss, mmd_loss)\n",
        "  \n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './teacher/t_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./teacher/', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './teacher/t_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "EVpwdHSC0Ww1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_distillation(train_dataloader, val_dataloader, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN4XmJxz0Wza",
        "outputId": "39e52973-7a84-41f0-e12c-0aa00754ec3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 39.99it/s, loss=3.503, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 3.5053\n",
            "epoch 1, total time 0.68\n",
            " * Acc@1 40.667 Acc@5 85.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 64.38it/s, loss=2.681, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 2.6818\n",
            "epoch 2, total time 0.42\n",
            " * Acc@1 43.667 Acc@5 85.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 59.99it/s, loss=2.143, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 2.1444\n",
            "epoch 3, total time 0.46\n",
            " * Acc@1 66.000 Acc@5 89.667\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 56.57it/s, loss=1.471, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 1.4720\n",
            "epoch 4, total time 0.48\n",
            " * Acc@1 72.333 Acc@5 97.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.80it/s, loss=1.136, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 1.1359\n",
            "epoch 5, total time 0.52\n",
            " * Acc@1 80.000 Acc@5 98.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.70it/s, loss=0.902, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.9019\n",
            "epoch 6, total time 0.58\n",
            " * Acc@1 87.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 44.02it/s, loss=0.687, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.6871\n",
            "epoch 7, total time 0.62\n",
            " * Acc@1 90.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 44.07it/s, loss=0.529, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.5295\n",
            "epoch 8, total time 0.62\n",
            " * Acc@1 94.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 44.46it/s, loss=0.427, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.4275\n",
            "epoch 9, total time 0.61\n",
            " * Acc@1 95.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.28it/s, loss=0.354, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.3541\n",
            "epoch 10, total time 0.59\n",
            " * Acc@1 96.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.32it/s, loss=0.305, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.3051\n",
            "epoch 11, total time 0.55\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.22it/s, loss=0.270, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.2696\n",
            "epoch 12, total time 0.56\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.80it/s, loss=0.239, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.2392\n",
            "epoch 13, total time 0.55\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.70it/s, loss=0.216, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.2157\n",
            "epoch 14, total time 0.54\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.86it/s, loss=0.196, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1957\n",
            "epoch 15, total time 0.55\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.18it/s, loss=0.180, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1797\n",
            "epoch 16, total time 0.55\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.18it/s, loss=0.165, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1651\n",
            "epoch 17, total time 0.53\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.46it/s, loss=0.155, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1551\n",
            "epoch 18, total time 0.53\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.32it/s, loss=0.147, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1470\n",
            "epoch 19, total time 0.53\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.88it/s, loss=0.140, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1398\n",
            "epoch 20, total time 0.54\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.55it/s, loss=0.134, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1340\n",
            "epoch 21, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.42it/s, loss=0.129, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1291\n",
            "epoch 22, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.03it/s, loss=0.126, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1256\n",
            "epoch 23, total time 0.55\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.11it/s, loss=0.122, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1220\n",
            "epoch 24, total time 0.55\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.74it/s, loss=0.121, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1207\n",
            "epoch 25, total time 0.53\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.09it/s, loss=0.118, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1180\n",
            "epoch 26, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.69it/s, loss=0.116, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1157\n",
            "epoch 27, total time 0.54\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.66it/s, loss=0.112, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1119\n",
            "epoch 28, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.65it/s, loss=0.112, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1120\n",
            "epoch 29, total time 0.59\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.88it/s, loss=0.109, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1091\n",
            "epoch 30, total time 0.59\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.93it/s, loss=0.110, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1095\n",
            "epoch 31, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.35it/s, loss=0.106, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1057\n",
            "epoch 32, total time 0.59\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.81it/s, loss=0.108, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1077\n",
            "epoch 33, total time 0.56\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.38it/s, loss=0.107, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1067\n",
            "epoch 34, total time 0.55\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.62it/s, loss=0.107, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1069\n",
            "epoch 35, total time 0.54\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.95it/s, loss=0.106, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1062\n",
            "epoch 36, total time 0.54\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.63it/s, loss=0.103, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1034\n",
            "epoch 37, total time 0.53\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 39.84it/s, loss=0.105, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1047\n",
            "epoch 38, total time 0.68\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.35it/s, loss=0.106, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1061\n",
            "epoch 39, total time 0.53\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.60it/s, loss=0.103, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1026\n",
            "epoch 40, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.59it/s, loss=0.103, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1028\n",
            "epoch 41, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.09it/s, loss=0.103, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1029\n",
            "epoch 42, total time 0.54\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.95it/s, loss=0.102, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1016\n",
            "epoch 43, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.70it/s, loss=0.106, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1059\n",
            "epoch 44, total time 0.53\n",
            " * Acc@1 98.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.13it/s, loss=0.107, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1069\n",
            "epoch 45, total time 0.53\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.71it/s, loss=0.107, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1069\n",
            "epoch 46, total time 0.53\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.14it/s, loss=0.113, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1131\n",
            "epoch 47, total time 0.53\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.78it/s, loss=0.110, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1103\n",
            "epoch 48, total time 0.54\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.59it/s, loss=0.098, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0981\n",
            "epoch 49, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.54it/s, loss=0.095, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0952\n",
            "epoch 50, total time 0.56\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.83it/s, loss=0.088, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0880\n",
            "epoch 51, total time 0.60\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 44.73it/s, loss=0.087, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0865\n",
            "epoch 52, total time 0.61\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.92it/s, loss=0.087, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0869\n",
            "epoch 53, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.38it/s, loss=0.085, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0850\n",
            "epoch 54, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.43it/s, loss=0.085, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0850\n",
            "epoch 55, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.63it/s, loss=0.084, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0842\n",
            "epoch 56, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.91it/s, loss=0.083, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0827\n",
            "epoch 57, total time 0.52\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.13it/s, loss=0.082, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0821\n",
            "epoch 58, total time 0.56\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.04it/s, loss=0.082, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0819\n",
            "epoch 59, total time 0.56\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.19it/s, loss=0.081, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0806\n",
            "epoch 60, total time 0.55\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.88it/s, loss=0.081, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0805\n",
            "epoch 61, total time 0.55\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.26it/s, loss=0.080, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0798\n",
            "epoch 62, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.84it/s, loss=0.080, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0800\n",
            "epoch 63, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.17it/s, loss=0.079, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0791\n",
            "epoch 64, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.35it/s, loss=0.079, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0789\n",
            "epoch 65, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.20it/s, loss=0.078, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0781\n",
            "epoch 66, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.11it/s, loss=0.078, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0775\n",
            "epoch 67, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.36it/s, loss=0.078, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0775\n",
            "epoch 68, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.73it/s, loss=0.076, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0757\n",
            "epoch 69, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.52it/s, loss=0.076, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0762\n",
            "epoch 70, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.26it/s, loss=0.075, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0749\n",
            "epoch 71, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.89it/s, loss=0.075, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0750\n",
            "epoch 72, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.48it/s, loss=0.075, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0749\n",
            "epoch 73, total time 0.57\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.48it/s, loss=0.075, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0745\n",
            "epoch 74, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.42it/s, loss=0.074, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0735\n",
            "epoch 75, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.24it/s, loss=0.073, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0734\n",
            "epoch 76, total time 0.59\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.86it/s, loss=0.074, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0739\n",
            "epoch 77, total time 0.56\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.42it/s, loss=0.077, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0772\n",
            "epoch 78, total time 0.53\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.13it/s, loss=0.077, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0772\n",
            "epoch 79, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.64it/s, loss=0.081, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0812\n",
            "epoch 80, total time 0.52\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 41.06it/s, loss=0.078, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0776\n",
            "epoch 81, total time 0.66\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.01it/s, loss=0.085, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0848\n",
            "epoch 82, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.02it/s, loss=0.104, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1035\n",
            "epoch 83, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.54it/s, loss=0.087, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0873\n",
            "epoch 84, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.49it/s, loss=0.074, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0744\n",
            "epoch 85, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.83it/s, loss=0.069, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0693\n",
            "epoch 86, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.83it/s, loss=0.070, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0702\n",
            "epoch 87, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.30it/s, loss=0.070, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0695\n",
            "epoch 88, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.47it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0676\n",
            "epoch 89, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.17it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0682\n",
            "epoch 90, total time 0.54\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.97it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0678\n",
            "epoch 91, total time 0.53\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.74it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0684\n",
            "epoch 92, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.93it/s, loss=0.067, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0675\n",
            "epoch 93, total time 0.51\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.53it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0680\n",
            "epoch 94, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.92it/s, loss=0.066, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0664\n",
            "epoch 95, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.99it/s, loss=0.070, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0695\n",
            "epoch 96, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.88it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0684\n",
            "epoch 97, total time 0.57\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.95it/s, loss=0.069, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0688\n",
            "epoch 98, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.23it/s, loss=0.067, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0672\n",
            "epoch 99, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.75it/s, loss=0.069, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0690\n",
            "epoch 100, total time 0.55\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.98it/s, loss=0.072, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0722\n",
            "epoch 101, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.11it/s, loss=0.083, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0834\n",
            "epoch 102, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.67it/s, loss=0.074, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0744\n",
            "epoch 103, total time 0.53\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.24it/s, loss=0.072, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0723\n",
            "epoch 104, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.22it/s, loss=0.068, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0676\n",
            "epoch 105, total time 0.53\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.63it/s, loss=0.070, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0702\n",
            "epoch 106, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.47it/s, loss=0.066, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0659\n",
            "epoch 107, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 53.35it/s, loss=0.070, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0696\n",
            "epoch 108, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.41it/s, loss=0.065, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0652\n",
            "epoch 109, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 53.04it/s, loss=0.069, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0689\n",
            "epoch 110, total time 0.51\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.75it/s, loss=0.070, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0695\n",
            "epoch 111, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.43it/s, loss=0.075, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0748\n",
            "epoch 112, total time 0.53\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.66it/s, loss=0.079, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0789\n",
            "epoch 113, total time 0.54\n",
            " * Acc@1 98.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.71it/s, loss=0.092, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0921\n",
            "epoch 114, total time 0.54\n",
            " * Acc@1 97.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.57it/s, loss=0.102, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1021\n",
            "epoch 115, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.94it/s, loss=0.073, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0728\n",
            "epoch 116, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.79it/s, loss=0.071, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0714\n",
            "epoch 117, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.65it/s, loss=0.065, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0648\n",
            "epoch 118, total time 0.56\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.57it/s, loss=0.065, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0651\n",
            "epoch 119, total time 0.57\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.40it/s, loss=0.064, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0644\n",
            "epoch 120, total time 0.60\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.00it/s, loss=0.065, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0647\n",
            "epoch 121, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.42it/s, loss=0.065, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0646\n",
            "epoch 122, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.62it/s, loss=0.064, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0645\n",
            "epoch 123, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 40.86it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0629\n",
            "epoch 124, total time 0.67\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.45it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0619\n",
            "epoch 125, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.40it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0622\n",
            "epoch 126, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.82it/s, loss=0.061, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0609\n",
            "epoch 127, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.43it/s, loss=0.061, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0615\n",
            "epoch 128, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.27it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0618\n",
            "epoch 129, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.69it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0620\n",
            "epoch 130, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.96it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0616\n",
            "epoch 131, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.37it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0625\n",
            "epoch 132, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.56it/s, loss=0.061, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0615\n",
            "epoch 133, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.77it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0618\n",
            "epoch 134, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.40it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0603\n",
            "epoch 135, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.51it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0631\n",
            "epoch 136, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.67it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0619\n",
            "epoch 137, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.64it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0619\n",
            "epoch 138, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.17it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0624\n",
            "epoch 139, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 53.37it/s, loss=0.066, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0658\n",
            "epoch 140, total time 0.51\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.27it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0629\n",
            "epoch 141, total time 0.57\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.17it/s, loss=0.061, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0612\n",
            "epoch 142, total time 0.57\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.67it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0605\n",
            "epoch 143, total time 0.59\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.07it/s, loss=0.067, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0665\n",
            "epoch 144, total time 0.58\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.01it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0632\n",
            "epoch 145, total time 0.61\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.02it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0615\n",
            "epoch 146, total time 0.56\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.04it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0621\n",
            "epoch 147, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.53it/s, loss=0.074, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0736\n",
            "epoch 148, total time 0.52\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.02it/s, loss=0.069, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0684\n",
            "epoch 149, total time 0.52\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.76it/s, loss=0.069, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0693\n",
            "epoch 150, total time 0.52\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.70it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0626\n",
            "epoch 151, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.81it/s, loss=0.064, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0639\n",
            "epoch 152, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.52it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0591\n",
            "epoch 153, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.12it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0624\n",
            "epoch 154, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.15it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0593\n",
            "epoch 155, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.58it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0617\n",
            "epoch 156, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.23it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0588\n",
            "epoch 157, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.90it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0605\n",
            "epoch 158, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.01it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0586\n",
            "epoch 159, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.33it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0592\n",
            "epoch 160, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.92it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0603\n",
            "epoch 161, total time 0.54\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.62it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0603\n",
            "epoch 162, total time 0.53\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.60it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0634\n",
            "epoch 163, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.59it/s, loss=0.063, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0629\n",
            "epoch 164, total time 0.57\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.98it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0589\n",
            "epoch 165, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.24it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0588\n",
            "epoch 166, total time 0.60\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 38.43it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0584\n",
            "epoch 167, total time 0.71\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.13it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0585\n",
            "epoch 168, total time 0.57\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.61it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0577\n",
            "epoch 169, total time 0.53\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.88it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0591\n",
            "epoch 170, total time 0.52\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.79it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0588\n",
            "epoch 171, total time 0.52\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.05it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0585\n",
            "epoch 172, total time 0.54\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.49it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0577\n",
            "epoch 173, total time 0.52\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.80it/s, loss=0.057, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0568\n",
            "epoch 174, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.75it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0581\n",
            "epoch 175, total time 0.53\n",
            " * Acc@1 99.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.35it/s, loss=0.057, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0569\n",
            "epoch 176, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.28it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0580\n",
            "epoch 177, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.72it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0588\n",
            "epoch 178, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.00it/s, loss=0.057, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0569\n",
            "epoch 179, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.33it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0575\n",
            "epoch 180, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 53.33it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0586\n",
            "epoch 181, total time 0.51\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 31.84it/s, loss=0.059, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0586\n",
            "epoch 182, total time 0.86\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 48.14it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0598\n",
            "epoch 183, total time 0.58\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 39.96it/s, loss=0.060, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0596\n",
            "epoch 184, total time 0.68\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 35.50it/s, loss=0.057, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0572\n",
            "epoch 185, total time 0.77\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 45.88it/s, loss=0.062, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0616\n",
            "epoch 186, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.17it/s, loss=0.080, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0800\n",
            "epoch 187, total time 0.56\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 47.42it/s, loss=0.123, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1229\n",
            "epoch 188, total time 0.58\n",
            " * Acc@1 97.333 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.79it/s, loss=0.142, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.1424\n",
            "epoch 189, total time 0.58\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 46.49it/s, loss=0.082, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0818\n",
            "epoch 190, total time 0.59\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 49.63it/s, loss=0.065, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0651\n",
            "epoch 191, total time 0.55\n",
            " * Acc@1 98.667 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.12it/s, loss=0.057, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0570\n",
            "epoch 192, total time 0.54\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 50.23it/s, loss=0.058, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0581\n",
            "epoch 193, total time 0.55\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.74it/s, loss=0.056, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0562\n",
            "epoch 194, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.22it/s, loss=0.056, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0560\n",
            "epoch 195, total time 0.52\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.95it/s, loss=0.055, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0554\n",
            "epoch 196, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.69it/s, loss=0.056, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0558\n",
            "epoch 197, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.63it/s, loss=0.055, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0554\n",
            "epoch 198, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 52.08it/s, loss=0.056, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0561\n",
            "epoch 199, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 51.53it/s, loss=0.056, lr=0.000800]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train accuracy: 100.0000, training loss: 0.0559\n",
            "epoch 200, total time 0.53\n",
            " * Acc@1 99.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.3333, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_model = cnn2d(num_components, num_classes, cfg=s_cfg, fc=s_fc)\n",
        "self_model.load_state_dict(torch.load('./teacher/t_best.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsipyaD10ala",
        "outputId": "91ecbdbe-7227-4d4c-e2c0-57b61440ed17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "self_model = self_model.to(cuda_device)\n",
        "\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, self_model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPmvDcr80aoG",
        "outputId": "dbad483c-cf02-43c6-c3fd-2ddffdcaee49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 99.387 Acc@5 100.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(t_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys4AYEHn0aqk",
        "outputId": "547d7e29-eff5-4878-cba0-14f0c62cd9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 99.387\n",
            "Teacher top-5 test accuracy: 100.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHVQNYZ0C2Fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}