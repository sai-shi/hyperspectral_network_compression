{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "WGdX7wy8CYsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard_logger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k6-AtbeQoIz",
        "outputId": "1ace7a15-a7f0-410a-9e42-b5ccde17bd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboard_logger\n",
            "  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.10.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (8.4.0)\n",
            "Installing collected packages: tensorboard_logger\n",
            "Successfully installed tensorboard_logger-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorboard_logger as tb_logger\n",
        "import torch.backends.cudnn as cudnn\n",
        "import sys\n",
        "import time"
      ],
      "metadata": {
        "id": "N9lU9RlgN1Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")"
      ],
      "metadata": {
        "id": "Yy2YtOdhxgBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ['India', 'Pavia'][1]\n",
        "random_split = False\n",
        "use_cnn_1d = False"
      ],
      "metadata": {
        "id": "VG3LHkQIkVUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == 'India':\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/indian_pines_corrected.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indianpines_disjoint_dset.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indian_pines_gt.mat')\n",
        "\n",
        "    X = raw_data['indian_pines_corrected']\n",
        "    y_disjoint = disjoint_data['indianpines_disjoint_dset']\n",
        "    y_all = all_labels['indian_pines_gt']\n",
        "\n",
        "    test_ratio = 0.45\n",
        "else:\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/paviaU.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/TRpavia_fixed.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/paviaU_gt.mat')\n",
        "\n",
        "    X = raw_data['paviaU']\n",
        "    y_disjoint = disjoint_data['TRpavia_fixed']\n",
        "    y_all = all_labels['paviaU_gt']\n",
        "\n",
        "    test_ratio = 0.93"
      ],
      "metadata": {
        "id": "dIOOzXj1Rje4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the grid\n",
        "plt.imshow(y_all, cmap='bwr')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "wqZnaNKnSHO7",
        "outputId": "c94f8e09-1138-40a7-abbf-4c6bf194ef1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the grid\n",
        "plt.imshow(y_disjoint, cmap='bwr')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "ywD14p3oSJc-",
        "outputId": "6889e796-1761-464e-9868-ced68883ed5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Uci233aici8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "_nYjGrFZci-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quGUfFsw5HoJ",
        "outputId": "bfb66439-df07-48bd-bd69-c7f07db3bc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = X.shape[-1]\n",
        "W = X.shape[0]\n",
        "H = X.shape[1]"
      ],
      "metadata": {
        "id": "Iq60eCjE3uN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape(W*H, num_channels)\n",
        "y_all = y_all.reshape(-1, 1).flatten()\n",
        "y_disjoint = y_disjoint.reshape(-1, 1).flatten()\n",
        "\n",
        "X = X.astype(np.float32)\n",
        "y = y_all.astype(np.int16)"
      ],
      "metadata": {
        "id": "pfy8EN34aYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYGzgOymjYHE",
        "outputId": "5d5d8551-9ff7-41d8-be1c-c33a1a858ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(207400, 103)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if random_split:\n",
        "    nonzero_idx = np.where(y != 0)\n",
        "    X = X[nonzero_idx]\n",
        "    y = y[nonzero_idx]\n",
        "    y = y - 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
        "else:\n",
        "    train_idx = np.where((y_disjoint != 0) & (y_all != 0))\n",
        "    X_train = X[train_idx]\n",
        "    y_train = y[train_idx]\n",
        "\n",
        "    test_idx = np.where((y_disjoint == 0) & (y_all != 0))\n",
        "    X_test = X[test_idx]\n",
        "    y_test = y[test_idx]\n",
        "\n",
        "    y_train -= 1\n",
        "    y_test -= 1\n",
        "\n",
        "    print('Disjoint')"
      ],
      "metadata": {
        "id": "mLayt2rYcVVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0632648-b101-4d0e-beca-c1d5f8bec0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disjoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "RvPU3Bc6zvya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=200, min_samples_split=3, max_features=10, max_depth=10)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRrz29cdxmIR",
        "outputId": "80948d85-1bf3-4197-cfa5-56a48f99db4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6573975801316069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(C=100, gamma=0.125, tol=1e-7)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmwl9lrDxmNt",
        "outputId": "46701ded-038e-4c20-99d0-ab2b00eff091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7976351182440878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(penalty='l2', dual=False, tol=1e-20, C=1, \\\n",
        "                        solver='lbfgs', multi_class='multinomial', max_iter=5000,\\\n",
        "                        fit_intercept=True, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH1i5X1bxmRH",
        "outputId": "652805d5-0534-4f2b-deed-7080ddeb032c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7463126843657817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Researve 10% validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "HXY841qwCwEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_cnn_1d:\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_val = X_val[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    X_train = np.transpose(X_train, (0, 2, 1))\n",
        "    X_val = np.transpose(X_val, (0, 2, 1))\n",
        "    X_test = np.transpose(X_test, (0, 2, 1))"
      ],
      "metadata": {
        "id": "2OauRISfsjNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape, y_test.shape)\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmLLjlmz525w",
        "outputId": "40553a8d-9c77-45e2-b374-21e692f9d9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2496,) (40002,)\n",
            "(2496, 103) (40002, 103)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_train) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_train)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100) # create your dataloader"
      ],
      "metadata": {
        "id": "JY6DCjbUcWIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_val) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_val)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "val_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100) # create your dataloader"
      ],
      "metadata": {
        "id": "NvNX80DayaKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_test)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100) # create your dataloader"
      ],
      "metadata": {
        "id": "hm46r6mzcWKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_freq = 40\n",
        "print_freq = 100\n",
        "\n",
        "learning_rate = 1e-3\n",
        "num_classes = len(np.unique(y_train))"
      ],
      "metadata": {
        "id": "Ro4nA_gLMJD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fnn(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline network\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm1d):\n",
        "            m.weight.data.fill_(0.5)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "    def __init__(self, input_channels, n_classes):\n",
        "        super(fnn, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_channels, 143)\n",
        "        self.fc2 = nn.Linear(143, n_classes)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EOBdgausc6Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class recurrent(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline network\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            init.kaiming_normal_(m.weight)\n",
        "            init.zeros_(m.bias)\n",
        "\n",
        "    def __init__(self, input_channels, n_classes):\n",
        "        super(recurrent, self).__init__()\n",
        "\n",
        "        self.rnn1 = nn.LSTM(input_channels, 64, 1, batch_first=True)\n",
        "        self.rnn2 = nn.LSTM(64, 64, 1, batch_first=True)\n",
        "        self.fc = nn.Linear(64, n_classes)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, states = self.rnn1(x)\n",
        "        x, states = self.rnn2(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lyIEkvts121u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn1d(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN1d network\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Conv1d):\n",
        "                n = m.kernel_size[0] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm1d):\n",
        "            m.weight.data.fill_(0.5)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "    def __init__(self, input_channels, n_classes):\n",
        "        super(cnn1d, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels=1, out_channels=20, kernel_size=24, stride=1)\n",
        "        self.pool = nn.MaxPool1d(5)\n",
        "\n",
        "        self.fc1 = nn.Linear(700, 100)\n",
        "        self.batch_norm = nn.BatchNorm1d(100)\n",
        "        self.fc2 = nn.Linear(100, n_classes)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.batch_norm(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AzRgleCR8woG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "KYFNvIVZLzVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "0-QhJ2RtL0JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"validation\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input = input.float()\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if idx % print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                      'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                       idx, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                       top1=top1, top5=top5))\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "W3ESwgTuL0My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vanilla(epoch, train_loader, model, criterion, optimizer):\n",
        "    \"\"\"vanilla training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.float()\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # ===================forward=====================\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "\n",
        "        # ===================backward=====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ===================meters=====================\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # tensorboard logger\n",
        "        pass\n",
        "\n",
        "        # print info\n",
        "        if idx % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                  'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                   epoch, idx, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, losses.avg"
      ],
      "metadata": {
        "id": "KAhxXrxVL3AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    if use_cnn_1d:\n",
        "        model = cnn1d(num_channels, num_classes).to(device)\n",
        "    else:\n",
        "        model = fnn(num_channels, num_classes).to(device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=\"./model_tensorboard\", flush_secs=2)\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        logger.log_value('train_acc', train_acc, epoch)\n",
        "        logger.log_value('train_loss', train_loss, epoch)\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "        logger.log_value('test_acc', test_acc, epoch)\n",
        "        logger.log_value('test_acc_top5', test_acc_top5, epoch)\n",
        "        logger.log_value('test_loss', test_loss, epoch)\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './model_best.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './model_last.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "98V1EjMWL3Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model, label=\"\"):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size=os.path.getsize(\"temp.p\")\n",
        "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
        "    os.remove('temp.p')\n",
        "    return size"
      ],
      "metadata": {
        "id": "oQSOCvpixA7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 40, 19, 19),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "BLqn0ELxxEDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_dataloader, val_dataloader, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu_lSZ26OiIB",
        "outputId": "9cc08d6e-8a2a-463d-cc77-72495d85af85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            "Epoch: [1][0/25]\tTime 0.008 (0.008)\tData 0.002 (0.002)\tLoss 2.2021 (2.2021)\tAcc@1 13.000 (13.000)\tAcc@5 44.000 (44.000)\n",
            " * Acc@1 33.894 Acc@5 91.587\n",
            "epoch 1, total time 0.09\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 1.4149 (1.4149)\tAcc@1 38.000 (38.000)\tAcc@5 98.000 (98.000)\n",
            " * Acc@1 42.446 Acc@5 97.842\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [2][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.2596 (1.2596)\tAcc@1 51.000 (51.000)\tAcc@5 97.000 (97.000)\n",
            " * Acc@1 58.814 Acc@5 99.239\n",
            "epoch 2, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.9073 (0.9073)\tAcc@1 70.000 (70.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 66.187 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [3][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.9040 (0.9040)\tAcc@1 67.000 (67.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 66.426 Acc@5 99.760\n",
            "epoch 3, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.7293 (0.7293)\tAcc@1 66.000 (66.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 63.669 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [4][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.7626 (0.7626)\tAcc@1 70.000 (70.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 67.628 Acc@5 99.840\n",
            "epoch 4, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.6776 (0.6776)\tAcc@1 67.000 (67.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 64.388 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [5][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.7005 (0.7005)\tAcc@1 70.000 (70.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 69.030 Acc@5 99.920\n",
            "epoch 5, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.6479 (0.6479)\tAcc@1 65.000 (65.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 63.309 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [6][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.6602 (0.6602)\tAcc@1 71.000 (71.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 69.712 Acc@5 99.920\n",
            "epoch 6, total time 0.07\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.6288 (0.6288)\tAcc@1 67.000 (67.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 65.108 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [7][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.6316 (0.6316)\tAcc@1 72.000 (72.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 70.112 Acc@5 99.960\n",
            "epoch 7, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.6144 (0.6144)\tAcc@1 68.000 (68.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 66.547 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [8][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.6092 (0.6092)\tAcc@1 72.000 (72.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 71.314 Acc@5 100.000\n",
            "epoch 8, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.6032 (0.6032)\tAcc@1 69.000 (69.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 67.626 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [9][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.5929 (0.5929)\tAcc@1 72.000 (72.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 72.436 Acc@5 100.000\n",
            "epoch 9, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5931 (0.5931)\tAcc@1 71.000 (71.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 68.345 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [10][0/25]\tTime 0.006 (0.006)\tData 0.002 (0.002)\tLoss 0.5786 (0.5786)\tAcc@1 73.000 (73.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 73.157 Acc@5 100.000\n",
            "epoch 10, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5833 (0.5833)\tAcc@1 72.000 (72.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 69.424 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [11][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.5657 (0.5657)\tAcc@1 73.000 (73.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 74.199 Acc@5 100.000\n",
            "epoch 11, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5718 (0.5718)\tAcc@1 73.000 (73.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 70.863 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [12][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.5536 (0.5536)\tAcc@1 74.000 (74.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 75.441 Acc@5 100.000\n",
            "epoch 12, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5608 (0.5608)\tAcc@1 73.000 (73.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 71.942 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [13][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.5414 (0.5414)\tAcc@1 74.000 (74.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 76.362 Acc@5 100.000\n",
            "epoch 13, total time 0.09\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5493 (0.5493)\tAcc@1 76.000 (76.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 73.381 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [14][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.5296 (0.5296)\tAcc@1 76.000 (76.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 77.804 Acc@5 100.000\n",
            "epoch 14, total time 0.08\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.5385 (0.5385)\tAcc@1 76.000 (76.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 75.540 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [15][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.5184 (0.5184)\tAcc@1 77.000 (77.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 78.846 Acc@5 100.000\n",
            "epoch 15, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5284 (0.5284)\tAcc@1 75.000 (75.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 75.899 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [16][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.5070 (0.5070)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 79.527 Acc@5 100.000\n",
            "epoch 16, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5172 (0.5172)\tAcc@1 77.000 (77.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 77.338 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [17][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.4965 (0.4965)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 80.649 Acc@5 100.000\n",
            "epoch 17, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.5067 (0.5067)\tAcc@1 77.000 (77.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 78.417 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [18][0/25]\tTime 0.005 (0.005)\tData 0.003 (0.003)\tLoss 0.4856 (0.4856)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 81.370 Acc@5 100.000\n",
            "epoch 18, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4964 (0.4964)\tAcc@1 77.000 (77.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 78.417 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [19][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.4745 (0.4745)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 81.811 Acc@5 100.000\n",
            "epoch 19, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.4874 (0.4874)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 79.137 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [20][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.4644 (0.4644)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 82.292 Acc@5 100.000\n",
            "epoch 20, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4793 (0.4793)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 79.137 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [21][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.4552 (0.4552)\tAcc@1 79.000 (79.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 82.812 Acc@5 100.000\n",
            "epoch 21, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4717 (0.4717)\tAcc@1 78.000 (78.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 79.496 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [22][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.4462 (0.4462)\tAcc@1 79.000 (79.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 83.253 Acc@5 100.000\n",
            "epoch 22, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4651 (0.4651)\tAcc@1 79.000 (79.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 79.856 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [23][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.4378 (0.4378)\tAcc@1 79.000 (79.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 83.814 Acc@5 100.000\n",
            "epoch 23, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4580 (0.4580)\tAcc@1 79.000 (79.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 80.216 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [24][0/25]\tTime 0.006 (0.006)\tData 0.004 (0.004)\tLoss 0.4301 (0.4301)\tAcc@1 80.000 (80.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.175 Acc@5 100.000\n",
            "epoch 24, total time 0.08\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4513 (0.4513)\tAcc@1 79.000 (79.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 80.576 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [25][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.4222 (0.4222)\tAcc@1 80.000 (80.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.415 Acc@5 100.000\n",
            "epoch 25, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4459 (0.4459)\tAcc@1 81.000 (81.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 82.374 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [26][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.4151 (0.4151)\tAcc@1 81.000 (81.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.495 Acc@5 100.000\n",
            "epoch 26, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4401 (0.4401)\tAcc@1 83.000 (83.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 83.813 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [27][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.4080 (0.4080)\tAcc@1 81.000 (81.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.936 Acc@5 100.000\n",
            "epoch 27, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4333 (0.4333)\tAcc@1 83.000 (83.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 83.813 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [28][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.4014 (0.4014)\tAcc@1 81.000 (81.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.296 Acc@5 100.000\n",
            "epoch 28, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4286 (0.4286)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.892 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [29][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3947 (0.3947)\tAcc@1 81.000 (81.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.697 Acc@5 100.000\n",
            "epoch 29, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4223 (0.4223)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.892 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [30][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3885 (0.3885)\tAcc@1 83.000 (83.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.138 Acc@5 100.000\n",
            "epoch 30, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.4173 (0.4173)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.532 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [31][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3823 (0.3823)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.338 Acc@5 100.000\n",
            "epoch 31, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.4112 (0.4112)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 84.892 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [32][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3770 (0.3770)\tAcc@1 83.000 (83.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.378 Acc@5 100.000\n",
            "epoch 32, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.4072 (0.4072)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.612 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [33][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.3715 (0.3715)\tAcc@1 85.000 (85.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.739 Acc@5 100.000\n",
            "epoch 33, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.4008 (0.4008)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.971 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [34][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.3661 (0.3661)\tAcc@1 85.000 (85.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.979 Acc@5 100.000\n",
            "epoch 34, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3954 (0.3954)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.971 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [35][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3616 (0.3616)\tAcc@1 85.000 (85.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 87.340 Acc@5 100.000\n",
            "epoch 35, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3912 (0.3912)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.971 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [36][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3568 (0.3568)\tAcc@1 85.000 (85.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 87.500 Acc@5 100.000\n",
            "epoch 36, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3860 (0.3860)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.331 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [37][0/25]\tTime 0.008 (0.008)\tData 0.001 (0.001)\tLoss 0.3517 (0.3517)\tAcc@1 86.000 (86.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 87.620 Acc@5 100.000\n",
            "epoch 37, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.3809 (0.3809)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.331 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [38][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3468 (0.3468)\tAcc@1 86.000 (86.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 87.780 Acc@5 100.000\n",
            "epoch 38, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3757 (0.3757)\tAcc@1 83.000 (83.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.612 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [39][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3425 (0.3425)\tAcc@1 86.000 (86.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.101 Acc@5 100.000\n",
            "epoch 39, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3714 (0.3714)\tAcc@1 83.000 (83.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 85.971 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [40][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3377 (0.3377)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.421 Acc@5 100.000\n",
            "epoch 40, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3669 (0.3669)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.331 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "Epoch: [41][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3333 (0.3333)\tAcc@1 88.000 (88.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.662 Acc@5 100.000\n",
            "epoch 41, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3624 (0.3624)\tAcc@1 84.000 (84.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.331 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [42][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.3291 (0.3291)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.782 Acc@5 100.000\n",
            "epoch 42, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3574 (0.3574)\tAcc@1 85.000 (85.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 86.691 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [43][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3245 (0.3245)\tAcc@1 88.000 (88.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.022 Acc@5 100.000\n",
            "epoch 43, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3522 (0.3522)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 87.770 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [44][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3203 (0.3203)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.383 Acc@5 100.000\n",
            "epoch 44, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3484 (0.3484)\tAcc@1 85.000 (85.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 87.410 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [45][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3140 (0.3140)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.503 Acc@5 100.000\n",
            "epoch 45, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3425 (0.3425)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.849 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [46][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.3060 (0.3060)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.704 Acc@5 100.000\n",
            "epoch 46, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3384 (0.3384)\tAcc@1 86.000 (86.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.130 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [47][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.3027 (0.3027)\tAcc@1 88.000 (88.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.704 Acc@5 100.000\n",
            "epoch 47, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.3332 (0.3332)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.849 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [48][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2974 (0.2974)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.944 Acc@5 100.000\n",
            "epoch 48, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3286 (0.3286)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.849 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [49][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2932 (0.2932)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.024 Acc@5 100.000\n",
            "epoch 49, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3237 (0.3237)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 88.849 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [50][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2881 (0.2881)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.304 Acc@5 100.000\n",
            "epoch 50, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3203 (0.3203)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.209 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [51][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.2848 (0.2848)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.665 Acc@5 100.000\n",
            "epoch 51, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.3168 (0.3168)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.209 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [52][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2805 (0.2805)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.946 Acc@5 100.000\n",
            "epoch 52, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3106 (0.3106)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.209 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [53][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2765 (0.2765)\tAcc@1 92.000 (92.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.026 Acc@5 100.000\n",
            "epoch 53, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.3070 (0.3070)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.209 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [54][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2727 (0.2727)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.986 Acc@5 100.000\n",
            "epoch 54, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.3022 (0.3022)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.568 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [55][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2688 (0.2688)\tAcc@1 92.000 (92.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.266 Acc@5 100.000\n",
            "epoch 55, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2986 (0.2986)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.568 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [56][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2647 (0.2647)\tAcc@1 92.000 (92.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.306 Acc@5 100.000\n",
            "epoch 56, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2947 (0.2947)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.928 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [57][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.2601 (0.2601)\tAcc@1 92.000 (92.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.466 Acc@5 100.000\n",
            "epoch 57, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2915 (0.2915)\tAcc@1 87.000 (87.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 89.928 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [58][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.2554 (0.2554)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.747 Acc@5 100.000\n",
            "epoch 58, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2876 (0.2876)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.647 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [59][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2518 (0.2518)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.827 Acc@5 100.000\n",
            "epoch 59, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2819 (0.2819)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.647 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [60][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2475 (0.2475)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.027 Acc@5 100.000\n",
            "epoch 60, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2782 (0.2782)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.647 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [61][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2440 (0.2440)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.027 Acc@5 100.000\n",
            "epoch 61, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2748 (0.2748)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 90.647 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [62][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.2413 (0.2413)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.188 Acc@5 100.000\n",
            "epoch 62, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.2722 (0.2722)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.367 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [63][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.2386 (0.2386)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.308 Acc@5 100.000\n",
            "epoch 63, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2682 (0.2682)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.007 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [64][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2352 (0.2352)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.508 Acc@5 100.000\n",
            "epoch 64, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2650 (0.2650)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.367 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [65][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2327 (0.2327)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.508 Acc@5 100.000\n",
            "epoch 65, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2616 (0.2616)\tAcc@1 89.000 (89.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.367 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [66][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2289 (0.2289)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.668 Acc@5 100.000\n",
            "epoch 66, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2581 (0.2581)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.727 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [67][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.2262 (0.2262)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.829 Acc@5 100.000\n",
            "epoch 67, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2542 (0.2542)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.727 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [68][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.2231 (0.2231)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.788 Acc@5 100.000\n",
            "epoch 68, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2512 (0.2512)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.086 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [69][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2200 (0.2200)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.949 Acc@5 100.000\n",
            "epoch 69, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2475 (0.2475)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.727 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [70][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2172 (0.2172)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.189 Acc@5 100.000\n",
            "epoch 70, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2460 (0.2460)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.086 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [71][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.2145 (0.2145)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.550 Acc@5 100.000\n",
            "epoch 71, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2433 (0.2433)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.727 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [72][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2117 (0.2117)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.550 Acc@5 100.000\n",
            "epoch 72, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2394 (0.2394)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.086 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [73][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.2092 (0.2092)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.630 Acc@5 100.000\n",
            "epoch 73, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2381 (0.2381)\tAcc@1 90.000 (90.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 91.727 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [74][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.2065 (0.2065)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.710 Acc@5 100.000\n",
            "epoch 74, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2348 (0.2348)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.446 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [75][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.2043 (0.2043)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.830 Acc@5 100.000\n",
            "epoch 75, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2327 (0.2327)\tAcc@1 91.000 (91.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 92.446 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [76][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.2023 (0.2023)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.950 Acc@5 100.000\n",
            "epoch 76, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2298 (0.2298)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.165 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [77][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.1995 (0.1995)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.950 Acc@5 100.000\n",
            "epoch 77, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2278 (0.2278)\tAcc@1 92.000 (92.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.525 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [78][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1975 (0.1975)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.071 Acc@5 100.000\n",
            "epoch 78, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2258 (0.2258)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [79][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1955 (0.1955)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.111 Acc@5 100.000\n",
            "epoch 79, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2230 (0.2230)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [80][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.1930 (0.1930)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.231 Acc@5 100.000\n",
            "epoch 80, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2215 (0.2215)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "Epoch: [81][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1907 (0.1907)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.351 Acc@5 100.000\n",
            "epoch 81, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2191 (0.2191)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [82][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1887 (0.1887)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.431 Acc@5 100.000\n",
            "epoch 82, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2173 (0.2173)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [83][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1863 (0.1863)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.471 Acc@5 100.000\n",
            "epoch 83, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2150 (0.2150)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [84][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1843 (0.1843)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.471 Acc@5 100.000\n",
            "epoch 84, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2137 (0.2137)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [85][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1821 (0.1821)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.511 Acc@5 100.000\n",
            "epoch 85, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.2121 (0.2121)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [86][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1807 (0.1807)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.591 Acc@5 100.000\n",
            "epoch 86, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2101 (0.2101)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [87][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1789 (0.1789)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.591 Acc@5 100.000\n",
            "epoch 87, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2088 (0.2088)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [88][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1770 (0.1770)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.671 Acc@5 100.000\n",
            "epoch 88, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2062 (0.2062)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [89][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1747 (0.1747)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.671 Acc@5 100.000\n",
            "epoch 89, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.2050 (0.2050)\tAcc@1 93.000 (93.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 93.885 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [90][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1733 (0.1733)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.832 Acc@5 100.000\n",
            "epoch 90, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2032 (0.2032)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.245 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [91][0/25]\tTime 0.008 (0.008)\tData 0.006 (0.006)\tLoss 0.1714 (0.1714)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.832 Acc@5 100.000\n",
            "epoch 91, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.2019 (0.2019)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.245 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [92][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1697 (0.1697)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.952 Acc@5 100.000\n",
            "epoch 92, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1995 (0.1995)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [93][0/25]\tTime 0.005 (0.005)\tData 0.002 (0.002)\tLoss 0.1681 (0.1681)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.072 Acc@5 100.000\n",
            "epoch 93, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1990 (0.1990)\tAcc@1 94.000 (94.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.245 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [94][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1667 (0.1667)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.072 Acc@5 100.000\n",
            "epoch 94, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1974 (0.1974)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [95][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1651 (0.1651)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.112 Acc@5 100.000\n",
            "epoch 95, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1956 (0.1956)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [96][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1632 (0.1632)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.152 Acc@5 100.000\n",
            "epoch 96, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1946 (0.1946)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [97][0/25]\tTime 0.005 (0.005)\tData 0.002 (0.002)\tLoss 0.1622 (0.1622)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.272 Acc@5 100.000\n",
            "epoch 97, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1925 (0.1925)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [98][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1605 (0.1605)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.232 Acc@5 100.000\n",
            "epoch 98, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1918 (0.1918)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [99][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1593 (0.1593)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.312 Acc@5 100.000\n",
            "epoch 99, total time 0.07\n",
            "Test: [0/3]\tTime 0.004 (0.004)\tLoss 0.1903 (0.1903)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [100][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1580 (0.1580)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.393 Acc@5 100.000\n",
            "epoch 100, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1897 (0.1897)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [101][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1566 (0.1566)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.393 Acc@5 100.000\n",
            "epoch 101, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1882 (0.1882)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [102][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1552 (0.1552)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.513 Acc@5 100.000\n",
            "epoch 102, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1873 (0.1873)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [103][0/25]\tTime 0.007 (0.007)\tData 0.001 (0.001)\tLoss 0.1537 (0.1537)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.513 Acc@5 100.000\n",
            "epoch 103, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1862 (0.1862)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [104][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1527 (0.1527)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.593 Acc@5 100.000\n",
            "epoch 104, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1849 (0.1849)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [105][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1514 (0.1514)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.593 Acc@5 100.000\n",
            "epoch 105, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1839 (0.1839)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [106][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1502 (0.1502)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.673 Acc@5 100.000\n",
            "epoch 106, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1830 (0.1830)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [107][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1488 (0.1488)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.673 Acc@5 100.000\n",
            "epoch 107, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1824 (0.1824)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [108][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1478 (0.1478)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.713 Acc@5 100.000\n",
            "epoch 108, total time 0.07\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1811 (0.1811)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [109][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1464 (0.1464)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.753 Acc@5 100.000\n",
            "epoch 109, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1800 (0.1800)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [110][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1450 (0.1450)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.873 Acc@5 100.000\n",
            "epoch 110, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1789 (0.1789)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [111][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1440 (0.1440)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.913 Acc@5 100.000\n",
            "epoch 111, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.1785 (0.1785)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [112][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1431 (0.1431)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.034 Acc@5 100.000\n",
            "epoch 112, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1772 (0.1772)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [113][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1414 (0.1414)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.994 Acc@5 100.000\n",
            "epoch 113, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1766 (0.1766)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [114][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1409 (0.1409)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.034 Acc@5 100.000\n",
            "epoch 114, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1748 (0.1748)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [115][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1392 (0.1392)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.074 Acc@5 100.000\n",
            "epoch 115, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.1745 (0.1745)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [116][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.1387 (0.1387)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.074 Acc@5 100.000\n",
            "epoch 116, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1738 (0.1738)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [117][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1377 (0.1377)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.074 Acc@5 100.000\n",
            "epoch 117, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1729 (0.1729)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [118][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.1365 (0.1365)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.194 Acc@5 100.000\n",
            "epoch 118, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1729 (0.1729)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [119][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1355 (0.1355)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.154 Acc@5 100.000\n",
            "epoch 119, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1709 (0.1709)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [120][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1341 (0.1341)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.194 Acc@5 100.000\n",
            "epoch 120, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1706 (0.1706)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "Epoch: [121][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1338 (0.1338)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.154 Acc@5 100.000\n",
            "epoch 121, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1696 (0.1696)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [122][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1319 (0.1319)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.154 Acc@5 100.000\n",
            "epoch 122, total time 0.06\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.1692 (0.1692)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [123][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1310 (0.1310)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.154 Acc@5 100.000\n",
            "epoch 123, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1682 (0.1682)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [124][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.1300 (0.1300)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.194 Acc@5 100.000\n",
            "epoch 124, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1682 (0.1682)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [125][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1287 (0.1287)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.274 Acc@5 100.000\n",
            "epoch 125, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1671 (0.1671)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.604 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [126][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1281 (0.1281)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.354 Acc@5 100.000\n",
            "epoch 126, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1658 (0.1658)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [127][0/25]\tTime 0.007 (0.007)\tData 0.002 (0.002)\tLoss 0.1273 (0.1273)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.314 Acc@5 100.000\n",
            "epoch 127, total time 0.07\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1658 (0.1658)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [128][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1261 (0.1261)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.314 Acc@5 100.000\n",
            "epoch 128, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1648 (0.1648)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [129][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1248 (0.1248)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.434 Acc@5 100.000\n",
            "epoch 129, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1643 (0.1643)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [130][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1243 (0.1243)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.354 Acc@5 100.000\n",
            "epoch 130, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1635 (0.1635)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [131][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1226 (0.1226)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.394 Acc@5 100.000\n",
            "epoch 131, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1632 (0.1632)\tAcc@1 95.000 (95.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [132][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1218 (0.1218)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.394 Acc@5 100.000\n",
            "epoch 132, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1628 (0.1628)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [133][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.1214 (0.1214)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.474 Acc@5 100.000\n",
            "epoch 133, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1617 (0.1617)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [134][0/25]\tTime 0.005 (0.005)\tData 0.003 (0.003)\tLoss 0.1200 (0.1200)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.554 Acc@5 100.000\n",
            "epoch 134, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1615 (0.1615)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [135][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.1192 (0.1192)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.554 Acc@5 100.000\n",
            "epoch 135, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1611 (0.1611)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [136][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1180 (0.1180)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.554 Acc@5 100.000\n",
            "epoch 136, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1603 (0.1603)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [137][0/25]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 0.1177 (0.1177)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.514 Acc@5 100.000\n",
            "epoch 137, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1593 (0.1593)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [138][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.1160 (0.1160)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.635 Acc@5 100.000\n",
            "epoch 138, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1590 (0.1590)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [139][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1158 (0.1158)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.554 Acc@5 100.000\n",
            "epoch 139, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1590 (0.1590)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [140][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.1148 (0.1148)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.795 Acc@5 100.000\n",
            "epoch 140, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1573 (0.1573)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [141][0/25]\tTime 0.011 (0.011)\tData 0.001 (0.001)\tLoss 0.1139 (0.1139)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.755 Acc@5 100.000\n",
            "epoch 141, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1576 (0.1576)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [142][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1129 (0.1129)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.835 Acc@5 100.000\n",
            "epoch 142, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1568 (0.1568)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [143][0/25]\tTime 0.005 (0.005)\tData 0.002 (0.002)\tLoss 0.1119 (0.1119)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.875 Acc@5 100.000\n",
            "epoch 143, total time 0.07\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1567 (0.1567)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [144][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.1113 (0.1113)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.835 Acc@5 100.000\n",
            "epoch 144, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1560 (0.1560)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [145][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1104 (0.1104)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.875 Acc@5 100.000\n",
            "epoch 145, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1553 (0.1553)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [146][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1100 (0.1100)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.875 Acc@5 100.000\n",
            "epoch 146, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1542 (0.1542)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 94.964 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [147][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1083 (0.1083)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.875 Acc@5 100.000\n",
            "epoch 147, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1536 (0.1536)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [148][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1076 (0.1076)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.915 Acc@5 100.000\n",
            "epoch 148, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1543 (0.1543)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [149][0/25]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 0.1072 (0.1072)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.955 Acc@5 100.000\n",
            "epoch 149, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1531 (0.1531)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [150][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1063 (0.1063)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.955 Acc@5 100.000\n",
            "epoch 150, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1518 (0.1518)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [151][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1051 (0.1051)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.875 Acc@5 100.000\n",
            "epoch 151, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1521 (0.1521)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [152][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1046 (0.1046)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.995 Acc@5 100.000\n",
            "epoch 152, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1513 (0.1513)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [153][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1034 (0.1034)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.955 Acc@5 100.000\n",
            "epoch 153, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1508 (0.1508)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [154][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1029 (0.1029)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.955 Acc@5 100.000\n",
            "epoch 154, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1502 (0.1502)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [155][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.1017 (0.1017)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.035 Acc@5 100.000\n",
            "epoch 155, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1510 (0.1510)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [156][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.1013 (0.1013)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.075 Acc@5 100.000\n",
            "epoch 156, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1493 (0.1493)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [157][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.1003 (0.1003)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.995 Acc@5 100.000\n",
            "epoch 157, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1499 (0.1499)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [158][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0999 (0.0999)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.035 Acc@5 100.000\n",
            "epoch 158, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1486 (0.1486)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [159][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0985 (0.0985)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.035 Acc@5 100.000\n",
            "epoch 159, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1492 (0.1492)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [160][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0981 (0.0981)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.075 Acc@5 100.000\n",
            "epoch 160, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1484 (0.1484)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            "Epoch: [161][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0974 (0.0974)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.075 Acc@5 100.000\n",
            "epoch 161, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1482 (0.1482)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [162][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0963 (0.0963)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.155 Acc@5 100.000\n",
            "epoch 162, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1471 (0.1471)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [163][0/25]\tTime 0.003 (0.003)\tData 0.002 (0.002)\tLoss 0.0956 (0.0956)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.155 Acc@5 100.000\n",
            "epoch 163, total time 0.07\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1472 (0.1472)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [164][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0948 (0.0948)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.196 Acc@5 100.000\n",
            "epoch 164, total time 0.09\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1466 (0.1466)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [165][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0941 (0.0941)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.155 Acc@5 100.000\n",
            "epoch 165, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1457 (0.1457)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [166][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.0932 (0.0932)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.196 Acc@5 100.000\n",
            "epoch 166, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1454 (0.1454)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [167][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.0921 (0.0921)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.196 Acc@5 100.000\n",
            "epoch 167, total time 0.09\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1455 (0.1455)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [168][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0918 (0.0918)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.236 Acc@5 100.000\n",
            "epoch 168, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1450 (0.1450)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [169][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0910 (0.0910)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.196 Acc@5 100.000\n",
            "epoch 169, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1445 (0.1445)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [170][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0901 (0.0901)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.196 Acc@5 100.000\n",
            "epoch 170, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1434 (0.1434)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [171][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0891 (0.0891)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.236 Acc@5 100.000\n",
            "epoch 171, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1439 (0.1439)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [172][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0888 (0.0888)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.316 Acc@5 100.000\n",
            "epoch 172, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1433 (0.1433)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [173][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0877 (0.0877)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.316 Acc@5 100.000\n",
            "epoch 173, total time 0.08\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1428 (0.1428)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [174][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0869 (0.0869)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.276 Acc@5 100.000\n",
            "epoch 174, total time 0.14\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1426 (0.1426)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [175][0/25]\tTime 0.017 (0.017)\tData 0.002 (0.002)\tLoss 0.0865 (0.0865)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.396 Acc@5 100.000\n",
            "epoch 175, total time 0.23\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1421 (0.1421)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [176][0/25]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 0.0857 (0.0857)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.356 Acc@5 100.000\n",
            "epoch 176, total time 0.24\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1407 (0.1407)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [177][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0846 (0.0846)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.356 Acc@5 100.000\n",
            "epoch 177, total time 0.20\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1415 (0.1415)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [178][0/25]\tTime 0.021 (0.021)\tData 0.011 (0.011)\tLoss 0.0846 (0.0846)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.436 Acc@5 100.000\n",
            "epoch 178, total time 0.21\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1405 (0.1405)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [179][0/25]\tTime 0.007 (0.007)\tData 0.001 (0.001)\tLoss 0.0834 (0.0834)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.396 Acc@5 100.000\n",
            "epoch 179, total time 0.19\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1404 (0.1404)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [180][0/25]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 0.0828 (0.0828)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.396 Acc@5 100.000\n",
            "epoch 180, total time 0.26\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1397 (0.1397)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [181][0/25]\tTime 0.013 (0.013)\tData 0.001 (0.001)\tLoss 0.0823 (0.0823)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.356 Acc@5 100.000\n",
            "epoch 181, total time 0.30\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1395 (0.1395)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.683 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [182][0/25]\tTime 0.013 (0.013)\tData 0.001 (0.001)\tLoss 0.0812 (0.0812)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.436 Acc@5 100.000\n",
            "epoch 182, total time 0.22\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1385 (0.1385)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.683 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [183][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.0809 (0.0809)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.396 Acc@5 100.000\n",
            "epoch 183, total time 0.31\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1390 (0.1390)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.324 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [184][0/25]\tTime 0.009 (0.009)\tData 0.001 (0.001)\tLoss 0.0800 (0.0800)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.436 Acc@5 100.000\n",
            "epoch 184, total time 0.22\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1381 (0.1381)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.683 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [185][0/25]\tTime 0.015 (0.015)\tData 0.001 (0.001)\tLoss 0.0792 (0.0792)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.516 Acc@5 100.000\n",
            "epoch 185, total time 0.21\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1378 (0.1378)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.683 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [186][0/25]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 0.0791 (0.0791)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.516 Acc@5 100.000\n",
            "epoch 186, total time 0.19\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1381 (0.1381)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            "Epoch: [187][0/25]\tTime 0.014 (0.014)\tData 0.003 (0.003)\tLoss 0.0783 (0.0783)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.516 Acc@5 100.000\n",
            "epoch 187, total time 0.23\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1365 (0.1365)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 95.683 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [188][0/25]\tTime 0.007 (0.007)\tData 0.005 (0.005)\tLoss 0.0771 (0.0771)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.596 Acc@5 100.000\n",
            "epoch 188, total time 0.17\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1372 (0.1372)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [189][0/25]\tTime 0.020 (0.020)\tData 0.001 (0.001)\tLoss 0.0767 (0.0767)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.556 Acc@5 100.000\n",
            "epoch 189, total time 0.20\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1372 (0.1372)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [190][0/25]\tTime 0.014 (0.014)\tData 0.001 (0.001)\tLoss 0.0763 (0.0763)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.636 Acc@5 100.000\n",
            "epoch 190, total time 0.17\n",
            "Test: [0/3]\tTime 0.003 (0.003)\tLoss 0.1363 (0.1363)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [191][0/25]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 0.0753 (0.0753)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.596 Acc@5 100.000\n",
            "epoch 191, total time 0.13\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1362 (0.1362)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [192][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0751 (0.0751)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.716 Acc@5 100.000\n",
            "epoch 192, total time 0.14\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1360 (0.1360)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [193][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0741 (0.0741)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.756 Acc@5 100.000\n",
            "epoch 193, total time 0.07\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1356 (0.1356)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [194][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0737 (0.0737)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.716 Acc@5 100.000\n",
            "epoch 194, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1347 (0.1347)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [195][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0727 (0.0727)\tAcc@1 98.000 (98.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.756 Acc@5 100.000\n",
            "epoch 195, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1347 (0.1347)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [196][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0720 (0.0720)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.716 Acc@5 100.000\n",
            "epoch 196, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1341 (0.1341)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [197][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0715 (0.0715)\tAcc@1 98.000 (98.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.796 Acc@5 100.000\n",
            "epoch 197, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1337 (0.1337)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [198][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0712 (0.0712)\tAcc@1 97.000 (97.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.756 Acc@5 100.000\n",
            "epoch 198, total time 0.06\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1339 (0.1339)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [199][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0701 (0.0701)\tAcc@1 98.000 (98.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.796 Acc@5 100.000\n",
            "epoch 199, total time 0.06\n",
            "Test: [0/3]\tTime 0.002 (0.002)\tLoss 0.1328 (0.1328)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> training...\n",
            "Epoch: [200][0/25]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 0.0696 (0.0696)\tAcc@1 98.000 (98.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 97.837 Acc@5 100.000\n",
            "epoch 200, total time 0.07\n",
            "Test: [0/3]\tTime 0.001 (0.001)\tLoss 0.1326 (0.1326)\tAcc@1 96.000 (96.000)\tAcc@5 100.000 (100.000)\n",
            " * Acc@1 96.043 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(96.0432, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_cnn_1d:\n",
        "    model = cnn1d(num_channels, num_classes).to(device)\n",
        "else:\n",
        "    model = fnn(num_channels, num_classes).to(device)\n",
        "model.load_state_dict(torch.load('./model_best.pth')['model'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "\n",
        "acc_1, acc_5, t_loss = validate(test_dataloader, model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpBgTG0EPN2T",
        "outputId": "4608c889-87bf-4c0e-ab60-9f3a3241a110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: [0/401]\tTime 0.004 (0.004)\tLoss 4.7903 (4.7903)\tAcc@1 80.000 (80.000)\tAcc@5 94.000 (94.000)\n",
            "Test: [100/401]\tTime 0.001 (0.001)\tLoss 1.3625 (0.6831)\tAcc@1 96.000 (91.069)\tAcc@5 98.000 (99.356)\n",
            "Test: [200/401]\tTime 0.002 (0.001)\tLoss 0.6011 (0.6182)\tAcc@1 86.000 (89.831)\tAcc@5 99.000 (99.388)\n",
            "Test: [300/401]\tTime 0.001 (0.001)\tLoss 0.4453 (0.7943)\tAcc@1 79.000 (83.983)\tAcc@5 100.000 (99.389)\n",
            "Test: [400/401]\tTime 0.001 (0.001)\tLoss 1.2174 (0.7508)\tAcc@1 50.000 (81.653)\tAcc@5 100.000 (99.538)\n",
            " * Acc@1 81.653 Acc@5 99.538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher top-1 test accuracy: {:.3f}\".format(acc_1))\n",
        "print(\"Teacher top-5 test accuracy: {:.3f}\".format(acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tenOVGevPN46",
        "outputId": "0d40f3d4-5b70-40e4-9ad0-d42f54e1ddf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher top-1 test accuracy: 81.653\n",
            "Teacher top-5 test accuracy: 99.538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of model parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nX2JA7OPN73",
        "outputId": "e6ca12a8-cafd-423c-9c4c-2d827af7dc8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of model parameters: 33709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size model(MB):', os.path.getsize(\"./model_best.pth\")/1e6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LNUyyBwPW38",
        "outputId": "522c0d16-960d-452a-bf06-3521c964981d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size model(MB): 0.415531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the sizes\n",
        "f=print_size_of_model(model,\"fp32\")\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,103), num_samples=100)\n",
        "print(f, fp32_cpu_inference_latency*1000)"
      ],
      "metadata": {
        "id": "arwDvmbUPW6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aae9aaf-be12-4462-9022-1abb4e200d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32  \t Size (MB): 0.066125\n",
            "66125 0.10264396667480469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3ozTd4bPW8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5x6EY6D2FtQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}