{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "V8MVvYkwPUsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "metadata": {
        "id": "9UWpj8baGdEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "\n",
        "    return patchesData, patchesLabels.astype(\"int\")"
      ],
      "metadata": {
        "id": "eyK_NRHiGdHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ['India', 'Pavia'][1]\n",
        "random_split = True"
      ],
      "metadata": {
        "id": "Y--Smq6b5-_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == 'India':\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/indian_pines_corrected.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indianpines_disjoint_dset.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indian_pines_gt.mat')\n",
        "\n",
        "    X_all = raw_data['indian_pines_corrected']\n",
        "    y_disjoint = disjoint_data['indianpines_disjoint_dset']\n",
        "    y_all = all_labels['indian_pines_gt']\n",
        "\n",
        "    test_ratio = 0.45\n",
        "else:\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/paviaU.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/TRpavia_fixed.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/paviaU_gt.mat')\n",
        "\n",
        "    X_all = raw_data['paviaU']\n",
        "    y_disjoint = disjoint_data['TRpavia_fixed']\n",
        "    y_all = all_labels['paviaU_gt']\n",
        "\n",
        "    test_ratio = 0.93"
      ],
      "metadata": {
        "id": "GW4q69mK6E-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Uci233aici8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub"
      ],
      "metadata": {
        "id": "_nYjGrFZci-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")"
      ],
      "metadata": {
        "id": "quGUfFsw5HoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = X_all.shape[-1]\n",
        "W = X_all.shape[0]\n",
        "H = X_all.shape[1]\n",
        "num_components = 40\n",
        "window = 19"
      ],
      "metadata": {
        "id": "Iq60eCjE3uN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = X_all.reshape(W*H, num_channels)\n",
        "y_disjoint = y_disjoint.reshape(-1, 1).flatten()\n",
        "\n",
        "X_all = X_all.astype(np.float32)"
      ],
      "metadata": {
        "id": "pfy8EN34aYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=num_components)\n",
        "X_all = pca.fit_transform(X_all)\n",
        "\n",
        "X_all = X_all.reshape(W, H, num_components)"
      ],
      "metadata": {
        "id": "CC7d6YQN7aI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = createImageCubes(X_all, y_all, windowSize=window, removeZeroLabels = False)"
      ],
      "metadata": {
        "id": "9oB6lWLu8HO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('data.npy', X)\n",
        "np.save('labels.npy', y)"
      ],
      "metadata": {
        "id": "Yxzb3j0_oBWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_all, y_all, X, y, raw_data"
      ],
      "metadata": {
        "id": "MF7Ll8wSagy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load('data.npy', mmap_mode='c')\n",
        "y = np.load('labels.npy', mmap_mode='c')"
      ],
      "metadata": {
        "id": "7SC0qR7soTS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.transpose(X, (0, 3, 1, 2))"
      ],
      "metadata": {
        "id": "UQbljf-2-DAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSszW6uQFKcW",
        "outputId": "52676cb7-cbb0-40a1-b5a9-1cb99cdb707b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(207400, 40, 19, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if random_split:\n",
        "    nonzero_idx = np.where(y != 0)\n",
        "    X = X[nonzero_idx]\n",
        "    y = y[nonzero_idx]\n",
        "    y = y - 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
        "else:\n",
        "    train_idx = np.where((y_disjoint != 0) & (y != 0))\n",
        "    X_train = X[train_idx]\n",
        "    y_train = y[train_idx]\n",
        "\n",
        "    test_idx = np.where((y_disjoint == 0) & (y != 0))\n",
        "    X_test = X[test_idx]\n",
        "    y_test = y[test_idx]\n",
        "\n",
        "    y_train -= 1\n",
        "    y_test -= 1\n",
        "\n",
        "    print('Disjoint')"
      ],
      "metadata": {
        "id": "mLayt2rYcVVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], num_components*window*window)\n",
        "X_test = X_test.reshape(X_test.shape[0], num_components*window*window)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], num_components, window, window)\n",
        "X_test = X_test.reshape(X_test.shape[0], num_components, window, window)"
      ],
      "metadata": {
        "id": "VEgdEKhfY6t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "HXY841qwCwEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2anZduvxjhup",
        "outputId": "d7380200-96a4-486c-8b0b-b7ade6ee3f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39782, 40, 19, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_train) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_train)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100) # create your dataloader"
      ],
      "metadata": {
        "id": "JY6DCjbUcWIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_val) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_val)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "val_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100) # create your dataloader"
      ],
      "metadata": {
        "id": "NvNX80DayaKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_test)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100) # create your dataloader"
      ],
      "metadata": {
        "id": "hm46r6mzcWKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn2d(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN2d network\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(0.5)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "    def __init__(self, input_channels, n_classes):\n",
        "        super(cnn2d, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=50, kernel_size=(5, 5))\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=100, kernel_size=(5, 5))\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.fc1 = nn.Linear(2500, 100)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(100, n_classes)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.pool(self.relu2(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TjB1JL3BCR2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy"
      ],
      "metadata": {
        "id": "7NbI7nFW3AaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, epochs, lr, device, save_file):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.02)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    min_valid_loss = np.inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Save model.\n",
        "        if min_valid_loss > (eval_loss):\n",
        "            min_valid_loss = (eval_loss)\n",
        "            save_model(model=model, model_dir=\"./\", model_filename=save_file)\n",
        "\n",
        "        if epoch % (epochs//10) == 0 or epoch == (epochs-1):\n",
        "            print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, \\\n",
        "            train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rqzTk9df3Ac_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)"
      ],
      "metadata": {
        "id": "iQEcDM9m3GXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 40, 19, 19),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "-FzaAoTm3Vu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KR9g6yo-3VxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dnvW1qax3V0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model, label=\"\"):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size=os.path.getsize(\"temp.p\")\n",
        "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
        "    os.remove('temp.p')\n",
        "    return size"
      ],
      "metadata": {
        "id": "0PhBAOt-F2qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedCNN(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedCNN, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L8pKmV_N3V3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,40,19,19)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "lPIbnZaz6cEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn2d(num_components, len(np.unique(y_train)))"
      ],
      "metadata": {
        "id": "DaWaaEPh3V5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model=model, train_loader=train_dataloader, test_loader=val_dataloader, \\\n",
        "                    epochs=100, lr=1e-3, device=cuda_device, save_file=\"best_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMD1QpLi3Gam",
        "outputId": "36648f93-8012-47e0-c19d-138a3c8b6343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00 Train Loss: 1.406 Train Acc: 0.507 Eval Loss: 0.963 Eval Acc: 0.640\n",
            "Epoch: 10 Train Loss: 0.035 Train Acc: 0.994 Eval Loss: 0.042 Eval Acc: 0.990\n",
            "Epoch: 20 Train Loss: 0.025 Train Acc: 0.997 Eval Loss: 0.062 Eval Acc: 0.987\n",
            "Epoch: 30 Train Loss: 0.015 Train Acc: 1.000 Eval Loss: 0.033 Eval Acc: 0.990\n",
            "Epoch: 40 Train Loss: 0.028 Train Acc: 0.994 Eval Loss: 0.086 Eval Acc: 0.977\n",
            "Epoch: 50 Train Loss: 0.014 Train Acc: 1.000 Eval Loss: 0.036 Eval Acc: 0.990\n",
            "Epoch: 60 Train Loss: 0.014 Train Acc: 1.000 Eval Loss: 0.034 Eval Acc: 0.987\n",
            "Epoch: 70 Train Loss: 0.020 Train Acc: 0.996 Eval Loss: 0.050 Eval Acc: 0.980\n",
            "Epoch: 80 Train Loss: 0.012 Train Acc: 1.000 Eval Loss: 0.031 Eval Acc: 0.990\n",
            "Epoch: 90 Train Loss: 0.016 Train Acc: 0.999 Eval Loss: 0.048 Eval Acc: 0.980\n",
            "Epoch: 99 Train Loss: 0.012 Train Acc: 1.000 Eval Loss: 0.028 Eval Acc: 0.990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic Quantization**"
      ],
      "metadata": {
        "id": "cYkuXpypxwgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=\"./best_model.pth\", device=cuda_device)\n",
        "model.to(cpu_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbAXDvDmmSPX",
        "outputId": "2e27a64c-f33f-4377-d271-8f87c09bc04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cnn2d(\n",
              "  (conv1): Conv2d(40, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU(inplace=True)\n",
              "  (conv2): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU(inplace=True)\n",
              "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=2500, out_features=100, bias=True)\n",
              "  (relu3): ReLU(inplace=True)\n",
              "  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.Conv2d, nn.Linear, nn.ReLU}, dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "8BnC3kl-megT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_dataloader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_model, test_loader=test_dataloader, device=cpu_device, criterion=None)\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2lcXSRZmejO",
        "outputId": "301ea7ac-641d-4719-9241-59732762832d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 evaluation accuracy: 0.996\n",
            "INT8 evaluation accuracy: 0.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the sizes\n",
        "f=print_size_of_model(model,\"fp32\")\n",
        "q=print_size_of_model(quantized_model,\"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1byuOz6IzAW",
        "outputId": "bcc46f20-da8f-4b73-bee9-00aa01f477b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32  \t Size (MB): 1.707121\n",
            "model:  int8  \t Size (MB): 0.955863\n",
            "1.79 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnE6k5SrpUVz",
        "outputId": "eb5c535f-4052-40bb-df5e-be6f5c35a016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 CPU Inference Latency: 1.05 ms / sample\n",
            "INT8 CPU Inference Latency: 1.05 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Static Quantization**"
      ],
      "metadata": {
        "id": "5PwYlmv_J0Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=\"./best_model.pth\", device=cuda_device)\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "ECCGE60k6AT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "# The model has to be switched to evaluation mode before any layer fusion.\n",
        "# Otherwise the quantization will not work correctly.\n",
        "fused_model.eval()\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"relu1\"],\n",
        "                                                            [\"conv2\", \"relu2\"],\n",
        "                                                            [\"fc1\", \"relu3\"]], inplace=True)"
      ],
      "metadata": {
        "id": "XFJJesZj6AWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print FP32 model.\n",
        "print(model)\n",
        "# Print fused model.\n",
        "print(fused_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snmfdpkW6AZh",
        "outputId": "72ff168a-b463-4901-bf6a-994896d02a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn2d(\n",
            "  (conv1): Conv2d(40, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu2): ReLU(inplace=True)\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2500, out_features=100, bias=True)\n",
            "  (relu3): ReLU(inplace=True)\n",
            "  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",
            ")\n",
            "cnn2d(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(40, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (relu1): Identity()\n",
            "  (conv2): ConvReLU2d(\n",
            "    (0): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (relu2): Identity()\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): LinearReLU(\n",
            "    (0): Linear(in_features=2500, out_features=100, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (relu3): Identity()\n",
            "  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and fused model should be equivalent.\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, \\\n",
        "                         atol=1e-06, num_tests=100, input_size=(1,num_components, window, window)),\\\n",
        "                         \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "mxrPH1D86T9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedCNN(model_fp32=fused_model)\n",
        "# Select quantization schemes from \n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")"
      ],
      "metadata": {
        "id": "kCzY6ZEM6T_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.qconfig = quantization_config\n",
        "    \n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(quantized_model, inplace=True)\n",
        "\n",
        "# Use training data for calibration.\n",
        "calibrate_model(model=quantized_model, loader=train_dataloader, device=cpu_device)\n",
        "\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrfmNxtg6UCG",
        "outputId": "24363e53-6fd1-490f-ecda-6658f934c791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using high-level static quantization wrapper\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=\"./\", model_filename=\"quantized_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef7dg_Lf6UEi",
        "outputId": "b05f7f54-9ba3-4933-d59e-ea40fc0de235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedCNN(\n",
            "  (quant): Quantize(scale=tensor([0.3600]), zero_point=tensor([53]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): cnn2d(\n",
            "    (conv1): QuantizedConvReLU2d(40, 50, kernel_size=(5, 5), stride=(1, 1), scale=0.11217733472585678, zero_point=0)\n",
            "    (relu1): Identity()\n",
            "    (conv2): QuantizedConvReLU2d(50, 100, kernel_size=(5, 5), stride=(1, 1), scale=0.28110361099243164, zero_point=0)\n",
            "    (relu2): Identity()\n",
            "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (fc1): QuantizedLinearReLU(in_features=2500, out_features=100, scale=0.26749753952026367, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "    (relu3): Identity()\n",
            "    (fc2): QuantizedLinear(in_features=100, out_features=9, scale=0.3958379030227661, zero_point=83, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=\"./quantized_model.pth\", device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_dataloader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_model, test_loader=test_dataloader, device=cpu_device, criterion=None)\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk1-d87H6UHM",
        "outputId": "b291373d-0335-4a7c-ed5c-af045d508a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 evaluation accuracy: 0.996\n",
            "INT8 evaluation accuracy: 0.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v19vPr96UJp",
        "outputId": "6a13cf9e-9663-4bef-9e34-56d2467f3133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 CPU Inference Latency: 0.96 ms / sample\n",
            "FP32 CUDA Inference Latency: 0.44 ms / sample\n",
            "INT8 CPU Inference Latency: 0.68 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 0.52 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the sizes\n",
        "f=print_size_of_model(model,\"fp32\")\n",
        "q=print_size_of_model(quantized_model,\"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z50X_ngD7Lwz",
        "outputId": "4b8401e8-e823-43d0-d42d-159447375581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32  \t Size (MB): 1.707249\n",
            "model:  int8  \t Size (MB): 0.438659\n",
            "3.89 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantization Aware Training (QAT)**"
      ],
      "metadata": {
        "id": "mlmCKoEsJd8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=\"./best_model.pth\", device=cuda_device)\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "zzviLVOj4S0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "# The model has to be switched to training mode before any layer fusion.\n",
        "# Otherwise the quantization aware training will not work correctly.\n",
        "fused_model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2S12hRJJ5Te",
        "outputId": "59af1652-c15a-45e4-c2bc-13d5d3d30c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cnn2d(\n",
              "  (conv1): Conv2d(40, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU(inplace=True)\n",
              "  (conv2): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU(inplace=True)\n",
              "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=2500, out_features=100, bias=True)\n",
              "  (relu3): ReLU(inplace=True)\n",
              "  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"relu1\"],\n",
        "                                                            [\"conv2\", \"relu2\"],\n",
        "                                                            [\"fc1\", \"relu3\"]], inplace=True)"
      ],
      "metadata": {
        "id": "3Ab4fMK9J5WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print FP32 model.\n",
        "print(model)\n",
        "# Print fused model.\n",
        "print(fused_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-CZB2ibJ5ZQ",
        "outputId": "96340b75-eb1e-479e-8abc-1b07762bec38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn2d(\n",
            "  (conv1): Conv2d(40, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu2): ReLU(inplace=True)\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2500, out_features=100, bias=True)\n",
            "  (relu3): ReLU(inplace=True)\n",
            "  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",
            ")\n",
            "cnn2d(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(40, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (relu1): Identity()\n",
            "  (conv2): ConvReLU2d(\n",
            "    (0): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (relu2): Identity()\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): LinearReLU(\n",
            "    (0): Linear(in_features=2500, out_features=100, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (relu3): Identity()\n",
            "  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and fused model should be equivalent.\n",
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "fused_model.eval()\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, \\\n",
        "                         atol=1e-06, num_tests=100, input_size=(1,num_components, window, window)),\\\n",
        "                         \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "rLL6qc7c4S24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedCNN(model_fp32=fused_model)\n",
        "# Select quantization schemes from \n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")"
      ],
      "metadata": {
        "id": "5C0cXJSWKOrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.qconfig = quantization_config\n",
        "    \n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare_qat(quantized_model, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDxR4DEzKOt1",
        "outputId": "9263b413-7680-4074-fbd0-76b3d3126de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedCNN(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              "  (model_fp32): cnn2d(\n",
              "    (conv1): ConvReLU2d(\n",
              "      40, 50, kernel_size=(5, 5), stride=(1, 1)\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (relu1): Identity()\n",
              "    (conv2): ConvReLU2d(\n",
              "      50, 100, kernel_size=(5, 5), stride=(1, 1)\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (relu2): Identity()\n",
              "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (fc1): LinearReLU(\n",
              "      in_features=2500, out_features=100, bias=True\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (relu3): Identity()\n",
              "    (fc2): Linear(\n",
              "      in_features=100, out_features=9, bias=True\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Use training data for calibration.\n",
        "print(\"Training QAT Model...\")\n",
        "quantized_model.train()\n",
        "train_model(model=quantized_model, train_loader=train_dataloader, test_loader=val_dataloader, \\\n",
        "            epochs=100, lr=1e-5, device=cuda_device, save_file=\"qat_model.pth\")\n",
        "quantized_model.to(cpu_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPg2PCokKOwT",
        "outputId": "f6346951-e682-4fbc-9f30-169d0c0a04aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training QAT Model...\n",
            "Epoch: 00 Train Loss: 0.009 Train Acc: 1.000 Eval Loss: 0.022 Eval Acc: 0.993\n",
            "Epoch: 10 Train Loss: 0.009 Train Acc: 1.000 Eval Loss: 0.024 Eval Acc: 0.993\n",
            "Epoch: 20 Train Loss: 0.010 Train Acc: 1.000 Eval Loss: 0.026 Eval Acc: 0.990\n",
            "Epoch: 30 Train Loss: 0.010 Train Acc: 1.000 Eval Loss: 0.027 Eval Acc: 0.990\n",
            "Epoch: 40 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.027 Eval Acc: 0.990\n",
            "Epoch: 50 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.028 Eval Acc: 0.990\n",
            "Epoch: 60 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.029 Eval Acc: 0.990\n",
            "Epoch: 70 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.029 Eval Acc: 0.990\n",
            "Epoch: 80 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.029 Eval Acc: 0.990\n",
            "Epoch: 90 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.030 Eval Acc: 0.990\n",
            "Epoch: 99 Train Loss: 0.011 Train Acc: 1.000 Eval Loss: 0.030 Eval Acc: 0.990\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedCNN(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): HistogramObserver(min_val=-51.8465461730957, max_val=55.524837493896484)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              "  (model_fp32): cnn2d(\n",
              "    (conv1): ConvReLU2d(\n",
              "      40, 50, kernel_size=(5, 5), stride=(1, 1)\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "        min_val=tensor([-0.0203, -0.0091, -0.0382, -0.0292, -0.0437, -0.0423, -0.0455, -0.0029,\n",
              "                -0.0425, -0.0339, -0.0325, -0.0018, -0.0034, -0.0419, -0.0523, -0.0217,\n",
              "                -0.0033, -0.0137, -0.0512, -0.0333, -0.0083, -0.0444, -0.0088, -0.0056,\n",
              "                -0.0042, -0.0101, -0.0739, -0.0115, -0.0835, -0.0034, -0.0234, -0.0059,\n",
              "                -0.0398, -0.0708, -0.0105, -0.0622, -0.0508, -0.0027, -0.0052, -0.0030,\n",
              "                -0.0160, -0.0128, -0.0139, -0.0067, -0.0121, -0.0102, -0.0041, -0.0299,\n",
              "                -0.0272, -0.0051]), max_val=tensor([0.0139, 0.0099, 0.0341, 0.0495, 0.0531, 0.0213, 0.0331, 0.0040, 0.0452,\n",
              "                0.0158, 0.0383, 0.0024, 0.0022, 0.0236, 0.0262, 0.0304, 0.0021, 0.0096,\n",
              "                0.0677, 0.0326, 0.0062, 0.0357, 0.0080, 0.0047, 0.0052, 0.0099, 0.0639,\n",
              "                0.0075, 0.0509, 0.0022, 0.0137, 0.0083, 0.0539, 0.0399, 0.0121, 0.0617,\n",
              "                0.0332, 0.0034, 0.0045, 0.0042, 0.0149, 0.0119, 0.0118, 0.0131, 0.0187,\n",
              "                0.0097, 0.0035, 0.0491, 0.0413, 0.0066])\n",
              "      )\n",
              "      (activation_post_process): HistogramObserver(min_val=0.0, max_val=18.29271697998047)\n",
              "    )\n",
              "    (relu1): Identity()\n",
              "    (conv2): ConvReLU2d(\n",
              "      50, 100, kernel_size=(5, 5), stride=(1, 1)\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "        min_val=tensor([-0.0008, -0.0037, -0.0008, -0.0101, -0.0008, -0.0308, -0.0044, -0.0008,\n",
              "                -0.0113, -0.0008, -0.0177, -0.0008, -0.0178, -0.0091, -0.0008, -0.0007,\n",
              "                -0.0007, -0.0007, -0.0264, -0.0007, -0.0008, -0.0008, -0.0007, -0.0008,\n",
              "                -0.0008, -0.0249, -0.0019, -0.0007, -0.0165, -0.0008, -0.0011, -0.0061,\n",
              "                -0.0007, -0.0036, -0.0326, -0.0007, -0.0007, -0.0068, -0.0016, -0.0286,\n",
              "                -0.0029, -0.0406, -0.0028, -0.0008, -0.0007, -0.0007, -0.0008, -0.0119,\n",
              "                -0.0008, -0.0044, -0.0008, -0.0008, -0.0097, -0.0007, -0.0007, -0.0007,\n",
              "                -0.0008, -0.0008, -0.0008, -0.0008, -0.0008, -0.0008, -0.0195, -0.0007,\n",
              "                -0.0044, -0.0216, -0.0008, -0.0267, -0.0054, -0.0007, -0.0008, -0.0007,\n",
              "                -0.0035, -0.0008, -0.0008, -0.0007, -0.0008, -0.0007, -0.0008, -0.0008,\n",
              "                -0.0015, -0.0007, -0.0243, -0.0007, -0.0188, -0.0008, -0.0008, -0.0008,\n",
              "                -0.0088, -0.0008, -0.0008, -0.0008, -0.0008, -0.0142, -0.0008, -0.0349,\n",
              "                -0.0492, -0.0008, -0.0191, -0.0128]), max_val=tensor([0.0032, 0.0031, 0.0032, 0.0313, 0.0030, 0.0921, 0.0010, 0.0030, 0.0005,\n",
              "                0.0031, 0.0108, 0.0033, 0.0479, 0.0166, 0.0032, 0.0027, 0.0028, 0.0027,\n",
              "                0.0288, 0.0003, 0.0032, 0.0033, 0.0027, 0.0032, 0.0032, 0.0503, 0.0006,\n",
              "                0.0028, 0.0273, 0.0032, 0.0002, 0.0037, 0.0027, 0.0036, 0.0580, 0.0028,\n",
              "                0.0027, 0.0003, 0.0001, 0.0444, 0.0009, 0.0844, 0.0070, 0.0032, 0.0028,\n",
              "                0.0027, 0.0031, 0.0323, 0.0029, 0.0031, 0.0032, 0.0032, 0.0132, 0.0028,\n",
              "                0.0028, 0.0028, 0.0030, 0.0028, 0.0031, 0.0031, 0.0031, 0.0033, 0.0534,\n",
              "                0.0028, 0.0019, 0.0572, 0.0031, 0.0567, 0.0004, 0.0027, 0.0032, 0.0027,\n",
              "                0.0006, 0.0031, 0.0028, 0.0027, 0.0032, 0.0027, 0.0032, 0.0032, 0.0003,\n",
              "                0.0005, 0.0720, 0.0027, 0.0148, 0.0029, 0.0030, 0.0002, 0.0157, 0.0033,\n",
              "                0.0032, 0.0030, 0.0032, 0.0197, 0.0032, 0.0402, 0.0648, 0.0030, 0.0517,\n",
              "                0.0190])\n",
              "      )\n",
              "      (activation_post_process): HistogramObserver(min_val=0.0, max_val=45.316349029541016)\n",
              "    )\n",
              "    (relu2): Identity()\n",
              "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (fc1): LinearReLU(\n",
              "      in_features=2500, out_features=100, bias=True\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "        min_val=tensor([-1.3099e-02, -1.0623e-02, -3.7128e-02, -9.3260e-03, -1.3762e-02,\n",
              "                -1.6362e-02, -1.4512e-02, -2.9350e-02, -3.3144e-02, -1.1124e-02,\n",
              "                -2.9855e-03, -1.9133e-02, -2.0351e-02, -1.9000e-02, -5.8446e-04,\n",
              "                -1.8645e-02, -3.8672e-02, -5.8905e-03, -3.6830e-02, -1.6461e-02,\n",
              "                -3.2059e-02, -2.3632e-02, -1.0160e-02, -1.4372e-02, -1.3680e-02,\n",
              "                -3.8083e-02, -1.1964e-02, -1.1152e-02, -1.1319e-04, -4.1635e-02,\n",
              "                -3.7759e-02, -2.2060e-02, -1.3936e-02, -3.7472e-03, -2.8197e-02,\n",
              "                -1.8795e-02, -1.9524e-04, -5.7450e-02, -1.9064e-02, -5.2579e-02,\n",
              "                -1.2656e-02, -3.1324e-02, -1.6499e-02, -9.5939e-05, -2.7175e-02,\n",
              "                -9.2450e-05, -1.1808e-02, -1.4564e-02, -2.1109e-02, -2.1361e-02,\n",
              "                -1.0118e-03, -7.7716e-03, -2.1741e-02, -9.1099e-03, -1.8018e-02,\n",
              "                -3.9369e-05, -3.1682e-02, -9.1297e-04, -1.8874e-02, -2.3577e-02,\n",
              "                -9.4075e-04, -5.1599e-02, -1.1858e-02, -2.4465e-02, -1.9540e-02,\n",
              "                -3.0836e-02, -1.8863e-02, -8.9499e-05, -1.7534e-02, -1.0520e-03,\n",
              "                -2.9302e-02, -3.8874e-04, -3.5251e-02, -1.3589e-02, -1.9142e-04,\n",
              "                -1.9335e-03, -3.7926e-02, -4.3530e-03, -1.8009e-02, -1.2237e-04,\n",
              "                -1.1420e-04, -9.6749e-03, -3.7962e-02, -2.1169e-02, -3.1035e-02,\n",
              "                -3.4493e-02, -3.9673e-02, -2.4433e-03, -1.3268e-03, -1.5363e-02,\n",
              "                -3.9540e-02, -1.1130e-02, -1.9847e-02, -1.7018e-02, -2.2169e-02,\n",
              "                -1.6504e-02, -4.0254e-03, -2.3903e-02, -4.8787e-02, -1.9931e-04]), max_val=tensor([3.9821e-02, 1.3269e-02, 3.6946e-02, 1.0486e-02, 1.7467e-02, 2.0994e-02,\n",
              "                3.4961e-02, 2.0137e-02, 2.6808e-02, 3.0704e-02, 3.4105e-03, 2.7049e-02,\n",
              "                3.5900e-02, 1.7458e-02, 9.5229e-04, 1.5508e-02, 4.4745e-02, 1.8922e-02,\n",
              "                4.7606e-02, 1.4911e-02, 8.1099e-02, 1.7788e-02, 2.6370e-02, 2.0642e-02,\n",
              "                1.5269e-02, 3.1220e-02, 1.2334e-02, 1.2310e-02, 1.9056e-04, 5.7537e-02,\n",
              "                5.5196e-02, 3.0558e-02, 2.3793e-02, 1.2171e-03, 3.7058e-02, 3.8936e-02,\n",
              "                2.3448e-04, 4.8741e-02, 1.5512e-02, 5.3776e-02, 1.8027e-02, 3.8188e-02,\n",
              "                2.0196e-02, 1.2881e-04, 4.9034e-02, 1.1789e-04, 2.4990e-02, 2.2725e-02,\n",
              "                3.7144e-02, 2.7022e-02, 1.1081e-03, 2.2690e-02, 2.2334e-02, 2.2270e-02,\n",
              "                1.7220e-02, 6.8718e-05, 4.9179e-02, 1.9676e-04, 6.0212e-02, 3.3146e-02,\n",
              "                1.2350e-03, 2.4559e-02, 2.9444e-02, 2.1650e-02, 1.7116e-02, 5.0021e-02,\n",
              "                2.1532e-02, 8.1266e-05, 1.9449e-02, 1.3626e-03, 2.7855e-02, 2.1628e-03,\n",
              "                6.8794e-02, 1.6577e-02, 2.2417e-04, 1.6988e-03, 4.7794e-02, 2.0219e-03,\n",
              "                1.6122e-02, 7.8751e-05, 1.9044e-04, 2.4319e-02, 3.3900e-02, 3.6510e-02,\n",
              "                2.6098e-02, 4.5220e-02, 4.0736e-02, 5.9879e-04, 1.6653e-03, 1.9106e-02,\n",
              "                4.3404e-02, 1.0890e-02, 1.4805e-02, 2.7073e-02, 2.0175e-02, 3.5884e-02,\n",
              "                2.5138e-03, 8.3180e-02, 6.9221e-02, 4.1114e-04])\n",
              "      )\n",
              "      (activation_post_process): HistogramObserver(min_val=0.0, max_val=36.5031623840332)\n",
              "    )\n",
              "    (relu3): Identity()\n",
              "    (fc2): Linear(\n",
              "      in_features=100, out_features=9, bias=True\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "        min_val=tensor([-0.1703, -0.1579, -0.1449, -0.1112, -0.0981, -0.1640, -0.1413, -0.2273,\n",
              "                -0.1390]), max_val=tensor([0.1219, 0.1498, 0.0921, 0.1422, 0.1043, 0.1697, 0.1137, 0.1024, 0.0953])\n",
              "      )\n",
              "      (activation_post_process): HistogramObserver(min_val=-32.667640686035156, max_val=18.487957000732422)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
      ],
      "metadata": {
        "id": "OSDa23-UKOyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using high-level static quantization wrapper\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=\"./\", model_filename=\"qat_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO-sHf_yK9ha",
        "outputId": "754aa99f-8880-4996-b042-bdaf59ea0efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedCNN(\n",
            "  (quant): Quantize(scale=tensor([0.3596]), zero_point=tensor([52]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): cnn2d(\n",
            "    (conv1): QuantizedConvReLU2d(40, 50, kernel_size=(5, 5), stride=(1, 1), scale=0.10366735607385635, zero_point=0)\n",
            "    (relu1): Identity()\n",
            "    (conv2): QuantizedConvReLU2d(50, 100, kernel_size=(5, 5), stride=(1, 1), scale=0.2531552016735077, zero_point=0)\n",
            "    (relu2): Identity()\n",
            "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (fc1): QuantizedLinearReLU(in_features=2500, out_features=100, scale=0.25121748447418213, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "    (relu3): Identity()\n",
            "    (fc2): QuantizedLinear(in_features=100, out_features=9, scale=0.37192127108573914, zero_point=83, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=\"./qat_model.pth\", device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_dataloader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_dataloader, device=cpu_device, criterion=None)\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkEror9uK9kH",
        "outputId": "ed4456f4-ccbb-4c9e-d647-040920099d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 evaluation accuracy: 0.996\n",
            "INT8 evaluation accuracy: 0.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,num_components, window, window), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtIFEE8lK9m-",
        "outputId": "cea875b5-eebb-4b6e-eab1-54da95d2c7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 CPU Inference Latency: 1.08 ms / sample\n",
            "FP32 CUDA Inference Latency: 0.45 ms / sample\n",
            "INT8 CPU Inference Latency: 0.73 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 0.46 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the sizes\n",
        "f=print_size_of_model(model,\"fp32\")\n",
        "q=print_size_of_model(quantized_model,\"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uV6O5VBK9pN",
        "outputId": "011fb1c3-a417-4ab6-fe16-eaa4bd4c4b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32  \t Size (MB): 1.707249\n",
            "model:  int8  \t Size (MB): 0.438659\n",
            "3.89 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42G02Vv3LUCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NqF7nBDLUFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXXkiLOfLUIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkF6LWM6LUK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}