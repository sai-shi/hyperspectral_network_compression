{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8MVvYkwPUsH"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import math, time\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UWpj8baGdEY"
      },
      "outputs": [],
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyK_NRHiGdHi"
      },
      "outputs": [],
      "source": [
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "\n",
        "    return patchesData, patchesLabels.astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0mC8PYPd6f"
      },
      "outputs": [],
      "source": [
        "data = envi.open('./drive/MyDrive/HSI-datasets/IP_DataSet/indianpines_ds_raw.hdr', \\\n",
        "                './drive/MyDrive/HSI-datasets/IP_DataSet/indianpines_ds_raw.raw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp5B8r7wPsJY"
      },
      "outputs": [],
      "source": [
        "disjoint_pixels = envi.open('./drive/MyDrive/HSI-datasets/IP_TrainSet/indianpines_ts_raw_classes.hdr', \\\n",
        "                './drive/MyDrive/HSI-datasets/IP_TrainSet/indianpines_ts_raw_classes.raw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ7MKLXSPsLz"
      },
      "outputs": [],
      "source": [
        "disjoint_pixels = disjoint_pixels.read_bands([0])\n",
        "disjoint_pixels = disjoint_pixels.reshape(disjoint_pixels.shape[0], disjoint_pixels.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FWA7sOfVLfu"
      },
      "outputs": [],
      "source": [
        "y_train = copy.deepcopy(disjoint_pixels)\n",
        "for i, val in enumerate([0,2,3,5,6,8,10,11,12,14,1,4,7,9,13,15,16]): y_train[disjoint_pixels==i] = val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "aixUM7b0QBak",
        "outputId": "931d1c62-3a74-4c06-ece6-12aff5063e15"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the grid\n",
        "plt.imshow(y_train, cmap='bwr')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6yYoYl5RIUp"
      },
      "outputs": [],
      "source": [
        "labels = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/indian_pines_gt.mat')\n",
        "labels = labels['indian_pines_gt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "hUds9I73RT4M",
        "outputId": "f5e3f6b7-026b-4225-a45b-830e6767ead7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the grid\n",
        "plt.imshow(y_test, cmap='bwr')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ['India', 'Pavia'][1]\n",
        "random_split = True"
      ],
      "metadata": {
        "id": "DV_v3qQHdkLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == 'India':\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/indian_pines_corrected.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indianpines_disjoint_dset.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/indian_pines_gt.mat')\n",
        "\n",
        "    X_all = raw_data['indian_pines_corrected']\n",
        "    y_disjoint = disjoint_data['indianpines_disjoint_dset']\n",
        "    y_all = all_labels['indian_pines_gt']\n",
        "\n",
        "    test_ratio = 0.45\n",
        "else:\n",
        "    raw_data = scipy.io.loadmat('./drive/MyDrive/HSI-datasets/paviaU.mat')\n",
        "    disjoint_data = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/TRpavia_fixed.mat')\n",
        "    all_labels = scipy.io.loadmat(os.getcwd() + '/drive/MyDrive/HSI-datasets/paviaU_gt.mat')\n",
        "\n",
        "    X_all = raw_data['paviaU']\n",
        "    y_disjoint = disjoint_data['TRpavia_fixed']\n",
        "    y_all = all_labels['paviaU_gt']\n",
        "\n",
        "    test_ratio = 0.93"
      ],
      "metadata": {
        "id": "esT48HKRd1N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uci233aici8G"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nYjGrFZci-u"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quGUfFsw5HoJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq60eCjE3uN2"
      },
      "outputs": [],
      "source": [
        "num_channels = X_all.shape[-1]\n",
        "W = X_all.shape[0]\n",
        "H = X_all.shape[1]\n",
        "num_components = 40\n",
        "window = 19\n",
        "num_classes = len(np.unique(y_all)) - 1 # Ignore label=0 (undefined class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfy8EN34aYpI"
      },
      "outputs": [],
      "source": [
        "X_all = X_all.reshape(W*H, num_channels)\n",
        "y_disjoint = y_disjoint.reshape(-1, 1).flatten()\n",
        "\n",
        "X_all = X_all.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC7d6YQN7aI8"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=num_components)\n",
        "X_all = pca.fit_transform(X_all)\n",
        "\n",
        "X_all = X_all.reshape(W, H, num_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oB6lWLu8HO7"
      },
      "outputs": [],
      "source": [
        "X, y = createImageCubes(X_all, y_all, windowSize=window, removeZeroLabels = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxzb3j0_oBWl"
      },
      "outputs": [],
      "source": [
        "np.save('data.npy', X)\n",
        "np.save('labels.npy', y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF7Ll8wSagy9"
      },
      "outputs": [],
      "source": [
        "del X_all, y_all, raw_data, X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SC0qR7soTS7"
      },
      "outputs": [],
      "source": [
        "X = np.load('data.npy', mmap_mode='c')\n",
        "y = np.load('labels.npy', mmap_mode='c')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQbljf-2-DAf"
      },
      "outputs": [],
      "source": [
        "X = np.transpose(X, (0, 3, 1, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSszW6uQFKcW",
        "outputId": "4434854f-7bdf-4a5e-f866-d9f07525de29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(207400, 40, 19, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLayt2rYcVVN"
      },
      "outputs": [],
      "source": [
        "if random_split:\n",
        "    nonzero_idx = np.where(y != 0)\n",
        "    X = X[nonzero_idx]\n",
        "    y = y[nonzero_idx]\n",
        "    y = y - 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
        "else:\n",
        "    train_idx = np.where((y_disjoint != 0) & (y != 0))\n",
        "    X_train = X[train_idx]\n",
        "    y_train = y[train_idx]\n",
        "\n",
        "    test_idx = np.where((y_disjoint == 0) & (y != 0))\n",
        "    X_test = X[test_idx]\n",
        "    y_test = y[test_idx]\n",
        "\n",
        "    y_train -= 1\n",
        "    y_test -= 1\n",
        "\n",
        "    print('Disjoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEgdEKhfY6t3"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], num_components*window*window)\n",
        "X_test = X_test.reshape(X_test.shape[0], num_components*window*window)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], num_components, window, window)\n",
        "X_test = X_test.reshape(X_test.shape[0], num_components, window, window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXY841qwCwEU"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2anZduvxjhup",
        "outputId": "48319eb4-d954-4f94-874b-647aafdb23e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39782, 40, 19, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY6DCjbUcWIm"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.Tensor(X_train) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_train)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100) # create your dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvNX80DayaKZ"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.Tensor(X_val) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_val)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "val_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100) # create your dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm46r6mzcWKx"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_test)\n",
        "tensor_y = tensor_y.type(torch.LongTensor)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100) # create your dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjB1JL3BCR2j"
      },
      "outputs": [],
      "source": [
        "class cnn2d(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN2d network\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(0.5)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "    def __init__(self, input_channels, n_classes, filters=(50, 100), fc=100, bn=False):\n",
        "        super(cnn2d, self).__init__()\n",
        "        self.bn = bn\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=filters[0], kernel_size=(5, 5))\n",
        "        if bn:\n",
        "            self.bn1 = nn.BatchNorm2d(filters[0])\n",
        "        self.conv2 = nn.Conv2d(in_channels=filters[0], out_channels=filters[1], kernel_size=(5, 5))\n",
        "        if bn:\n",
        "            self.bn2 = nn.BatchNorm2d(filters[1])\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.fc1 = nn.Linear(int(25*filters[1]), fc)\n",
        "        if bn:\n",
        "            self.bn3 = nn.BatchNorm1d(fc)\n",
        "        self.fc2 = nn.Linear(fc, n_classes)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        if self.bn:\n",
        "            x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        if self.bn:\n",
        "            x = self.bn2(x)\n",
        "        x = self.pool(F.relu(x))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        if self.bn:\n",
        "            x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPz5J4F9QwPy"
      },
      "outputs": [],
      "source": [
        "__all__ = ['vgg']\n",
        "\n",
        "defaultcfg = {\n",
        "    2: [50, 100, 'M'],\n",
        "    11 : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
        "    13 : [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
        "    16 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n",
        "    19 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n",
        "}\n",
        "\n",
        "defaultfc = [100, 100]\n",
        "\n",
        "class vgg(nn.Module):\n",
        "    def __init__(self, depth=2, init_weights=True, cfg=None, fc=None, n_classes=16, kernel=5, padding=0):\n",
        "        super(vgg, self).__init__()\n",
        "\n",
        "        if cfg is None:\n",
        "            cfg = defaultcfg[depth]\n",
        "        if fc is not None:\n",
        "            fc = defaultfc\n",
        "        self.config = [cfg,fc]\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.feature = self.make_layers(cfg, kernel, padding, batch_norm=False)\n",
        "\n",
        "        step = kernel - 1 - (2*padding)\n",
        "        w = 19\n",
        "        for i in cfg:\n",
        "            if i != \"M\":\n",
        "                w -= step\n",
        "                last_conv = i\n",
        "            else:\n",
        "                w = w // 2\n",
        "        \n",
        "        if fc:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(int(w*w*last_conv), fc[0]),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(fc[0], fc[1]),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(fc[1], n_classes),\n",
        "                )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(int(w*w*last_conv), 100),\n",
        "                nn.Linear(100, n_classes),\n",
        "                )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def make_layers(self, cfg, kernel, padding, batch_norm=False):\n",
        "        layers = []\n",
        "        in_channels = num_components\n",
        "        for v in cfg:\n",
        "            if v == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=kernel, padding=padding, bias=False)\n",
        "                if batch_norm:\n",
        "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "                else:\n",
        "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "                in_channels = v\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        # x = nn.AvgPool2d(2)(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y = self.classifier(x)\n",
        "        return y\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(0.5)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "2z08zy5qqFaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "kcGIEH7nqFcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model):\n",
        "    \"\"\"validation\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input = input.float()\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(acc1[0], input.size(0))\n",
        "            top5.update(acc5[0], input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "Bfp9PFkvqks3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vanilla(epoch, train_loader, model, criterion, optimizer):\n",
        "    \"\"\"vanilla training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.float()\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # ===================forward=====================\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0], input.size(0))\n",
        "        top5.update(acc5[0], input.size(0))\n",
        "\n",
        "        # ===================backward=====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ===================meters=====================\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # tensorboard logger\n",
        "        pass\n",
        "\n",
        "        # print info\n",
        "        # if idx % print_freq == 0:\n",
        "        #     print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #           'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #           'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #           'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "        #           'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "        #            epoch, idx, len(train_loader), batch_time=batch_time,\n",
        "        #            data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "        #     sys.stdout.flush()\n",
        "\n",
        "    print(' * epoch {}: Losses {losses.avg:.3f} Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(epoch, losses=losses, top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, losses.avg"
      ],
      "metadata": {
        "id": "iqMkIk2Jqkvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_big_network(train_loader, val_loader, epochs, cfg, fc):\n",
        "    best_acc = 0\n",
        "\n",
        "    # model\n",
        "    model = cnn2d(num_components, num_classes, filters=cfg, fc=fc)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "\n",
        "        # print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model)\n",
        "\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './best_model.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        if epoch % save_freq == 0:\n",
        "            print('==> Saving...')\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = os.path.join('./', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './last_model.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "WGIpxeYMqk0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_small_network(model, train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "    best_model = None\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "\n",
        "        # print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model)\n",
        "\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_model = model\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './best_small_model.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './last_small_model.pth'\n",
        "    torch.save(state, save_file)\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "lA2LvBRAybC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_freq = 40\n",
        "print_freq = 200\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "learning_rate = 8e-4\n",
        "\n",
        "t_cfg = (50, 100) # dimention of teacher conv layers\n",
        "t_fc = 100 # dimention of teacher linear layers"
      ],
      "metadata": {
        "id": "S29NyptLqFf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_big_network(train_dataloader, val_dataloader, epochs=200, cfg=t_cfg, fc=t_fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ_H_0o9tETA",
        "outputId": "585502a3-687f-4ecc-c3c3-53ba0db7f69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 1.444 Acc@1 47.290 Acc@5 84.744\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.594 Acc@1 79.027 Acc@5 97.142\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.224 Acc@1 91.759 Acc@5 99.926\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.113 Acc@1 96.585 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.063 Acc@1 98.478 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.048 Acc@1 98.961 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.039 Acc@1 99.592 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 8: Losses 0.034 Acc@1 99.629 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 9: Losses 0.042 Acc@1 98.886 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.035 Acc@1 99.480 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.024 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.024 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.022 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.021 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.020 Acc@1 99.889 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 18: Losses 0.030 Acc@1 99.517 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.029 Acc@1 99.666 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.018 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.020 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.017 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.016 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.017 Acc@1 99.926 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 25: Losses 0.020 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.023 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.049 Acc@1 99.109 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 28: Losses 0.025 Acc@1 99.555 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.049 Acc@1 98.552 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.025 Acc@1 99.369 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.019 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.015 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 41: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.016 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.015 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.017 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.019 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.021 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.019 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.020 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 51: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.015 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.018 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.016 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.016 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.013 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.015 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.016 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.019 Acc@1 99.666 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.014 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.014 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.017 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.022 Acc@1 99.592 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 81: Losses 0.014 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.016 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.017 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.013 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 101: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 102: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 103: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 104: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 105: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 106: Losses 0.017 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 107: Losses 0.017 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 108: Losses 0.019 Acc@1 99.629 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 109: Losses 0.027 Acc@1 99.517 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 110: Losses 0.013 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 111: Losses 0.010 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 112: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 113: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 114: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 115: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 116: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 117: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 118: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 119: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 120: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 121: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 122: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 123: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 124: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 125: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 126: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 127: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 128: Losses 0.015 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 129: Losses 0.022 Acc@1 99.629 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 130: Losses 0.020 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 131: Losses 0.015 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 132: Losses 0.013 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 133: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 134: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 135: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 136: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 137: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 138: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 139: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 140: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 141: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 142: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 143: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 144: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 145: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 146: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 147: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 148: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 149: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 150: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 151: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 152: Losses 0.014 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 153: Losses 0.015 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 154: Losses 0.014 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 155: Losses 0.014 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 156: Losses 0.012 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 157: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 158: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 159: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 160: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 161: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 162: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 163: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 164: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 165: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 166: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 167: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 168: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 169: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 170: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 171: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 172: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 173: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 174: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 175: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 176: Losses 0.017 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 177: Losses 0.012 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 178: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 179: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 180: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 181: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 182: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 183: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 184: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 185: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 186: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 187: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 188: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 189: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 190: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 191: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 192: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 193: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 194: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 195: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 196: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 197: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 198: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 199: Losses 0.013 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 200: Losses 0.015 Acc@1 99.814 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn2d(num_components, num_classes, filters=t_cfg, fc=t_fc)\n",
        "model.load_state_dict(torch.load('./best_model.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8A-bZwwubTT",
        "outputId": "601fe224-31a3-4c9e-a00c-fd9542aa9b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "t_acc_1, t_acc_5, t_loss = validate(test_dataloader, model)"
      ],
      "metadata": {
        "id": "EPEGWwUYubWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Big top-1 test accuracy: {:.3f}\".format(t_acc_1))\n",
        "print(\"Big top-5 test accuracy: {:.3f}\".format(t_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld60pWAOuhCg",
        "outputId": "8f725527-cfe2-4b35-81d3-f6532171494d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Big top-1 test accuracy: 99.256\n",
            "Big top-5 test accuracy: 100.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97ZeOhpJPSJf",
        "outputId": "eb52b04f-dd5a-454f-e968-91b61ca705a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 50000\n",
            "conv1.bias 50\n",
            "conv2.weight 125000\n",
            "conv2.bias 100\n",
            "fc1.weight 250000\n",
            "fc1.bias 100\n",
            "fc2.weight 900\n",
            "fc2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_r = 0.89  # prune ratio of conv layers\n",
        "linear_r = 0.89 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "O7kmQl6jDn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_big_network(train_dataloader, val_dataloader, epochs=200, cfg=s_cfg, fc=s_fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX6Vg9NUDpBL",
        "outputId": "77997209-6cfb-454f-de4b-6734760f8544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 1.912 Acc@1 41.759 Acc@5 78.248\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 1.443 Acc@1 43.690 Acc@5 86.229\n",
            "==> training...\n",
            " * epoch 3: Losses 1.311 Acc@1 44.209 Acc@5 86.229\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 1.232 Acc@1 52.450 Acc@5 86.229\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 1.092 Acc@1 60.987 Acc@5 89.235\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.882 Acc@1 67.112 Acc@5 93.207\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 7: Losses 0.762 Acc@1 69.785 Acc@5 95.805\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 8: Losses 0.703 Acc@1 71.121 Acc@5 96.622\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 9: Losses 0.660 Acc@1 72.383 Acc@5 97.439\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 10: Losses 0.621 Acc@1 74.350 Acc@5 97.996\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 11: Losses 0.582 Acc@1 74.944 Acc@5 98.404\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 12: Losses 0.544 Acc@1 76.281 Acc@5 99.035\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 13: Losses 0.505 Acc@1 78.656 Acc@5 99.258\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 14: Losses 0.463 Acc@1 81.180 Acc@5 99.555\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 15: Losses 0.405 Acc@1 84.298 Acc@5 99.777\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 16: Losses 0.347 Acc@1 86.711 Acc@5 99.852\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 17: Losses 0.294 Acc@1 89.310 Acc@5 99.889\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 18: Losses 0.248 Acc@1 91.240 Acc@5 99.889\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 19: Losses 0.211 Acc@1 92.984 Acc@5 99.889\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 20: Losses 0.183 Acc@1 93.987 Acc@5 99.926\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 21: Losses 0.161 Acc@1 95.026 Acc@5 99.926\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 22: Losses 0.143 Acc@1 95.805 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 23: Losses 0.128 Acc@1 96.399 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 24: Losses 0.113 Acc@1 96.882 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 25: Losses 0.102 Acc@1 97.105 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 26: Losses 0.093 Acc@1 97.439 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 27: Losses 0.086 Acc@1 97.587 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 28: Losses 0.080 Acc@1 97.736 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 29: Losses 0.076 Acc@1 97.996 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 30: Losses 0.073 Acc@1 98.107 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 31: Losses 0.070 Acc@1 98.070 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.067 Acc@1 98.181 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 33: Losses 0.067 Acc@1 98.293 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.066 Acc@1 98.367 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 35: Losses 0.067 Acc@1 98.330 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 36: Losses 0.068 Acc@1 98.144 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 37: Losses 0.069 Acc@1 98.293 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 38: Losses 0.065 Acc@1 98.441 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 39: Losses 0.059 Acc@1 98.701 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 40: Losses 0.053 Acc@1 98.775 Acc@5 99.963\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 41: Losses 0.048 Acc@1 99.183 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 42: Losses 0.047 Acc@1 99.220 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 43: Losses 0.046 Acc@1 99.295 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 44: Losses 0.046 Acc@1 99.295 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 45: Losses 0.045 Acc@1 99.332 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 46: Losses 0.044 Acc@1 99.332 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 47: Losses 0.044 Acc@1 99.369 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 48: Losses 0.043 Acc@1 99.332 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 49: Losses 0.043 Acc@1 99.369 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 50: Losses 0.042 Acc@1 99.406 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 51: Losses 0.042 Acc@1 99.406 Acc@5 99.963\n",
            "==> training...\n",
            " * epoch 52: Losses 0.041 Acc@1 99.480 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.040 Acc@1 99.443 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.040 Acc@1 99.517 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.039 Acc@1 99.480 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.038 Acc@1 99.555 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 57: Losses 0.038 Acc@1 99.555 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.037 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.037 Acc@1 99.592 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 60: Losses 0.037 Acc@1 99.555 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.036 Acc@1 99.666 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.036 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.035 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.035 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.034 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.034 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.033 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.033 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.032 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.032 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.032 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.032 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.031 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.031 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.030 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.030 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.030 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.029 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.029 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.029 Acc@1 99.852 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 81: Losses 0.029 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.029 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.028 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.028 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.028 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.028 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.028 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.027 Acc@1 99.889 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 89: Losses 0.028 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.027 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.027 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.027 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.027 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.027 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.027 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.027 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.026 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.026 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.026 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.026 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 101: Losses 0.026 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 102: Losses 0.025 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 103: Losses 0.025 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 104: Losses 0.025 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 105: Losses 0.025 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 106: Losses 0.025 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 107: Losses 0.024 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 108: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 109: Losses 0.024 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 110: Losses 0.023 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 111: Losses 0.023 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 112: Losses 0.023 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 113: Losses 0.023 Acc@1 99.926 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 114: Losses 0.023 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 115: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 116: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 117: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 118: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 119: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 120: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 121: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 122: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 123: Losses 0.022 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 124: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 125: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 126: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 127: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 128: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 129: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 130: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 131: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 132: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 133: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 134: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 135: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 136: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 137: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 138: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 139: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 140: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 141: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 142: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 143: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 144: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 145: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 146: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 147: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 148: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 149: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 150: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 151: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 152: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 153: Losses 0.021 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 154: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 155: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 156: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 157: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 158: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 159: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 160: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> Saving...\n",
            "==> training...\n",
            " * epoch 161: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 162: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 163: Losses 0.019 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 164: Losses 0.020 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 165: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 166: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 167: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 168: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 169: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 170: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 171: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 172: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 173: Losses 0.021 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 174: Losses 0.022 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 175: Losses 0.023 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 176: Losses 0.020 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 177: Losses 0.022 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 178: Losses 0.022 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 179: Losses 0.024 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 180: Losses 0.023 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 181: Losses 0.022 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 182: Losses 0.025 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 183: Losses 0.028 Acc@1 99.406 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 184: Losses 0.025 Acc@1 99.517 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 185: Losses 0.025 Acc@1 99.666 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 186: Losses 0.021 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 187: Losses 0.019 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 188: Losses 0.017 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 189: Losses 0.017 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 190: Losses 0.018 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 191: Losses 0.018 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 192: Losses 0.018 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 193: Losses 0.019 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 194: Losses 0.018 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 195: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 196: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 197: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 198: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 199: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 200: Losses 0.018 Acc@1 99.963 Acc@5 100.000\n",
            "==> Saving...\n",
            "best accuracy: tensor(99.3333, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_model = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc)\n",
        "s_model.load_state_dict(torch.load('./best_model.pth')['model'])\n",
        "s_model = s_model.to(device)"
      ],
      "metadata": {
        "id": "rQml_zzqEfR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, s_model)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wJKyv39DpD4",
        "outputId": "f5d496be-3b9d-4434-e7cb-f7c9adce53d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 98.137\n",
            "Small top-5 test accuracy: 99.997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5PYziW_EeaW"
      },
      "source": [
        "# **L1-norm magnitude pruning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEnXdyMLEiZw"
      },
      "outputs": [],
      "source": [
        "def create_conv_mask(model, cfg):\n",
        "    cfg_mask = []\n",
        "    layer_id = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            out_channels = m.weight.data.shape[0]\n",
        "            if out_channels == cfg[layer_id]:\n",
        "                cfg_mask.append(torch.ones(out_channels))\n",
        "                layer_id += 1\n",
        "                continue\n",
        "            weight_copy = m.weight.data.abs().clone()\n",
        "            weight_copy = weight_copy.cpu().numpy()\n",
        "            L1_norm = np.sum(weight_copy, axis=(1, 2, 3))\n",
        "            arg_max = np.argsort(L1_norm)\n",
        "            arg_max_rev = arg_max[::-1][:cfg[layer_id]]\n",
        "            assert arg_max_rev.size == cfg[layer_id], \"size of arg_max_rev not correct\"\n",
        "            mask = torch.zeros(out_channels)\n",
        "            mask[arg_max_rev.tolist()] = 1\n",
        "            cfg_mask.append(mask)\n",
        "            layer_id += 1\n",
        "        elif isinstance(m, nn.MaxPool2d):\n",
        "            layer_id += 1\n",
        "    \n",
        "    return cfg_mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fc_mask(model, fc):\n",
        "    layer_id = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            weight_copy = m.weight.data.abs().clone()\n",
        "            weight_copy = weight_copy.cpu().numpy()\n",
        "            L1_norm = np.sum(weight_copy, axis=0)\n",
        "            arg_max = np.argsort(L1_norm)\n",
        "            #alive_param_num = int(weight_copy.shape[1]*(1-linear_r))\n",
        "            arg_max_rev = arg_max[::-1][:fc]\n",
        "    return arg_max_rev"
      ],
      "metadata": {
        "id": "DsA5CvvnmTYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji4TQqPQFyh9"
      },
      "outputs": [],
      "source": [
        "def copy_pruned_weights(cfg_mask, fc_mask, model, newmodel):\n",
        "    start_mask = torch.ones(num_components)\n",
        "    layer_id_in_cfg = 0\n",
        "    end_mask = cfg_mask[layer_id_in_cfg]\n",
        "\n",
        "    for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "        if isinstance(m0, nn.BatchNorm2d):\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "            if idx1.size == 1:\n",
        "                idx1 = np.resize(idx1,(1,))\n",
        "            m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "            m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "            m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            start_mask = end_mask\n",
        "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "                end_mask = cfg_mask[layer_id_in_cfg]\n",
        "        elif isinstance(m0, nn.Conv2d):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "            print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            if idx1.size == 1:\n",
        "                idx1 = np.resize(idx1, (1,))\n",
        "            w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "            w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "            m1.weight.data = w1.clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            start_mask = end_mask\n",
        "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "                end_mask = cfg_mask[layer_id_in_cfg]\n",
        "        elif isinstance(m0, nn.Linear):\n",
        "            #print(m1.weight.data.shape, m0.weight.data.shape, m1.bias.data.shape, m0.bias.data.shape)\n",
        "            if layer_id_in_cfg == len(cfg_mask):\n",
        "                idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "                if idx0.size == 1:\n",
        "                    idx0 = np.resize(idx0, (1,))\n",
        "\n",
        "                k_idx0 = []\n",
        "                for k in idx0:\n",
        "                    new_k = range(k*25, (k+1)*25)\n",
        "                    k_idx0 += list(new_k)\n",
        "                # m1.weight.data = m0.weight.data[:, k_idx0].clone()\n",
        "                # m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "                w = m0.weight.data[:, k_idx0].clone()\n",
        "                m1.weight.data = w[fc_mask.tolist()].clone()\n",
        "                m1.bias.data = m0.bias.data[fc_mask.tolist()].clone()\n",
        "                layer_id_in_cfg += 1\n",
        "                continue\n",
        "\n",
        "            m1.weight.data = m0.weight.data[:, fc_mask.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "        elif isinstance(m0, nn.BatchNorm1d):\n",
        "            m1.weight.data = m0.weight.data.clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "            m1.running_mean = m0.running_mean.clone()\n",
        "            m1.running_var = m0.running_var.clone()\n",
        "\n",
        "    return newmodel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_r = 0.89  # prune ratio of conv layers\n",
        "linear_r = 0.89 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "Luw4cgkDOkeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk8HeWinNe4B"
      },
      "outputs": [],
      "source": [
        "cfg_mask = create_conv_mask(model, s_cfg)\n",
        "fc_mask = create_fc_mask(model, s_fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgd0QAQNRRdC",
        "outputId": "0bd27896-5753-4eb6-fcc7-93e9761e57e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn2d(\n",
            "  (conv1): Conv2d(40, 5, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(5, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=250, out_features=10, bias=True)\n",
            "  (fc2): Linear(in_features=10, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "newmodel = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "print(newmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkHAyAw_GHGj",
        "outputId": "273321f1-cdba-419d-9bd8-5afd7a72fd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In shape: 40, Out shape 5.\n",
            "In shape: 5, Out shape 10.\n"
          ]
        }
      ],
      "source": [
        "newmodel = copy_pruned_weights(cfg_mask, fc_mask, model, newmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01K3VlbOH_uC",
        "outputId": "50ec200c-5c98-4b77-ff11-b39a6bc8649b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 15.892\n",
            "Small top-5 test accuracy: 42.615\n"
          ]
        }
      ],
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newmodel = train_small_network(newmodel, train_dataloader, val_dataloader, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71QqXzdHyvOx",
        "outputId": "addc208b-b806-43fa-d53f-ad976575d862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 1.929 Acc@1 22.866 Acc@5 79.213\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 1.368 Acc@1 63.400 Acc@5 95.991\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.801 Acc@1 79.733 Acc@5 98.849\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.355 Acc@1 89.532 Acc@5 99.777\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.224 Acc@1 92.836 Acc@5 99.889\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.175 Acc@1 94.395 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 7: Losses 0.141 Acc@1 95.917 Acc@5 99.963\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 8: Losses 0.117 Acc@1 96.696 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 9: Losses 0.100 Acc@1 97.179 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 10: Losses 0.087 Acc@1 97.587 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 11: Losses 0.077 Acc@1 98.107 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.068 Acc@1 98.589 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.061 Acc@1 98.664 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.056 Acc@1 98.701 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 15: Losses 0.052 Acc@1 98.812 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.049 Acc@1 98.998 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.046 Acc@1 99.109 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.044 Acc@1 99.183 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 19: Losses 0.042 Acc@1 99.258 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.040 Acc@1 99.369 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.038 Acc@1 99.443 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.037 Acc@1 99.480 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.035 Acc@1 99.480 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.034 Acc@1 99.517 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.033 Acc@1 99.517 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.032 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.032 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.031 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.030 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.030 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.029 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.029 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.028 Acc@1 99.703 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 34: Losses 0.028 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.027 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.027 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.027 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.026 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.026 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.026 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.026 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.025 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.025 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.025 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.023 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.023 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.023 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.023 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.023 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.023 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.023 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.022 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.022 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.022 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.021 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.020 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.020 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.019 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.019 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.019 Acc@1 99.926 Acc@5 100.000\n",
            "best accuracy: tensor(97.3333, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in newmodel.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n52IcjOyo9on",
        "outputId": "13680569-1922-4e3f-b2ec-73af3599c63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 4000\n",
            "conv1.bias 4\n",
            "conv2.weight 10000\n",
            "conv2.bias 100\n",
            "fc1.weight 250000\n",
            "fc1.bias 100\n",
            "fc2.weight 900\n",
            "fc2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg56YpxXLQi6",
        "outputId": "3458bccf-b69a-4e5e-fb7a-ad1d93ca65f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size big network(MB): 5.123307\n",
            "Size small network(MB): 0.115709\n"
          ]
        }
      ],
      "source": [
        "print('Size big network(MB):', os.path.getsize(\"best_model.pth\")/1e6)\n",
        "print('Size small network(MB):', os.path.getsize(\"best_small_model.pth\")/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MMM_L6LQn2",
        "outputId": "2e56f01d-b962-4e77-e524-4ade8cf1d2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of big network parameters: 426159\n",
            "Num of small network parameters: 265113\n"
          ]
        }
      ],
      "source": [
        "print('Num of big network parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print('Num of small network parameters:', sum(p.numel() for p in newmodel.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8FZrCc5zOlv",
        "outputId": "f462eb92-5f2c-4328-a4fc-6e6a9ff928e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 98.276\n",
            "Small top-5 test accuracy: 99.950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LoGlgFHHDhW",
        "outputId": "a4369bcc-f1dd-4525-a85f-e5e2b93159c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In shape: 40, Out shape 5.\n",
            "In shape: 5, Out shape 100.\n",
            "==> training...\n",
            " * epoch 1: Losses 0.559 Acc@1 83.143 Acc@5 97.930\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.141 Acc@1 96.530 Acc@5 99.961\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.075 Acc@1 98.600 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.057 Acc@1 99.054 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.048 Acc@1 99.428 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.043 Acc@1 99.507 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 7: Losses 0.040 Acc@1 99.566 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.038 Acc@1 99.586 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 9: Losses 0.037 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.036 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.035 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.034 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.033 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.033 Acc@1 99.685 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 15: Losses 0.033 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.033 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.032 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.032 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.032 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.031 Acc@1 99.783 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 21: Losses 0.032 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.031 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.032 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.032 Acc@1 99.625 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 25: Losses 0.032 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.033 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.033 Acc@1 99.547 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.033 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.033 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.032 Acc@1 99.586 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 31: Losses 0.032 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.031 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.030 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.029 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.029 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.029 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.029 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.029 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.028 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.029 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.029 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.028 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.029 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.029 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.030 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.029 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.028 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.028 Acc@1 99.744 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 61: Losses 0.028 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.028 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.028 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.028 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 70: Losses 0.027 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.028 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.027 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "best accuracy: tensor(99.6454, device='cuda:0')\n",
            "In shape: 40, Out shape 5.\n",
            "In shape: 5, Out shape 10.\n",
            "==> training...\n",
            " * epoch 1: Losses 0.034 Acc@1 99.606 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.028 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 3: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 4: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 6: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 9: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 13: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.026 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.027 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.025 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.025 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.025 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.025 Acc@1 99.882 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.026 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.025 Acc@1 99.901 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.025 Acc@1 99.882 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.025 Acc@1 99.901 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.025 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.026 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.025 Acc@1 99.901 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.025 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.024 Acc@1 99.921 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.025 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.028 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.026 Acc@1 99.744 Acc@5 100.000\n",
            "best accuracy: tensor(99.4681, device='cuda:0')\n",
            "In shape: 40, Out shape 5.\n",
            "In shape: 5, Out shape 10.\n",
            "==> training...\n",
            " * epoch 1: Losses 0.214 Acc@1 96.806 Acc@5 99.921\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.066 Acc@1 99.073 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.045 Acc@1 99.330 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.037 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 5: Losses 0.033 Acc@1 99.704 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.031 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.030 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.030 Acc@1 99.744 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 9: Losses 0.029 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.029 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.029 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 14: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.027 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.027 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 85: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.026 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.026 Acc@1 99.862 Acc@5 100.000\n",
            "best accuracy: tensor(99.2908, device='cuda:0')\n",
            "Small top-1 test accuracy: 99.241\n",
            "Small top-5 test accuracy: 99.978\n"
          ]
        }
      ],
      "source": [
        "# Prune iteratively: layer by layer\n",
        "conv_r = 0.89\n",
        "linear_r = 0.89\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), t_cfg[1], 'M']\n",
        "s_fc = t_fc\n",
        "\n",
        "cfg_mask = create_conv_mask(model, s_cfg)\n",
        "fc_mask = create_fc_mask(model, s_fc)\n",
        "newmodel = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "\n",
        "newmodel = copy_pruned_weights(cfg_mask, fc_mask, model, newmodel)\n",
        "train_small_network(newmodel, train_dataloader, val_dataloader, epochs=100)\n",
        "#s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = t_fc\n",
        "\n",
        "cfg_mask = create_conv_mask(newmodel, s_cfg)\n",
        "fc_mask = create_fc_mask(newmodel, s_fc)\n",
        "\n",
        "newmodel2 = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "newmodel2 = copy_pruned_weights(cfg_mask, fc_mask, newmodel, newmodel2)\n",
        "\n",
        "train_small_network(newmodel2, train_dataloader, val_dataloader, epochs=100)\n",
        "#s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel2)\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))\n",
        "\n",
        "cfg_mask = create_conv_mask(newmodel2, s_cfg)\n",
        "fc_mask = create_fc_mask(newmodel2, s_fc)\n",
        "newmodel3 = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "\n",
        "newmodel3 = copy_pruned_weights(cfg_mask, fc_mask, newmodel2, newmodel3)\n",
        "train_small_network(newmodel3, train_dataloader, val_dataloader, epochs=100)\n",
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel3)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of small network parameters:', sum(p.numel() for p in newmodel3.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA0GxjHPmGDC",
        "outputId": "02b987de-c2f6-4d1a-b5e2-88f02256065e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of small network parameters: 8951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3-bZK_sOCEw",
        "outputId": "9a122f30-1a19-4226-95db-e9a9c1ce1ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In shape: 40, Out shape 25.\n",
            "In shape: 25, Out shape 50.\n",
            "==> training...\n",
            " * epoch 1: Losses 0.041 Acc@1 99.231 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.033 Acc@1 99.547 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.030 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 4: Losses 0.029 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 5: Losses 0.031 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 6: Losses 0.031 Acc@1 99.487 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.030 Acc@1 99.547 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.032 Acc@1 99.468 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 9: Losses 0.033 Acc@1 99.487 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.032 Acc@1 99.448 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.029 Acc@1 99.645 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 12: Losses 0.028 Acc@1 99.547 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.029 Acc@1 99.566 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.028 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.028 Acc@1 99.606 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.029 Acc@1 99.487 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.027 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.028 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.031 Acc@1 99.606 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.028 Acc@1 99.527 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.026 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.027 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.030 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.031 Acc@1 99.527 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.027 Acc@1 99.625 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 26: Losses 0.026 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.028 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.027 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.025 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.027 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.028 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.025 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.026 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.025 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.026 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.028 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.027 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.025 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.026 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.027 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.027 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.027 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.026 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.027 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.025 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.025 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.027 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.027 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.025 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.026 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.027 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.026 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.026 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.027 Acc@1 99.625 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.026 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.024 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.027 Acc@1 99.645 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.026 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.026 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.026 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.024 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.025 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.025 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.028 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.025 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.025 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.024 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.025 Acc@1 99.744 Acc@5 100.000\n",
            "best accuracy: tensor(99.1135, device='cuda:0')\n",
            "In shape: 40, Out shape 15.\n",
            "In shape: 15, Out shape 30.\n",
            "==> training...\n",
            " * epoch 1: Losses 0.029 Acc@1 99.645 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.024 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 3: Losses 0.026 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 4: Losses 0.025 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 5: Losses 0.025 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 6: Losses 0.023 Acc@1 99.862 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.025 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 9: Losses 0.024 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 15: Losses 0.024 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.024 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.024 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.025 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.023 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 41: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.023 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.024 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.024 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.024 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.024 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.023 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.024 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.023 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.023 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.023 Acc@1 99.842 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.023 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.023 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.023 Acc@1 99.823 Acc@5 100.000\n",
            "best accuracy: tensor(98.9362, device='cuda:0')\n",
            "In shape: 40, Out shape 5.\n",
            "In shape: 5, Out shape 10.\n",
            "==> training...\n",
            " * epoch 1: Losses 0.800 Acc@1 79.673 Acc@5 94.657\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.234 Acc@1 93.809 Acc@5 99.409\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.132 Acc@1 96.826 Acc@5 99.961\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.087 Acc@1 98.166 Acc@5 99.961\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.065 Acc@1 98.817 Acc@5 99.980\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.053 Acc@1 99.172 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 7: Losses 0.046 Acc@1 99.349 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 8: Losses 0.042 Acc@1 99.507 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 9: Losses 0.039 Acc@1 99.586 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.038 Acc@1 99.586 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.036 Acc@1 99.606 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 12: Losses 0.035 Acc@1 99.625 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 13: Losses 0.034 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.034 Acc@1 99.665 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.033 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.033 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.032 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.032 Acc@1 99.685 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.031 Acc@1 99.704 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.031 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.031 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.031 Acc@1 99.724 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.030 Acc@1 99.744 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.030 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.030 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.030 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.030 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.030 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.030 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.029 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.030 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.029 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.029 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.029 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.029 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.028 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.028 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.028 Acc@1 99.763 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 88: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.027 Acc@1 99.783 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.027 Acc@1 99.823 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.026 Acc@1 99.803 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.027 Acc@1 99.803 Acc@5 100.000\n",
            "best accuracy: tensor(98.7589, device='cuda:0')\n",
            "Small top-1 test accuracy: 99.241\n",
            "Small top-5 test accuracy: 99.978\n"
          ]
        }
      ],
      "source": [
        "# Multi-pass scheme pruning\n",
        "conv_r = 0.5\n",
        "linear_r = 0.5\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))\n",
        "\n",
        "cfg_mask = create_conv_mask(model, s_cfg)\n",
        "fc_mask = create_fc_mask(model, s_fc)\n",
        "newmodel = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "\n",
        "newmodel = copy_pruned_weights(cfg_mask, fc_mask, model, newmodel)\n",
        "train_small_network(newmodel, train_dataloader, val_dataloader, epochs=100)\n",
        "#s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "\n",
        "conv_r = 0.7\n",
        "linear_r = 0.7\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))\n",
        "\n",
        "cfg_mask = create_conv_mask(newmodel, s_cfg)\n",
        "fc_mask = create_fc_mask(newmodel, s_fc)\n",
        "newmodel2 = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "\n",
        "newmodel2 = copy_pruned_weights(cfg_mask, fc_mask, newmodel, newmodel2)\n",
        "train_small_network(newmodel2, train_dataloader, val_dataloader, epochs=100)\n",
        "#s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel2)\n",
        "\n",
        "conv_r = 0.89\n",
        "linear_r = 0.89\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))\n",
        "\n",
        "cfg_mask = create_conv_mask(newmodel2, s_cfg)\n",
        "fc_mask = create_fc_mask(newmodel2, s_fc)\n",
        "newmodel3 = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)\n",
        "\n",
        "newmodel3 = copy_pruned_weights(cfg_mask, fc_mask, newmodel2, newmodel3)\n",
        "train_small_network(newmodel3, train_dataloader, val_dataloader, epochs=100)\n",
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel3)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXwRfVVT4k45"
      },
      "source": [
        "# **ThiNet Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpfEDyh2ei5I"
      },
      "outputs": [],
      "source": [
        "def generate_windows(layer,last_output):\n",
        "    output_h = last_output.shape[2]\n",
        "    output_w = last_output.shape[3]\n",
        "    windows_size_h = layer.kernel_size[0]\n",
        "    windows_size_w = layer.kernel_size[1]\n",
        "    H = output_h -windows_size_h\n",
        "    W = output_w -windows_size_w\n",
        "    if H < 0: H = 0\n",
        "    if W < 0: W = 0\n",
        "    x1 = np.random.randint(0,H + 1)\n",
        "    x2 = x1 + windows_size_h\n",
        "    y1 = np.random.randint(0,W + 1)\n",
        "    y2 = y1 + windows_size_w\n",
        "    s_w = last_output[:,:,x1:x2,y1:y2]\n",
        "    return s_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAZkgFCX4aSd"
      },
      "outputs": [],
      "source": [
        "# paper: ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression\n",
        "# https://github.com/SSriven/ThiNet\n",
        "def get_T(s_w,w,r):\n",
        "    \n",
        "    T = []\n",
        "    C = s_w.shape[1]\n",
        "    I = [i for i in range(C)]\n",
        "    pad = 0\n",
        "    if s_w.shape[2] < w.shape[2]:\n",
        "        pad = w.shape[2] - s_w.shape[2]\n",
        "    s_w = np.pad(s_w.detach().numpy(),((0,0),(0,0),(0,pad),(0,pad)))\n",
        "    w = w.detach().numpy()\n",
        "\n",
        "    while len(T) < C * r:\n",
        "        min_value = float(\"inf\")\n",
        "        for i in I:\n",
        "            temT = T + [i]\n",
        "            value = np.sum(np.sum((s_w[:,temT,:,:]*w[temT,:,:]),axis=(1,2,3))) ** 2\n",
        "            if value < min_value:\n",
        "                min_value = value\n",
        "                min_i = i\n",
        "        T.append(min_i)\n",
        "        I.remove(i)\n",
        "    return I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIYWgjjO4aVL"
      },
      "outputs": [],
      "source": [
        "conv_i = 0\n",
        "linear_i = 0\n",
        "\n",
        "conv_r = 0.91  # prune ratio of conv layers\n",
        "linear_r = 0.91 # prune ratio of linear layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), 100, 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MCDIRDT4aaO"
      },
      "outputs": [],
      "source": [
        "newmodel = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJiTU1Bj4ac8"
      },
      "outputs": [],
      "source": [
        "x = next(iter(train_dataloader))[0].to(device)\n",
        "mask = np.random.choice(x.shape[0], 16)\n",
        "x = x[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_KfHE8A4afm"
      },
      "outputs": [],
      "source": [
        "def create_pruned_weights(model, pruned_model, conv_i, conv_r, linear_i, linear_r, x):\n",
        "    for (m1,m2) in zip(model.modules(),pruned_model.modules()):\n",
        "        if isinstance(m1,nn.Conv2d): \n",
        "            if conv_i == 0:\n",
        "                last_output = m1(x)\n",
        "                last_conv_2 = m2\n",
        "                last_conv_1 = m1\n",
        "                last_T = [i for i in range(m1.weight.data.shape[1])]\n",
        "                conv_i += 1\n",
        "                continue\n",
        "                \n",
        "            print('Pruning the {0}th Conv layer'.format(conv_i))\n",
        "            s_w = generate_windows(m1,last_output)\n",
        "            random_filter = np.random.randint(0, m1.weight.data.shape[0])\n",
        "            w = m1.weight.data[random_filter]\n",
        "            T = get_T(s_w.cpu(),w.cpu(),conv_r)\n",
        "\n",
        "            last_conv_2.weight.data = last_conv_1.weight.data[T].clone()\n",
        "            last_conv_2.weight.data = last_conv_2.weight.data[:,last_T,:,:].clone()\n",
        "            \n",
        "            # last_bn_2.weight.data = last_bn_1.weight.data[T].clone()\n",
        "            # last_bn_2.bias.data = last_bn_1.bias.data[T].clone()\n",
        "            # last_bn_2.running_mean = last_bn_1.running_mean[T].clone()\n",
        "            # last_bn_2.running_var = last_bn_1.running_var[T].clone()\n",
        "            \n",
        "            m2.weight.data = m1.weight.data[:,T,:,:].clone()\n",
        "\n",
        "            last_output = last_output[:,T,:,:]\n",
        "            last_output = m2(last_output)\n",
        "            last_conv_2 = m2\n",
        "            last_conv_1 = m1\n",
        "            last_T = T\n",
        "            conv_i += 1\n",
        "\n",
        "\n",
        "        elif isinstance(m1,nn.BatchNorm2d):\n",
        "            last_output = m1(last_output)\n",
        "            last_bn_2 = m2\n",
        "            last_bn_1 = m1\n",
        "            \n",
        "        elif isinstance(m1,nn.ReLU) or isinstance(m1,nn.MaxPool2d) or isinstance(m1,nn.Dropout):\n",
        "            last_output = m1(last_output)\n",
        "\n",
        "        elif isinstance(m1,nn.Linear):\n",
        "            if linear_i ==0:\n",
        "                last_linear_2 = m2\n",
        "                last_linear_1 = m1\n",
        "                linear_i += 1\n",
        "                last_T = [i for i in range(m1.weight.data.shape[1])]\n",
        "                continue\n",
        "                \n",
        "            print('Pruning the {0}th Linear layer'.format(linear_i))\n",
        "            weight_copy = m1.weight.data.abs().clone()\n",
        "            weight_copy = weight_copy.cpu().numpy()\n",
        "            L1_norm = np.sum(weight_copy, axis=0)\n",
        "            arg_max = np.argsort(L1_norm)\n",
        "            alive_param_num = int(weight_copy.shape[1]*(1-linear_r))\n",
        "            arg_max_rev = arg_max[::-1][:alive_param_num]\n",
        "            \n",
        "            last_linear_2.weight.data = last_linear_1.weight.data[arg_max_rev.tolist()].clone()\n",
        "            last_linear_2.weight.data = last_linear_2.weight.data[:,last_T].clone()\n",
        "            last_linear_2.bias.data = last_linear_1.bias.data[arg_max_rev.tolist()].clone()\n",
        "            \n",
        "            m2.weight.data = m1.weight.data[:,arg_max_rev.tolist()].clone()\n",
        "\n",
        "            last_linear_2 = m2\n",
        "            last_linear_1 = m1\n",
        "            linear_i += 1\n",
        "            last_T = arg_max_rev.tolist()\n",
        "\n",
        "    return pruned_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newmodel = create_pruned_weights(model, newmodel, conv_i, conv_r, linear_i, linear_r, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "RkHPYXbukl2A",
        "outputId": "e67a2e9c-e1f5-4f60-8641-7e81b317b773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning the 1th Conv layer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-57ac8df00b95>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_pruned_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-117-96fa3c1c5a15>\u001b[0m in \u001b[0;36mcreate_pruned_weights\u001b[0;34m(model, pruned_model, conv_i, conv_r, linear_i, linear_r, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlast_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mlast_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mlast_conv_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlast_conv_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given weight of size [100, 4, 5, 5], expected bias to be 1-dimensional with 100 elements, but got bias of size [8] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgl3RP6ed9VT",
        "outputId": "afff5f05-f53e-47ef-f646-eb77aba6df1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 18.064\n",
            "Small top-5 test accuracy: 53.534\n"
          ]
        }
      ],
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5305Y-ReLzM",
        "outputId": "25b66d4a-bbc8-4a04-f41d-96436fed5bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 0.407 Acc@1 88.403 Acc@5 98.335\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.193 Acc@1 95.947 Acc@5 99.358\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 0.127 Acc@1 97.311 Acc@5 99.579\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.100 Acc@1 97.893 Acc@5 99.860\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.085 Acc@1 98.435 Acc@5 99.960\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 6: Losses 0.076 Acc@1 98.736 Acc@5 99.980\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 7: Losses 0.069 Acc@1 98.917 Acc@5 99.980\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 8: Losses 0.063 Acc@1 98.997 Acc@5 99.980\n",
            "==> training...\n",
            " * epoch 9: Losses 0.059 Acc@1 99.057 Acc@5 99.980\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 10: Losses 0.055 Acc@1 99.057 Acc@5 99.980\n",
            "==> training...\n",
            " * epoch 11: Losses 0.052 Acc@1 99.137 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.049 Acc@1 99.177 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.047 Acc@1 99.278 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 14: Losses 0.045 Acc@1 99.418 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.043 Acc@1 99.679 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 16: Losses 0.041 Acc@1 99.699 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.039 Acc@1 99.739 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.038 Acc@1 99.739 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.036 Acc@1 99.719 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.035 Acc@1 99.739 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.034 Acc@1 99.779 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.034 Acc@1 99.759 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.033 Acc@1 99.779 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.033 Acc@1 99.799 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.032 Acc@1 99.839 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.032 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.031 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.031 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.031 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.031 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.030 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.030 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.030 Acc@1 99.839 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.030 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.029 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.029 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.029 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.029 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.029 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.029 Acc@1 99.860 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.028 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.029 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.028 Acc@1 99.900 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.028 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.028 Acc@1 99.880 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.027 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.027 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.027 Acc@1 99.900 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.027 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.027 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.026 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.026 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.026 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.026 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.026 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.026 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.025 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.024 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.024 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.024 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.023 Acc@1 99.960 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.023 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.023 Acc@1 99.960 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.023 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.023 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.023 Acc@1 99.960 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.023 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.023 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.022 Acc@1 99.920 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.022 Acc@1 99.960 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 94: Losses 0.022 Acc@1 99.960 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.022 Acc@1 99.940 Acc@5 100.000\n",
            "best accuracy: tensor(99.8195, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "train_small_network(newmodel, train_dataloader, val_dataloader, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW_BDhPoeL2D",
        "outputId": "baaafe14-6e92-4fcb-f6a7-13aea704bb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 83.676\n",
            "Small top-5 test accuracy: 97.453\n"
          ]
        }
      ],
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxZ2K-OceL6e",
        "outputId": "30418973-d8e4-43f6-f5ae-13d05c24c9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size big network(MB): 5.131947\n",
            "Size small network(MB): 0.420797\n"
          ]
        }
      ],
      "source": [
        "print('Size big network(MB):', os.path.getsize(\"best_model.pth\")/1e6)\n",
        "print('Size small network(MB):', os.path.getsize(\"best_small_model.pth\")/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzA-siXteL9G",
        "outputId": "d8c11621-13c8-4dd7-bdc3-6f5122a723fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of big network parameters: 426866\n",
            "Num of small network parameters: 34256\n"
          ]
        }
      ],
      "source": [
        "print('Num of big network parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print('Num of small network parameters:', sum(p.numel() for p in newmodel.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riPeeQkWeL_s",
        "outputId": "8a42b096-9582-4e03-aa52-57975ed37107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 4000\n",
            "conv1.bias 4\n",
            "conv2.weight 10000\n",
            "conv2.bias 100\n",
            "fc1.weight 20000\n",
            "fc1.bias 8\n",
            "fc2.weight 128\n",
            "fc2.bias 16\n"
          ]
        }
      ],
      "source": [
        "for name, p in newmodel.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkGKv1weTIta"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD915_0EO6_7"
      },
      "source": [
        "# **Network Slimming**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn2d(num_components, num_classes, filters=t_cfg, fc=t_fc).to(device)\n",
        "model.load_state_dict(torch.load('./best_model.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_evsyBzK0Yq",
        "outputId": "a93a473c-1c48-4297-9420-b8ae61e365b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQgWkKoDPWzH"
      },
      "outputs": [],
      "source": [
        "conv_i = 0\n",
        "linear_i = 0\n",
        "\n",
        "conv_r = 0.71  # prune ratio of conv layers\n",
        "linear_r = 0.71 # prune ratio of linear layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMrpB5nOgegE",
        "outputId": "af7fada1-5061-4fd2-c772-1804b5225e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14, 29, 'M']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conv_mask(model, s_cfg):\n",
        "    total = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            total += [m.weight.data.shape[0]]\n",
        "  \n",
        "    print(total)\n",
        "    bn = [torch.zeros(total[0]), torch.zeros(total[1])]\n",
        "    index = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            size = m.weight.data.shape[0]\n",
        "            bn[index] = m.weight.data.abs().clone()\n",
        "            index += 1\n",
        "\n",
        "    y0, i0 = torch.sort(bn[0])\n",
        "    y1, i1 = torch.sort(bn[1])\n",
        "    # thre_index = int(total * prune_ratio)\n",
        "    # print(total, thre_index)\n",
        "    thre0 = y0[s_cfg[0]]\n",
        "    thre1 = y1[s_cfg[1]]\n",
        "    thre = [thre0, thre1]\n",
        "\n",
        "    pruned = 0\n",
        "    cfg = []\n",
        "    cfg_mask = []\n",
        "    index = 0\n",
        "    for k, m in enumerate(model.modules()):\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            weight_copy = m.weight.data.abs().clone()\n",
        "            mask = weight_copy.le(thre[index]).float().to(device)\n",
        "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
        "            m.weight.data.mul_(mask)\n",
        "            m.bias.data.mul_(mask)\n",
        "            cfg.append(int(torch.sum(mask)))\n",
        "            cfg_mask.append(mask.clone())\n",
        "            index += 1\n",
        "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.\n",
        "                format(k, mask.shape[0], int(torch.sum(mask))))\n",
        "        elif isinstance(m, nn.MaxPool2d):\n",
        "            cfg.append('M')\n",
        "\n",
        "    return cfg, cfg_mask"
      ],
      "metadata": {
        "id": "JJLKf8lYeoPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fc_mask(model, fc):\n",
        "    total = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.BatchNorm1d):\n",
        "            total += m.weight.data.shape[0]\n",
        "  \n",
        "    bn = torch.zeros(total)\n",
        "    index = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.BatchNorm1d):\n",
        "            size = m.weight.data.shape[0]\n",
        "            bn[index:index+size] = m.weight.data.abs().clone()\n",
        "            index += size\n",
        "\n",
        "    y, i = torch.sort(bn)\n",
        "    thre = y[fc]\n",
        "\n",
        "    pruned = 0\n",
        "    # cfg = 0\n",
        "    # cfg_mask = 0\n",
        "\n",
        "    for k, m in enumerate(model.modules()):\n",
        "        if isinstance(m, nn.BatchNorm1d):\n",
        "            weight_copy = m.weight.data.abs().clone()\n",
        "            mask = weight_copy.le(thre).float().to(device)\n",
        "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
        "            m.weight.data.mul_(mask)\n",
        "            m.bias.data.mul_(mask)\n",
        "            cfg = int(torch.sum(mask))\n",
        "            cfg_mask = mask.clone()\n",
        "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.\n",
        "                format(k, mask.shape[0], int(torch.sum(mask))))\n",
        "        # elif isinstance(m, nn.MaxPool2d):\n",
        "        #     cfg.append('M')\n",
        "    cfg_mask = torch.nonzero(cfg_mask).cpu().detach().numpy().ravel()\n",
        "\n",
        "    return cfg, cfg_mask"
      ],
      "metadata": {
        "id": "zXKgHeLohSMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6MUsNK4O98v",
        "outputId": "709708ae-4de6-4125-a997-4a4f81977f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 100]\n",
            "layer index: 2 \t total channel: 50 \t remaining channel: 15\n",
            "layer index: 4 \t total channel: 100 \t remaining channel: 30\n"
          ]
        }
      ],
      "source": [
        "cfg, cfg_mask = create_conv_mask(model, s_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3PVUtukukFD",
        "outputId": "2fb01e24-11f4-44da-92ab-ccc32c5c2ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.],\n",
              "        device='cuda:0'),\n",
              " tensor([1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
              "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
              "         0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
              "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1.], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc, fc_mask = create_fc_mask(model, s_fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCwv-mo_hweC",
        "outputId": "9c2543ad-7e45-4e28-e899-e86cf10daad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer index: 7 \t total channel: 100 \t remaining channel: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoI4KY2c2E6s"
      },
      "outputs": [],
      "source": [
        "tempmodel = cnn2d(num_components, num_classes, filters=(cfg[0], cfg[1]), fc=fc, bn=True)\n",
        "tempmodel = tempmodel.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_pruned_weights(cfg_mask, fc_mask, model, newmodel):\n",
        "    start_mask = torch.ones(num_components)\n",
        "    layer_id_in_cfg = 0\n",
        "    end_mask = cfg_mask[layer_id_in_cfg]\n",
        "\n",
        "    for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "        if isinstance(m0, nn.BatchNorm2d):\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "            if idx1.size == 1:\n",
        "                idx1 = np.resize(idx1,(1,))\n",
        "            m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "            m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "            m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            start_mask = end_mask\n",
        "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "                end_mask = cfg_mask[layer_id_in_cfg]\n",
        "        elif isinstance(m0, nn.Conv2d):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "            print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            if idx1.size == 1:\n",
        "                idx1 = np.resize(idx1, (1,))\n",
        "            w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "            w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "            m1.weight.data = w1.clone()\n",
        "            # layer_id_in_cfg += 1\n",
        "            # start_mask = end_mask\n",
        "            # if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "            #     end_mask = cfg_mask[layer_id_in_cfg]\n",
        "        elif isinstance(m0, nn.Linear):\n",
        "            if layer_id_in_cfg == len(cfg_mask):\n",
        "                idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "                if idx0.size == 1:\n",
        "                    idx0 = np.resize(idx0, (1,))\n",
        "\n",
        "                k_idx0 = []\n",
        "                for k in idx0:\n",
        "                    new_k = range(k*25, (k+1)*25)\n",
        "                    k_idx0 += list(new_k)\n",
        "                # m1.weight.data = m0.weight.data[:, k_idx0].clone()\n",
        "                # m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "                w = m0.weight.data[:, k_idx0].clone()\n",
        "                m1.weight.data = w[fc_mask.tolist()].clone()\n",
        "                m1.bias.data = m0.bias.data[fc_mask.tolist()].clone()\n",
        "                layer_id_in_cfg += 1\n",
        "                continue\n",
        "\n",
        "            m1.weight.data = m0.weight.data[:, fc_mask.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "        elif isinstance(m0, nn.BatchNorm1d):\n",
        "            m1.weight.data = m0.weight.data[fc_mask.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data[fc_mask.tolist()].clone()\n",
        "            m1.running_mean = m0.running_mean[fc_mask.tolist()].clone()\n",
        "            m1.running_var = m0.running_var[fc_mask.tolist()].clone()\n",
        "\n",
        "    return newmodel"
      ],
      "metadata": {
        "id": "a60_RzmjJQqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempmodel = copy_pruned_weights(cfg_mask, fc_mask, model, tempmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT6Qnh_1u9oP",
        "outputId": "6d6ef1d4-7cd6-4979-c4ec-09ee6598d1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In shape: 40, Out shape 15.\n",
            "In shape: 15, Out shape 30.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_dict = tempmodel.state_dict()\n",
        "newmodel = cnn2d(num_components, num_classes, filters=(cfg[0], cfg[1]), fc=fc, bn=False)\n",
        "newmodel = newmodel.to(device)\n",
        "model_dict = newmodel.state_dict()\n",
        "\n",
        "for key, value in model_dict.items():\n",
        "    model_dict[key] = temp_dict[key]\n",
        "newmodel.load_state_dict(model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvYx1q0tWb5_",
        "outputId": "c2b4a857-858d-49eb-bc1f-2f6e40324824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in newmodel.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDNDNtrcd6qP",
        "outputId": "19f84fc5-c73b-4cca-efdd-51d577b7216d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 15000\n",
            "conv1.bias 15\n",
            "conv2.weight 11250\n",
            "conv2.bias 30\n",
            "fc1.weight 22500\n",
            "fc1.bias 30\n",
            "fc2.weight 270\n",
            "fc2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPc1mBkE3Qeg",
        "outputId": "45f3fc10-13fe-4419-877f-131db161560f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 1.487 Acc@1 43.690 Acc@5 79.696\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 1.222 Acc@1 44.506 Acc@5 83.408\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 3: Losses 1.047 Acc@1 59.391 Acc@5 84.967\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 4: Losses 0.968 Acc@1 65.664 Acc@5 87.231\n",
            "==> training...\n",
            " * epoch 5: Losses 0.849 Acc@1 66.036 Acc@5 88.085\n",
            "==> training...\n",
            " * epoch 6: Losses 0.794 Acc@1 66.889 Acc@5 90.535\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 7: Losses 0.730 Acc@1 70.156 Acc@5 91.982\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 8: Losses 0.634 Acc@1 75.167 Acc@5 92.094\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 9: Losses 0.565 Acc@1 76.392 Acc@5 93.022\n",
            "==> training...\n",
            " * epoch 10: Losses 0.498 Acc@1 79.659 Acc@5 93.950\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 11: Losses 0.411 Acc@1 84.521 Acc@5 96.474\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 12: Losses 0.285 Acc@1 88.085 Acc@5 97.661\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 13: Losses 0.168 Acc@1 93.764 Acc@5 98.886\n",
            "==> training...\n",
            " * epoch 14: Losses 0.128 Acc@1 95.286 Acc@5 99.889\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 15: Losses 0.100 Acc@1 96.399 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 16: Losses 0.085 Acc@1 97.327 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 17: Losses 0.056 Acc@1 98.849 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 18: Losses 0.034 Acc@1 99.592 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 19: Losses 0.025 Acc@1 99.666 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.020 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.019 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.019 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.018 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.017 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.017 Acc@1 100.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 26: Losses 0.017 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.016 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.016 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.016 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.016 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.025 Acc@1 99.592 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.019 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.017 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.020 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.024 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.018 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.013 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.018 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.018 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.013 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.015 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.018 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.020 Acc@1 99.740 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.014 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.015 Acc@1 99.889 Acc@5 100.000\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "newmodel = train_small_network(newmodel, train_dataloader, val_dataloader, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keybTqOK3Qhp",
        "outputId": "104cdfad-712d-4a43-8a5c-3ba41eee552f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 99.289\n",
            "Small top-5 test accuracy: 99.995\n"
          ]
        }
      ],
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uckl7xT53U6P",
        "outputId": "23d58f72-ec3d-4b35-fe95-be6260df7321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size big network(MB): 5.149163\n",
            "Size small network(MB): 0.601661\n"
          ]
        }
      ],
      "source": [
        "print('Size big network(MB):', os.path.getsize(\"best_model.pth\")/1e6)\n",
        "print('Size small network(MB):', os.path.getsize(\"best_small_model.pth\")/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFXBmOcl3U9U",
        "outputId": "cc7b03f4-bf52-431f-e217-7e2efb973132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of big network parameters: 427366\n",
            "Num of small network parameters: 9001\n"
          ]
        }
      ],
      "source": [
        "print('Num of big network parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print('Num of small network parameters:', sum(p.numel() for p in newmodel.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMhe3FWM0yf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq1ftwHsxeVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQlGaMnnugzG"
      },
      "source": [
        "# **Soft Filter Pruning**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn2d(num_components, num_classes, filters=t_cfg, fc=t_fc).to(device)\n",
        "model.load_state_dict(torch.load('./best_model.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhJLRcNFeYFX",
        "outputId": "3f96a3d5-8d94-4de4-d2c4-cf4733b55725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_r = 0.7  # prune ratio of conv layers\n",
        "linear_r = 0.7 # prune ratio of lienar layers\n",
        "\n",
        "s_cfg = [int(t_cfg[0]*(1-conv_r)), int(t_cfg[1]*(1-conv_r)), 'M']\n",
        "s_fc = int(t_fc*(1-linear_r))"
      ],
      "metadata": {
        "id": "qNboW3mf_I0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74V_HnZvEjus",
        "outputId": "fb0734d1-5658-4ff2-ba20-4912b60e011e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 30, 'M']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTNX-8txhAZT"
      },
      "outputs": [],
      "source": [
        "def check_channel(tensor):\n",
        "    size_0 = tensor.size()[0]\n",
        "    size_1 = tensor.size()[1] * tensor.size()[2] * tensor.size()[3]\n",
        "    tensor_resize = tensor.view(size_0, -1)\n",
        "    # indicator: if the channel contain all zeros\n",
        "    channel_if_zero = np.zeros(size_0)\n",
        "    for x in range(0, size_0, 1):\n",
        "        channel_if_zero[x] = np.count_nonzero(tensor_resize[x].cpu().numpy()) != 0\n",
        "    # indices = (torch.LongTensor(channel_if_zero) != 0 ).nonzero().view(-1)\n",
        "\n",
        "    indices_nonzero = torch.LongTensor((channel_if_zero != 0).nonzero()[0])\n",
        "    # indices_nonzero = torch.LongTensor((channel_if_zero != 0).nonzero()[0])\n",
        "\n",
        "    zeros = (channel_if_zero == 0).nonzero()[0]\n",
        "    indices_zero = torch.LongTensor(zeros) if zeros != [] else []\n",
        "\n",
        "    return indices_zero, indices_nonzero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqCexZQjQENS"
      },
      "outputs": [],
      "source": [
        "class Mask:\n",
        "    def __init__(self, model):\n",
        "        self.model_size = {}\n",
        "        self.model_length = {}\n",
        "        self.compress_rate = {}\n",
        "        self.mat = {}\n",
        "        self.model = model\n",
        "        self.mask_index = []\n",
        "\n",
        "    def get_codebook(self, weight_torch, compress_rate, length):\n",
        "        weight_vec = weight_torch.view(length)\n",
        "        weight_np = weight_vec.cpu().numpy()\n",
        "\n",
        "        weight_abs = np.abs(weight_np)\n",
        "        weight_sort = np.sort(weight_abs)\n",
        "\n",
        "        threshold = weight_sort[int(length * (1 - compress_rate))]\n",
        "        weight_np[weight_np <= -threshold] = 1\n",
        "        weight_np[weight_np >= threshold] = 1\n",
        "        weight_np[weight_np != 1] = 0\n",
        "\n",
        "        #print(\"codebook done\")\n",
        "        return weight_np\n",
        "\n",
        "    def get_filter_codebook(self, weight_torch, compress_rate, length):\n",
        "        codebook = np.ones(length)\n",
        "        if len(weight_torch.size()) == 4:\n",
        "            filter_pruned_num = int(weight_torch.size()[0] * (1 - compress_rate))\n",
        "            weight_vec = weight_torch.view(weight_torch.size()[0], -1)\n",
        "\n",
        "            norm2 = torch.norm(weight_vec, 2, 1)\n",
        "            norm2_np = norm2.cpu().numpy()\n",
        "            filter_index = norm2_np.argsort()[:filter_pruned_num]\n",
        "\n",
        "            kernel_length = weight_torch.size()[1] * weight_torch.size()[2] * weight_torch.size()[3]\n",
        "            for x in range(0, len(filter_index)):\n",
        "                codebook[filter_index[x] * kernel_length: (filter_index[x] + 1) * kernel_length] = 0\n",
        "\n",
        "            #print(\"filter codebook done\")\n",
        "        else:\n",
        "            pass\n",
        "        return codebook\n",
        "\n",
        "    def convert2tensor(self, x):\n",
        "        x = torch.FloatTensor(x)\n",
        "        return x\n",
        "\n",
        "    def init_length(self):\n",
        "        for index, item in enumerate(self.model.parameters()):\n",
        "            self.model_size[index] = item.size()\n",
        "\n",
        "        for index1 in self.model_size:\n",
        "            for index2 in range(0, len(self.model_size[index1])):\n",
        "                if index2 == 0:\n",
        "                    self.model_length[index1] = self.model_size[index1][0]\n",
        "                else:\n",
        "                    self.model_length[index1] *= self.model_size[index1][index2]\n",
        "\n",
        "    def init_rate(self, layer_rate):\n",
        "        # cfg_5x = [5, 10]\n",
        "        # #cfg_5x = [24, 22, 41, 51, 108, 89, 111, 184, 276, 228, 512, 512, 512]\n",
        "        # cfg_official = [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512]\n",
        "        # # cfg = [32, 64, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
        "        cfg_index = 0\n",
        "        pre_cfg = True\n",
        "        for index, item in enumerate(self.model.named_parameters()):\n",
        "            self.compress_rate[index] = 1\n",
        "            if len(item[1].size()) == 4:\n",
        "                #print(item[1].size())\n",
        "                if not pre_cfg:\n",
        "                    self.compress_rate[index] = layer_rate\n",
        "                    self.mask_index.append(index)\n",
        "                    #print(item[0], \"self.mask_index\", self.mask_index)\n",
        "                else:\n",
        "                    self.compress_rate[index] =  s_cfg[cfg_index] / item[1].size()[0]\n",
        "                    self.mask_index.append(index)\n",
        "                    #print(item[0], \"self.mask_index\", self.mask_index, cfg_index, cfg_5x[cfg_index], item[1].size()[0])\n",
        "                    cfg_index += 1\n",
        "\n",
        "    def init_mask(self, layer_rate):\n",
        "        self.init_rate(layer_rate)\n",
        "        for index, item in enumerate(self.model.parameters()):\n",
        "            if (index in self.mask_index):\n",
        "                self.mat[index] = self.get_filter_codebook(item.data, self.compress_rate[index],\n",
        "                                                           self.model_length[index])\n",
        "                self.mat[index] = self.convert2tensor(self.mat[index])\n",
        "                self.mat[index] = self.mat[index].to(device)\n",
        "        #print(\"mask Ready\")\n",
        "\n",
        "    def do_mask(self):\n",
        "        for index, item in enumerate(self.model.parameters()):\n",
        "            if (index in self.mask_index):\n",
        "                a = item.data.view(self.model_length[index])\n",
        "                b = a * self.mat[index]\n",
        "                item.data = b.view(self.model_size[index])\n",
        "        #print(\"mask Done\")\n",
        "\n",
        "    def if_zero(self):\n",
        "        for index, item in enumerate(self.model.parameters()):\n",
        "            if index == 0:\n",
        "                a = item.data.view(self.model_length[index])\n",
        "                b = a.cpu().numpy()\n",
        "\n",
        "                # print(\"layer: %d, number of nonzero weight is %d, zero is %d\" % (\n",
        "                #     index, np.count_nonzero(b), len(b) - np.count_nonzero(b)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oZn-cfqQEPy"
      },
      "outputs": [],
      "source": [
        "R = 0.1\n",
        "m = Mask(model)\n",
        "m.init_length()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g13LHbwBukKF"
      },
      "outputs": [],
      "source": [
        "m.model = model\n",
        "\n",
        "m.init_mask(R)\n",
        "# m.if_zero()\n",
        "m.do_mask()\n",
        "model = m.model\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glGUl1oVyhF0",
        "outputId": "5a600ec9-f588-401e-fc87-c0874f0f3b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 50000\n",
            "conv1.bias 50\n",
            "conv2.weight 125000\n",
            "conv2.bias 100\n",
            "fc1.weight 250000\n",
            "fc1.bias 100\n",
            "fc2.weight 900\n",
            "fc2.bias 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sfp_network(model, train_loader, val_loader, epochs):\n",
        "    best_acc = 0\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # routine\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        #adjust_learning_rate(epoch, opt, optimizer)\n",
        "        print(\"==> training...\")\n",
        "\n",
        "        time1 = time.time()\n",
        "        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "\n",
        "        # print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        if (epoch % 1 == 0 or epoch == epochs - 1):\n",
        "            #        if (random.randint(1,args.epoch_prune)==1 or epoch == args.epochs-1):\n",
        "            m.model = model\n",
        "            m.if_zero()\n",
        "            m.init_mask(R)\n",
        "            m.do_mask()\n",
        "            m.if_zero()\n",
        "            model = m.model\n",
        "            model = model.to(device)\n",
        "            \n",
        "        test_acc, test_acc_top5, test_loss = validate(val_loader, model)\n",
        "\n",
        "\n",
        "        # save the best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            save_file = './best_small_model.pth'\n",
        "            print('saving the best model!')\n",
        "            torch.save(state, save_file)\n",
        "\n",
        "        # regular saving\n",
        "        # if epoch % save_freq == 0:\n",
        "        #     print('==> Saving...')\n",
        "        #     state = {\n",
        "        #         'epoch': epoch,\n",
        "        #         'model': model.state_dict(),\n",
        "        #         'accuracy': test_acc,\n",
        "        #         'optimizer': optimizer.state_dict(),\n",
        "        #     }\n",
        "        #     save_file = os.path.join('./', 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "        #     torch.save(state, save_file)\n",
        "\n",
        "    # This best accuracy is only for printing purpose.\n",
        "    # The results reported in the paper/README is from the last epoch.\n",
        "    print('best accuracy:', best_acc)\n",
        "\n",
        "    # save model\n",
        "    state = {\n",
        "        #'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    save_file = './last_small_model.pth'\n",
        "    torch.save(state, save_file)"
      ],
      "metadata": {
        "id": "KWbTsblh7r52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_pruned_weights(cfg_mask, fc_mask, model, newmodel):\n",
        "    start_mask = torch.ones(num_components)\n",
        "    layer_id_in_cfg = 0\n",
        "    end_mask = cfg_mask[layer_id_in_cfg]\n",
        "\n",
        "    for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "        if isinstance(m0, nn.BatchNorm2d):\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "            if idx1.size == 1:\n",
        "                idx1 = np.resize(idx1,(1,))\n",
        "            m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "            m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "            m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            start_mask = end_mask\n",
        "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "                end_mask = cfg_mask[layer_id_in_cfg]\n",
        "        elif isinstance(m0, nn.Conv2d):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "            print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            if idx1.size == 1:\n",
        "                idx1 = np.resize(idx1, (1,))\n",
        "            w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "            w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "            m1.weight.data = w1.clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            start_mask = end_mask\n",
        "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "                end_mask = cfg_mask[layer_id_in_cfg]\n",
        "        elif isinstance(m0, nn.Linear):\n",
        "            #print(m1.weight.data.shape, m0.weight.data.shape, m1.bias.data.shape, m0.bias.data.shape)\n",
        "            if layer_id_in_cfg == len(cfg_mask):\n",
        "                idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "                if idx0.size == 1:\n",
        "                    idx0 = np.resize(idx0, (1,))\n",
        "\n",
        "                k_idx0 = []\n",
        "                for k in idx0:\n",
        "                    new_k = range(k*25, (k+1)*25)\n",
        "                    k_idx0 += list(new_k)\n",
        "                # m1.weight.data = m0.weight.data[:, k_idx0].clone()\n",
        "                # m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "                w = m0.weight.data[:, k_idx0].clone()\n",
        "                m1.weight.data = w[fc_mask.tolist()].clone()\n",
        "                m1.bias.data = m0.bias.data[fc_mask.tolist()].clone()\n",
        "                layer_id_in_cfg += 1\n",
        "                continue\n",
        "\n",
        "            m1.weight.data = m0.weight.data[:, fc_mask.tolist()].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "        elif isinstance(m0, nn.BatchNorm1d):\n",
        "            m1.weight.data = m0.weight.data.clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "            m1.running_mean = m0.running_mean.clone()\n",
        "            m1.running_var = m0.running_var.clone()\n",
        "\n",
        "    return newmodel"
      ],
      "metadata": {
        "id": "1SQlEGyVAOFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sfp_network(model, train_dataloader, val_dataloader, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtPL8YMs8J82",
        "outputId": "72e0cc62-1dbc-4885-a489-10c4c6532ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 0.024 Acc@1 99.852 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.016 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 3: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 4: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 5: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 6: Losses 0.017 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.015 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 9: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.015 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.014 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.016 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.025 Acc@1 99.517 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 23: Losses 0.021 Acc@1 99.703 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.012 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.014 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.015 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.029 Acc@1 99.369 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.015 Acc@1 99.814 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.013 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.014 Acc@1 99.889 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.012 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.014 Acc@1 99.852 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.019 Acc@1 99.629 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.018 Acc@1 99.740 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 67: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.016 Acc@1 99.777 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.013 Acc@1 99.926 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.011 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "best accuracy: tensor(99.6667, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn2d(num_components, num_classes, filters=t_cfg, fc=t_fc).to(device)\n",
        "model.load_state_dict(torch.load('./best_small_model.pth')['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOCb-4PA9JIb",
        "outputId": "dd446220-a505-4954-9d50-9008563894fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5188jsGiukPK",
        "outputId": "7c1e0849-d7c3-446f-f179-f540e4661915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 99.261\n",
            "Small top-5 test accuracy: 99.960\n"
          ]
        }
      ],
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, model)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conv_mask(model, t_cfg):\n",
        "    item = list(model.state_dict().items())\n",
        "    print(\"length of state dict is\", len(item))\n",
        "\n",
        "    kept_index_per_layer = {}\n",
        "    kept_filter_per_layer = {}\n",
        "    pruned_index_per_layer = {}\n",
        "\n",
        "    for x in range(0, len(item)-4, 2):\n",
        "        indices_zero, indices_nonzero = check_channel(item[x][1])\n",
        "        # indices_list.append(indices_nonzero)\n",
        "        pruned_index_per_layer[item[x][0]] = indices_zero\n",
        "        kept_index_per_layer[item[x][0]] = indices_nonzero\n",
        "        kept_filter_per_layer[item[x][0]] = indices_nonzero.shape[0]\n",
        "\n",
        "    print('Kept indices per layer: ', kept_index_per_layer)\n",
        "    new_cfg = (len(kept_index_per_layer['conv1.weight']), len(kept_index_per_layer['conv2.weight']))\n",
        "\n",
        "    cfg_mask = []\n",
        "\n",
        "    for i in range(len(t_cfg)):\n",
        "        index_array = list(kept_index_per_layer.values())[i]\n",
        "        mask_array = torch.zeros(t_cfg[i])\n",
        "        mask_array[index_array] = 1\n",
        "        cfg_mask.append(mask_array)\n",
        "\n",
        "    return new_cfg, cfg_mask"
      ],
      "metadata": {
        "id": "purpJ8MM-zwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fc_mask(model, fc):\n",
        "    layer_id = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            weight_copy = m.weight.data.abs().clone()\n",
        "            weight_copy = weight_copy.cpu().numpy()\n",
        "            L1_norm = np.sum(weight_copy, axis=0)\n",
        "            arg_max = np.argsort(L1_norm)\n",
        "            #alive_param_num = int(weight_copy.shape[1]*(1-linear_r))\n",
        "            arg_max_rev = arg_max[::-1][:fc]\n",
        "    return arg_max_rev"
      ],
      "metadata": {
        "id": "JEHAeQCQAWEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cfg, cfg_mask = create_conv_mask(model, t_cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rOwmkk5_y32",
        "outputId": "8610777d-feec-467b-9702-c0163f7937d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of state dict is 8\n",
            "Kept indices per layer:  {'conv1.weight': tensor([ 0,  1,  7, 12, 13, 15, 19, 22, 26, 29, 36, 38, 39, 46, 48]), 'conv2.weight': tensor([ 1,  3,  8, 14, 16, 17, 19, 20, 21, 23, 24, 28, 35, 41, 43, 45, 50, 58,\n",
            "        60, 65, 67, 70, 77, 80, 82, 85, 89, 93, 96, 99])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-59776d26007f>:15: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  indices_zero = torch.LongTensor(zeros) if zeros != [] else []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9YX3VUhCkzS",
        "outputId": "71dafe57-3381-4865-f823-1c4551ed2f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc_mask = create_fc_mask(model, s_fc)"
      ],
      "metadata": {
        "id": "DQDYFsqsAY_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fc_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVCXe13C_6-3",
        "outputId": "f68494d4-52b2-4088-d2ef-9e2e480ed97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([93, 16, 50, 65,  1, 67, 99, 84, 92, 18, 80, 94, 35,  3,  8,  6, 23,\n",
              "       40, 89, 33, 30, 60, 38, 73, 47, 26, 79, 19,  7, 75])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nh1l8BNn4Gz"
      },
      "outputs": [],
      "source": [
        "newmodel = cnn2d(num_components, num_classes, filters=s_cfg, fc=s_fc).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newmodel = copy_pruned_weights(cfg_mask, fc_mask, model, newmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec_TpPgvi_a4",
        "outputId": "d7ea8542-f66a-4977-8686-2c35143f4bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In shape: 40, Out shape 15.\n",
            "In shape: 15, Out shape 30.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiH1qUI5rQwu",
        "outputId": "dee0d100-b42a-440e-912e-5767c7c89ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight 15000\n",
            "conv1.bias 15\n",
            "conv2.weight 11250\n",
            "conv2.bias 30\n",
            "fc1.weight 22500\n",
            "fc1.bias 30\n",
            "fc2.weight 270\n",
            "fc2.bias 9\n"
          ]
        }
      ],
      "source": [
        "for name, p in newmodel.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wt_h5pNrGyc",
        "outputId": "adf05d86-be1f-4a98-b4dd-ef0bb016598d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 98.959\n",
            "Small top-5 test accuracy: 99.822\n"
          ]
        }
      ],
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, child in newmodel.named_children():\n",
        "    if 'fc' not in name:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "VqdGfEgbA3FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_small_network(newmodel, train_dataloader, val_dataloader, epochs=100)"
      ],
      "metadata": {
        "id": "goe-jZOfjZpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17007f23-b8c1-440d-d309-b899915b4d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> training...\n",
            " * epoch 1: Losses 0.021 Acc@1 99.814 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 2: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 3: Losses 0.013 Acc@1 99.963 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 4: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "saving the best model!\n",
            "==> training...\n",
            " * epoch 5: Losses 0.013 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 6: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 7: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 8: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 9: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 10: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 11: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 12: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 13: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 14: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 15: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 16: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 17: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 18: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 19: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 20: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 21: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 22: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 23: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 24: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 25: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 26: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 27: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 28: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 29: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 30: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 31: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 32: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 33: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 34: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 35: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 36: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 37: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 38: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 39: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 40: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 41: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 42: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 43: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 44: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 45: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 46: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 47: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 48: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 49: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 50: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 51: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 52: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 53: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 54: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 55: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 56: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 57: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 58: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 59: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 60: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 61: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 62: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 63: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 64: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 65: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 66: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 67: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 68: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 69: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 70: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 71: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 72: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 73: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 74: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 75: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 76: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 77: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 78: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 79: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 80: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 81: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 82: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 83: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 84: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 85: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 86: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 87: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 88: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 89: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 90: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 91: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 92: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 93: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 94: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 95: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 96: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 97: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 98: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 99: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "==> training...\n",
            " * epoch 100: Losses 0.012 Acc@1 100.000 Acc@5 100.000\n",
            "best accuracy: tensor(99.3333, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cnn2d(\n",
              "  (conv1): Conv2d(40, 15, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(15, 30, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=750, out_features=30, bias=True)\n",
              "  (fc2): Linear(in_features=30, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_acc_1, s_acc_5, s_loss = validate(test_dataloader, newmodel)\n",
        "print(\"Small top-1 test accuracy: {:.3f}\".format(s_acc_1))\n",
        "print(\"Small top-5 test accuracy: {:.3f}\".format(s_acc_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz6UXDrzJaih",
        "outputId": "9a3ae4aa-3ce7-4e9a-919d-08a4abacdb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small top-1 test accuracy: 99.316\n",
            "Small top-5 test accuracy: 99.980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num of big network parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print('Num of small network parameters:', sum(p.numel() for p in newmodel.parameters()))"
      ],
      "metadata": {
        "id": "GvO7pIaoJan1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67bc9b0c-9fca-41e6-eafc-6224a1e51e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of big network parameters: 426159\n",
            "Num of small network parameters: 49104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2XpJNiTN9g_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}